<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://waleopakunle.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://waleopakunle.com/" rel="alternate" type="text/html" /><updated>2020-11-05T09:15:10-06:00</updated><id>https://waleopakunle.com/feed.xml</id><title type="html">AI Tech Blog</title><subtitle>A chronicle of my passion for developing witty computers.</subtitle><entry><title type="html">Stack Gan Exposition</title><link href="https://waleopakunle.com/2020/11/04/STACK-GAN-Exposition.html" rel="alternate" type="text/html" title="Stack Gan Exposition" /><published>2020-11-04T00:00:00-06:00</published><updated>2020-11-04T00:00:00-06:00</updated><id>https://waleopakunle.com/2020/11/04/STACK-GAN-Exposition</id><content type="html" xml:base="https://waleopakunle.com/2020/11/04/STACK-GAN-Exposition.html">&lt;h1 id=&quot;face-aging-using-conditional-gan&quot;&gt;Face Aging Using Conditional GAN&lt;/h1&gt;

&lt;p&gt;StackGAN is a two-stage network. Each stage has two generators and two discriminators. StackGAN is made up of many networks, which are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stack-I GAN&lt;/strong&gt;: text encoder, Conditioning Augmentation network, generator network, discriminator network, embedding compressor network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stack-II GAN&lt;/strong&gt;: text encoder, Conditioning Augmentation network, generator network, discriminator network, embedding compressor network&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*** INSERT IMAGE OF STACK-GAN ARCHITECTURE ***&lt;/p&gt;

&lt;p&gt;The text encoder network&lt;/p&gt;

&lt;p&gt;The sole purpose of the text encoder network is to convert a text description (t) to a text embedding&lt;/p&gt;

&lt;p&gt;(&lt;img src=&quot;assets/img/2020-11-04-STACK-GAN-Exposition/media/image1.png&quot; alt=&quot;http://localhost:5001/assets/b202e2a1-36da-4851-8a99-1cf84585df1d.png&quot; /&gt;). I won’t train the text encoder network. I will be working with pre-trained text embeddings, but you can train yours by using the steps discussed in this paper: &lt;a href=&quot;https://arxiv.org/pdf/1605.05395.pdf&quot;&gt;https://arxiv.org/pdf/1605.05395.pdf&lt;/a&gt;. The text encoder network encodes a sentence to a 1,024-dimensional text embedding. The text encoder network is common to both of the stages&lt;/p&gt;

&lt;p&gt;The conditioning augmentation block&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;conditioning augmentation&lt;/strong&gt; (&lt;strong&gt;CA&lt;/strong&gt;) network samples random latent variables &lt;img src=&quot;assets/img/2020-11-04-STACK-GAN-Exposition/media/image2.png&quot; alt=&quot;http://localhost:5001/assets/d86d468c-b92c-41c9-83f1-75baf727847c.png&quot; /&gt; from a distribution, which is represented as &lt;img src=&quot;assets/img/2020-11-04-STACK-GAN-Exposition/media/image3.png&quot; alt=&quot;http://localhost:5001/assets/cb774d54-76ee-4c94-89a7-94e8b16c4493.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It adds randomness to the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It makes the generator network robust by capturing various objects with various poses and appearances. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It produces more image-text pairs. With a higher number of image-text pairs, we can train a robust network that can handle perturbations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;STAGE-1&lt;/p&gt;

&lt;p&gt;STAGE-II&lt;/p&gt;

&lt;p&gt;TERMS TO READ UP ON&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;diagonal covariance matrix&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;upsampling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Face Aging Using Conditional GAN</summary></entry><entry><title type="html">Face Aging Cgan Exposition</title><link href="https://waleopakunle.com/2020/11/04/Face-Aging-cGAN-Exposition.html" rel="alternate" type="text/html" title="Face Aging Cgan Exposition" /><published>2020-11-04T00:00:00-06:00</published><updated>2020-11-04T00:00:00-06:00</updated><id>https://waleopakunle.com/2020/11/04/Face-Aging-cGAN-Exposition</id><content type="html" xml:base="https://waleopakunle.com/2020/11/04/Face-Aging-cGAN-Exposition.html">&lt;h1 id=&quot;face-aging-using-conditional-gan&quot;&gt;Face Aging Using Conditional GAN&lt;/h1&gt;

&lt;p&gt;cGANs are a type of GAN that are conditioned on some extra information. We feed the extra information &lt;em&gt;y&lt;/em&gt; to the generator as an additional input layer. In vanilla GANs, there is no control over the category of the generated images. When we add a condition &lt;em&gt;y&lt;/em&gt; to the generator, we can generate images of a specific category, using &lt;em&gt;y&lt;/em&gt;, which might be any kind of data, such as a class label or integer data. Vanilla GANs can learn only one category and it is extremely difficult to architect GANs for multiple categories. A cGAN, however, can be used to generate multi-modal models with different conditions for different categories.&lt;/p&gt;

&lt;p&gt;The architecture of a cGAN is shown in the following diagram:&lt;/p&gt;

&lt;p&gt;** INSERT IMAGE OF ARCHITECTURE **&lt;/p&gt;

&lt;p&gt;The training objective function for cGANs can be expressed as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/img/2020-11-04-Face-Aging-cGAN-Exposition/media/image1.png&quot; alt=&quot;http://localhost:5001/assets/115b5d5f-6623-46dc-bed1-e99a6aa5910c.png&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Here, G is the generator network and D is the discriminator network. The loss for the discriminator is and the loss for the generator is . We can say the G(z&lt;/td&gt;
      &lt;td&gt;y) is modeling the distribution of our data given z and y. Here, z is a prior noise distribution&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The architecture of a cGAN for face aging is slightly more complicated. The Age-cGan consists of four networks: an encoder, the FaceNet, a generator network, and a discriminator network. With the encoder, we learn the inverse mapping of input face images and the age condition with the latent vector &lt;img src=&quot;assets/img/2020-11-04-Face-Aging-cGAN-Exposition/media/image2.png&quot; alt=&quot;http://localhost:5001/assets/577645cb-a80b-4a10-98e8-5cf21f336b77.png&quot; /&gt;. FaceNet is a face recognition network that learns the difference between an input image &lt;em&gt;x&lt;/em&gt; and a reconstructed image &lt;img src=&quot;assets/img/2020-11-04-Face-Aging-cGAN-Exposition/media/image3.png&quot; alt=&quot;http://localhost:5001/assets/a77a78e0-3ea2-4ce3-8f96-deca552c9b9f.png&quot; /&gt;. We have a generator network, which takes a hidden representation consisting of a face image and a condition vector and generates an image. The discriminator network is to discriminate between the real images and the fake images. &lt;/p&gt;

&lt;p&gt;The problem with cGANs is that they can’t learn the task of inverse mapping an input image &lt;em&gt;x&lt;/em&gt; with attributes &lt;em&gt;y&lt;/em&gt; to a latent vector &lt;em&gt;z.&lt;/em&gt; The solution to this problem is to use an encoder network. We can train an encoder network to approximate the inverse mapping of the input images &lt;em&gt;x.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-encoder-network&quot;&gt;The encoder network&lt;/h1&gt;

&lt;p&gt;The primary goal of the encoder network is to generate a latent vector of the provided images. Basically, it takes an image of a dimension of (64, 64, 3) and converts it into a 100-dimensional vector. The encoder network is a deep convolutional neural network. The network contains four convolutional blocks and two dense layers. Each convolutional block contains a convolutional layer, a batch normalization layer, and an activation function. In each convolutional block, each convolutional layer is followed by a batch normalization layer, except the first convolutional layer&lt;/p&gt;

&lt;h1 id=&quot;face-recognition-network&quot;&gt;Face recognition network&lt;/h1&gt;

&lt;p&gt;The primary goal of the face recognition network is to recognize a person’s identity in a given image. For our task, we will be using the pre-trained or ResNet-50 model without fully connected layers. The pre-trained or ResNet-50 network, once provided with an image, returns the corresponding embedding. The extracted embeddings for the real image and the reconstructed image can be calculated by calculating the Euclidean distance of the embeddings&lt;/p&gt;

&lt;h1 id=&quot;the-generator-network&quot;&gt;The generator network&lt;/h1&gt;

&lt;p&gt;The primary goal of the generator is to generate an image of a dimension of (64, 64, 3). It takes a 100-dimensional latent vector and some extra information, &lt;em&gt;y&lt;/em&gt;, and&lt;strong&gt; &lt;/strong&gt;tries to generate realistic images. The generator network is a deep convolutional neural network too. It is made up of dense, upsampling, and convolutional layers. It takes two input values: a noise vector and a conditioning value. The conditioning value is the additional information provided to the network. For the Age-cGAN, this will be the age. &lt;/p&gt;

&lt;h2 id=&quot;the-discriminator-network&quot;&gt;The discriminator network&lt;/h2&gt;

&lt;p&gt;The primary goal of the discriminator network is to identify whether the provided image is fake or real. It does this by passing the image through a series of downsampling layers and some classification layers. In other words, it predicts whether the image is real or fake. Like the other networks, the discriminator network is another deep convolutional network. It contains several convolutional blocks. Each convolutional block contains a convolutional layer, a batch normalization layer, and an activation function, apart from the first convolutional block, which doesn’t have the batch normalization layer. &lt;/p&gt;

&lt;h2 id=&quot;aging-cgan-training&quot;&gt;Aging-CGAN TRAINING&lt;/h2&gt;

&lt;p&gt;The training of the Age-cGAN is made up of three stages:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conditional GAN training:&lt;/strong&gt; In this stage, we train the generator network and the discriminator network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Initial latent vector approximation:&lt;/strong&gt; In this stage, we train the encoder network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Latent vector optimization:&lt;/strong&gt; In this stage, we optimize both the encoder and the generator network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;*** CREATE YOUR OWN DIAGRAM OF AGING-CGAN TRAINING AND PLACE HERE ***&lt;/p&gt;

&lt;h2 id=&quot;conditional-gan-training&quot;&gt;Conditional GAN training&lt;/h2&gt;

&lt;p&gt;In this stage, we train the generator network and the discriminator network. Once trained, the generator network can generate blurred images of a face. This stage is similar to training a vanilla GAN, in which we train both networks simultaneously. &lt;/p&gt;

&lt;h2 id=&quot;the-training-objective-function&quot;&gt;The training objective function&lt;/h2&gt;

&lt;p&gt;** HANDWRITTEN IMAGE OF THE EQUATION AND EXPLAIN IT **&lt;/p&gt;

&lt;h2 id=&quot;initial-latent-vector-approximation&quot;&gt;Initial latent vector approximation&lt;/h2&gt;

&lt;p&gt;Initial latent vector approximation is a method to approximate a latent vector to optimize the reconstruction of face images. To approximate a latent vector, we have an encoder network. We train the encoder network on the generated images and real images. Once trained, the encoder network will start generating latent vectors from the learned distribution. The training objective function for training the encoder network is the Euclidean distance loss.&lt;/p&gt;

&lt;p&gt;Latent vector optimization&lt;/p&gt;

&lt;p&gt;During latent vector optimization, we optimize the encoder network and the generator network simultaneously. The equation we use for latent vector optimization is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/img/2020-11-04-Face-Aging-cGAN-Exposition/media/image4.png&quot; alt=&quot;http://localhost:5001/assets/98bda784-ced5-4608-8c1d-38477d40cb8f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FR is the face recognition network. This equation indicates that the Euclidean distance between the real image and the reconstructed images should be minimal. In this stage, we try to minimize the distance to maximize identity preservation.&lt;/p&gt;</content><author><name></name></author><summary type="html">Face Aging Using Conditional GAN</summary></entry><entry><title type="html">June Papers Digest</title><link href="https://waleopakunle.com/papers/2020/06/21/june-papers-digest" rel="alternate" type="text/html" title="June Papers Digest" /><published>2020-06-21T00:00:00-05:00</published><updated>2020-06-21T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/06/21/june-papers-digest</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/06/21/june-papers-digest">&lt;h1 id=&quot;latest-in-ai-research---june-2020&quot;&gt;Latest in AI Research - June 2020&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/26FPC5oAdfeFPkQQE/giphy.gif&quot; alt=&quot;R for Research&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;authors-notes&quot;&gt;Author’s Notes&lt;/h2&gt;

&lt;p&gt;Hey there! Here is a collation of what’s new in AI research. I am doing things a bit different this time. I thought it would be more readable if I categorized them based on their context. This would also help you browse to the categories using the Table of Contents.&lt;/p&gt;

&lt;p&gt;You might also notice that there haven’t been posts over the couple of weeks. This is because I plan to transition to monthly posts instead. I am experimenting on this form of delivery to be able to firect more time at delivering on the other sections of this blog such as “how-to’s and data hacks.&lt;/p&gt;

&lt;p&gt;Now that that’s out of the way, here is June’s digest of papers.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  DetectoRS - Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/detectorRS.jpg&quot; alt=&quot;detectorRS&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 54.7% box AP for object detection, 47.1% mask AP for instance segmentation, and 49.6% PQ for panoptic segmentation. The code is made publicly available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;INSTANCE SEGMENTATION&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
        &lt;td&gt;PANOPTIC SEGMENTATION&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/joe-siyuan-qiao/DetectoRS&quot; target=&quot;_blank&quot;&gt;Qiao, Siyuan and Chen, Liang-Chieh and Yuille, Alan&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2006.02334v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Currently SOTA for Object Detection on &lt;a href=&quot;https://paperswithcode.com/sota/object-detection-on-coco&quot; target=&quot;_blank&quot;&gt;COCO test-dev&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Foreground-aware Semantic Representations for Image Harmonization&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/ih_teaser.jpg&quot; alt=&quot;harminization&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Image harmonization is an important step in photo editing to achieve visual consistency in composite images by adjusting the appearances of foreground to make it compatible with background. Previous approaches to harmonize composites are based on training of encoder-decoder networks from scratch, which makes it challenging for a neural network to learn a high-level representation of objects. We propose a novel architecture to utilize the space of high-level features learned by a pre-trained classification network. We create our models as a combination of existing encoder-decoder architectures and a pre-trained foreground-aware deep high-resolution network. We extensively evaluate the proposed method on existing image harmonization benchmark and set up a new state-of-the-art in terms of MSE and PSNR metrics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;INSTANCE SEGMENTATION&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
        &lt;td&gt;PANOPTIC SEGMENTATION&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/joe-siyuan-qiao/DetectoRS&quot; target=&quot;_blank&quot;&gt;Konstantin Sofiiuk, Polina Popenova, Anton Konushin&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2006.00809v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/Attention.png&quot; alt=&quot;attention&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range feature-wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IMAGE SUPER-RESOLUTION&lt;/td&gt;
        &lt;td&gt;SUPER-RESOLUTION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention&quot; target=&quot;_blank&quot;&gt;Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Neural Pose Transfer by Spatially Adaptive Instance Normalization&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/npt.jpg&quot; alt=&quot;npt&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Pose transfer has been studied for decades, in which the pose of a source mesh is applied to a target mesh. Particularly in this paper, we are interested in transferring the pose of source human mesh to deform the target human mesh, while the source and target meshes may have different identity information. Traditional studies assume that the paired source and target meshes are existed with the point-wise correspondences of user annotated landmarks/mesh points, which requires heavy labelling efforts. On the other hand, the generalization ability of deep models is limited, when the source and target meshes have different identities. To break this limitation, we proposes the first neural pose transfer model that solves the pose transfer via the latest technique for image style transfer, leveraging the newly proposed component – spatially adaptive instance normalization. Our model does not require any correspondences between the source and target meshes. Extensive experiments show that the proposed model can effectively transfer deformation from source to target meshes, and has good generalization ability to deal with unseen identities or poses of meshes. Code is available at &lt;a href=&quot;https://github.com/jiashunwang/Neural-Pose-Transfer&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Demo:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/1aQAc-bax9U&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;POSE TRANSFER&lt;/td&gt;
        &lt;td&gt;STYLE TRANSFER&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention&quot; target=&quot;_blank&quot;&gt;Jiashun Wang&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2003.07254v2.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  SEAN - Image Synthesis with Semantic Region-Adaptive Normalization&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/sean.png&quot; alt=&quot;sean&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IMAGE GENERATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/ZPdesu/SEAN&quot; target=&quot;_blank&quot;&gt;Peihao Zhu and Rameen Abdal and Yipeng Qin and Peter Wonka&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1911.12861v2.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;How to use the UI:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/0Vbj9xFgoUw&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  End-to-End Object Detection with Transformers&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/DETR.png&quot; alt=&quot;DETR&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
        &lt;td&gt;PANOPTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/facebookresearch/detr&quot; target=&quot;_blank&quot;&gt;Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.12872v3.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Try it out on &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb&quot;&gt;Colab&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:   Supervised Contrastive Learning - Tensorflow 2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/DETR.png&quot; alt=&quot;DETR&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Detailed report on the paper: &lt;a href=&quot;https://app.wandb.ai/authors/scl/reports/Improving-Image-Classification-with-Supervised-Contrastive-Learning--VmlldzoxMzQwNzE&quot; target=&quot;_blank&quot;&gt;Wandb&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;CALIBRATION&lt;/td&gt;
        &lt;td&gt;CALIBRATION&lt;/td&gt;
        &lt;td&gt;DATA AUGMENTATION&lt;/td&gt;
        &lt;td&gt;IMAGE CLASSIFICATION&lt;/td&gt;
        &lt;td&gt;SELF-SUPERVISED LEARNING&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/sayakpaul/Supervised-Contrastive-Learning-in-TensorFlow-2&quot; target=&quot;_blank&quot;&gt;Shweta Shaw and Sayak Paul&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.11362v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Try it out on &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb&quot; target=&quot;_blank&quot;&gt;Colab&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Cascaded Text Generation with Markov Transformers&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/cascaded-generation-fastest.gif&quot; alt=&quot;cascaded-generation-fastest&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;MACHINE TRANSLATION&lt;/td&gt;
        &lt;td&gt;TEXT GENERATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/harvardnlp/cascaded-generation&quot; target=&quot;_blank&quot;&gt;Yuntian Deng and Alexander M. Rush&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2006.01112v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Language Models are Few-Shot Learners&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;COMMON SENSE REASONING&lt;/td&gt;
        &lt;td&gt;COREFERENCE RESOLUTION&lt;/td&gt;
        &lt;td&gt;DOMAIN ADAPTATION&lt;/td&gt;
        &lt;td&gt;FEW-SHOT LEARNING&lt;/td&gt;
        &lt;td&gt;LANGUAGE MODELLING&lt;/td&gt;
        &lt;td&gt;NATURAL LANGUAGE INFERENCE&lt;/td&gt;
        &lt;td&gt;QUESTION ANSWERING&lt;/td&gt;
        &lt;td&gt;SENTENCE COMPLETION&lt;/td&gt;
        &lt;td&gt;UNSUPERVISED MACHINE TRANSLATION&lt;/td&gt;
        &lt;td&gt;WORD SENSE DISAMBIGUATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/openai/gpt-3&quot; target=&quot;_blank&quot;&gt;OpenAI&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;This repository has been archived by the owner at this time. It is now read-only.&lt;/span&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.14165v3.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Currently  SOTA for &lt;a href=&quot;https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word&quot; target=&quot;_blank&quot;&gt;Language Modelling on Penn Treebank&lt;/a&gt; (Word Level) (using extra training data).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  FastSpeech 2 - Fast and High-Quality End-to-End Text to Speech&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-21-june-papers/DETR.png&quot; alt=&quot;DETR&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Advanced text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs during training and use predicted values during inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of full end-to-end training and even faster inference than FastSpeech. Experimental results show that 1) FastSpeech 2 and 2s outperform FastSpeech in voice quality with much simplified training pipeline and reduced training time; 2) FastSpeech 2 and 2s can match the voice quality of autoregressive models while enjoying much faster inference speed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;SPEECH SYNTHESIS&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/dathudeptrai/TensorflowTTS&quot; target=&quot;_blank&quot;&gt;Dathudeptrai&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2006.04558v2.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Currently  SOTA for &lt;a href=&quot;https://paperswithcode.com/sota/speech-synthesis-on-north-american-english&quot; target=&quot;_blank&quot;&gt;Speech Synthesis on North American English&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;environments-and-frameworks&quot;&gt;Environments and Frameworks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Acme - A Research Framework for Distributed Reinforcement Learning&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;![acme]/images/2020-06-21-june-papers/acme.png)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Deep reinforcement learning has led to many recent-and groundbreaking-advancements. However, these advances have often come at the cost of both the scale and complexity of the underlying RL algorithms. Increases in complexity have in turn made it more difficult for researchers to reproduce published RL algorithms or rapidly prototype ideas. To address this, we introduce Acme, a tool to simplify the development of novel RL algorithms that is specifically designed to enable simple agent implementations that can be run at various scales of execution. Our aim is also to make the results of various RL algorithms developed in academia and industrial labs easier to reproduce and extend. To this end we are releasing baseline implementations of various algorithms, created using our framework. In this work we introduce the major design decisions behind Acme and show how these are used to construct these baselines. We also experiment with these agents at different scales of both complexity and computation-including distributed versions. Ultimately, we show that the design decisions behind Acme lead to agents that can be scaled both up and down and that, for the most part, greater levels of parallelization result in agents with equivalent performance, just faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;More Information:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://deepmind.com/research/publications/Acme&quot; target=&quot;_blank&quot;&gt;Deepmind Blogpost&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Examples:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/deepmind/acme/tree/master/examples&quot; target=&quot;_blank&quot;&gt;DeepMind GitHub&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/deepmind/acme&quot; target=&quot;_blank&quot;&gt;DeepMind&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2006.00979v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Adversarial Colorization Of Icons Based On Structure And Color Conditions&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/colorization-of-icons.png&quot; alt=&quot;colorization_of_icons&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours, whereas our system colorizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on our collected icon dataset. One condition requires the generated image and the drawn contour to possess a similar contour, while the other anticipates the image and the referenced icon to be similar in color style. Accordingly, the generator takes a contour image and a man-made icon image to colorize the contour, and then the discriminators determine whether the result fulfills the two conditions. The trained network is able to colorize icons demanded by designers and greatly reduces their workload. For the evaluation, we compared our dual conditional GAN to several state-of-the-art techniques. Experiment results demonstrate that our network is over the previous networks. Finally, we will provide the source code, icon dataset, and trained network for public use.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;COLORIZATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/jxcodetw/Adversarial-Colorization-Of-Icons-Based-On-Structure-And-Color-Conditions&quot; target=&quot;_blank&quot;&gt;Pytorch&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1910.05253v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;</content><author><name>Wale</name></author><summary type="html">Latest in AI Research - June 2020</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/2020-06-13-lpiair-wk21/oneshotSegmentation.gif" /><media:content medium="image" url="https://waleopakunle.com/images/2020-06-13-lpiair-wk21/oneshotSegmentation.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Latest Papers In Ai Research Wk21</title><link href="https://waleopakunle.com/papers/2020/06/12/latest-papers-in-ai-research-wk21" rel="alternate" type="text/html" title="Latest Papers In Ai Research Wk21" /><published>2020-06-12T00:00:00-05:00</published><updated>2020-06-12T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/06/12/latest-papers-in-ai-research-wk21</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/06/12/latest-papers-in-ai-research-wk21">&lt;h1 id=&quot;latest-in-ai-research---week-21&quot;&gt;Latest in AI Research - Week 21&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/26FPC5oAdfeFPkQQE/giphy.gif&quot; alt=&quot;R for Research&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hey there! Here is a collation of what’s new in AI research.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Adversarial Colorization Of Icons Based On Structure And Color Conditions&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/colorization-of-icons.png&quot; alt=&quot;colorization_of_icons&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours, whereas our system colorizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on our collected icon dataset. One condition requires the generated image and the drawn contour to possess a similar contour, while the other anticipates the image and the referenced icon to be similar in color style. Accordingly, the generator takes a contour image and a man-made icon image to colorize the contour, and then the discriminators determine whether the result fulfills the two conditions. The trained network is able to colorize icons demanded by designers and greatly reduces their workload. For the evaluation, we compared our dual conditional GAN to several state-of-the-art techniques. Experiment results demonstrate that our network is over the previous networks. Finally, we will provide the source code, icon dataset, and trained network for public use.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;COLORIZATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/jxcodetw/Adversarial-Colorization-Of-Icons-Based-On-Structure-And-Color-Conditions&quot; target=&quot;_blank&quot;&gt;Pytorch&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1910.05253v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  DeepFaceFlow - In-the-wild Dense 3D Facial Motion Estimation&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/deepFaceFlow.png&quot; alt=&quot;deepFaceFlow&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Dense 3D facial motion capture from only monocular in-the-wild pairs of RGB images is a highly challenging problem with numerous applications, ranging from facial expression recognition to facial reenactment. In this work, we propose DeepFaceFlow, a robust, fast, and highly-accurate framework for the dense estimation of 3D non-rigid facial flow between pairs of monocular images. Our DeepFaceFlow framework was trained and tested on two very large-scale facial video datasets, one of them of our own collection and annotation, with the aid of occlusion-aware and 3D-based loss function. We conduct comprehensive experiments probing different aspects of our approach and demonstrating its improved performance against state-of-the-art flow and 3D reconstruction methods. Furthermore, we incorporate our framework in a full-head state-of-the-art facial video synthesis method and demonstrate the ability of our method in better representing and capturing the facial dynamics, resulting in a highly-realistic facial video synthesis. Given registered pairs of images, our framework generates 3D flow maps at ~60 fps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;3D RECONSTRUCTION&lt;/td&gt;
        &lt;td&gt;FACIAL EXPRESSION RECOGNITION&lt;/td&gt;
        &lt;td&gt;MOTION CAPTURE&lt;/td&gt;
        &lt;td&gt;MOTION ESTIMATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Implementation:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;The group has only released a peper as at the time of writing this post. The dataset used and code implementations are still in the works and you can [stay updated here.](https://github.com/mrkoujan/DeepFaceFlow)&lt;/span&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.07298v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: EfficientPS - Efficient Panoptic Segmentation&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Demo:&lt;/p&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/mvcNEQcxo3w&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below is a short video that summarises this awesome piece of work. You will get a visual understanding of what image segmentation is about, the architecture used to achieve this, as well as see it in action:&lt;/p&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/j11mvFqFmfA&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;INSTANCE SEGMENTATION&lt;/td&gt;
        &lt;td&gt;PANOPTIC SEGMENTATION&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Implementation:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt; This piece is licenced under the GPL-3 License. It is currently the state of the art (SOTA) for Panoptic Segmentation based on the _Mapillary val_ dataset. Due to the inherent nature of the GPL-3 License, you need to contact the authors to get the source code. However, you can read a short article on its usage, architecture  as well as see several demos [here](http://panoptic.cs.uni-freiburg.de/) and see the License [here](https://github.com/DeepSceneSeg/EfficientPS).&lt;/span&gt;
&lt;/div&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;Links in alert boxes do not open into a new window... yet. Work in progress.&lt;/span&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.02307v2.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Current SOTA for &lt;a href=&quot;https://paperswithcode.com/sota/panoptic-segmentation-on-mapillary-val&quot; target=&quot;_blank&quot;&gt;Panoptic Segmentation on Mapillary val&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Order-Aware Generative Modeling Using the 3D-Craft Dataset&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/voxelcnn.png&quot; alt=&quot;voxelcnn&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;HOUSE GENERATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/facebookresearch/VoxelCNN&quot; target=&quot;_blank&quot;&gt;Facebook Research’s Pytorch Code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  U&lt;sup&gt;2&lt;/sup&gt;-Net - Going Deeper with Nested U-Structure for Salient Object Detection&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/U2Net.png&quot; alt=&quot;U2Net&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: In this paper, we design a simple yet powerful deep network architecture, U&lt;sup&gt;2&lt;/sup&gt;-Net, for salient object detection (SOD). The architecture of our U&lt;sup&gt;2&lt;/sup&gt;-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U2-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U2-Net† (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: &lt;a href=&quot;https://github.com/NathanUA/U-2-Net.&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IMAGE CLASSIFICATION&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
        &lt;td&gt;SALIENT OBJECT DETECTION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/NathanUA/U-2-Net&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.09007v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This code has been used in &lt;a href=&quot;https://github.com/cyrildiagne/ar-cutpaste&quot; target=&quot;_blank&quot;&gt;this AR Project by Cyril&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Surfboard: Audio Feature Extraction for Modern Machine Learning&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/surfboard.png&quot; alt=&quot;surfboard&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We introduce Surfboard, an open-source Python library for extracting audio features with application to the medical domain. Surfboard is written with the aim of addressing pain points of existing libraries and facilitating joint use with modern machine learning frameworks. The package can be accessed both programmatically in Python and via its command line interface, allowing it to be easily integrated within machine learning workflows. It builds on state-of-the-art audio analysis packages and offers multiprocessing support for processing large workloads. We review similar frameworks and describe Surfboard’s architecture, including the clinical motivation for its features. Using the mPower dataset, we illustrate Surfboard’s application to a Parkinson’s disease classification task, highlighting common pitfalls in existing research. The source code is opened up to the research community to facilitate future audio research in the clinical domain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;AUDIO FEATURE EXTRACTION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/novoic/surfboard&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.08848v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Single-Stage Semantic Segmentation from Image Labels&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-06-13-lpiair-wk21/oneshotSegmentation.gif&quot; alt=&quot;oneshotSegmentation&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage − training one segmentation network on image labels − which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/visinf/1-stage-wseg&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.08104v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Flowtron - an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://nv-adlr.github.io/images/flowtron_logo.png&quot; alt=&quot;Flowtron&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at [Github]](https://github.com/NVIDIA/flowtron)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;TEXT-TO-SPEECH SYNTHESIS&lt;/td&gt;
        &lt;td&gt;STYLE TRANSFER&lt;/td&gt;
        &lt;td&gt;SPEECH SYNTHESIS&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/flowtron&quot; target=&quot;_blank&quot;&gt;NVIDIA Pytorch code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.05957v2.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Variational quantum Gibbs state preparation with a truncated Taylor series&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: The preparation of quantum Gibbs state is an essential part of quantum computation and has wide-ranging applications in various areas, including quantum simulation, quantum optimization, and quantum machine learning. In this paper, we propose variational hybrid quantum-classical algorithms for quantum Gibbs state preparation. We first utilize a truncated Taylor series to evaluate the free energy and choose the truncated free energy as the loss function. Our protocol then trains the parameterized quantum circuits to learn the desired quantum Gibbs state. Notably, this algorithm can be implemented on near-term quantum computers equipped with parameterized quantum circuits. By performing numerical experiments, we show that shallow parameterized circuits with only one additional qubit can be trained to prepare the Ising chain and spin chain Gibbs states with a fidelity higher than 95%. In particular, for the Ising chain model, we find that a simplified circuit ansatz with only one parameter and one additional qubit can be trained to realize a 99% fidelity in Gibbs state preparation at inverse temperatures larger than 2.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;QUANTUM MACHINE LEARNING&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/Quantum&quot; target=&quot;_blank&quot;&gt;Github, Chinese&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;http://translate.google.com/translate?hl=en&amp;amp;sl=auto&amp;amp;tl=en&amp;amp;u=https%3A%2F%2Fgithub.com%2FPaddlePaddle%2FQuantum&quot; target=&quot;_blank&quot;&gt;Github, English Translated&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.08797v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title: Neural Controlled Differential Equations for Irregular Time Series&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/sbcIKugElZ4&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of \emph{controlled differential equations}. The resulting \emph{neural controlled differential equation} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IRREGULAR TIME SERIES&lt;/td&gt;
        &lt;td&gt;TIME SERIES&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/patrick-kidger/NeuralCDE&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.08926v1.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;other-ways-to-catch-up-on-research&quot;&gt;Other ways to catch up on research&lt;/h2&gt;

&lt;h3 id=&quot;henry-ai-labs&quot;&gt;Henry AI Labs&lt;/h3&gt;

&lt;p&gt;Truth be told, I thought to create this section because Henry does what he does exceedingly well. In fact, I look forward to his posts every week. Here’s the one he released for the corresponding 21&lt;sup&gt;st&lt;sup&gt; week of the year 2020:&lt;/sup&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/BA2Ft6YlG4g&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;p&gt;Here he describes what could potentially help me automate my “paper summary” adventures:&lt;/p&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/5WJZgSwRUSQ&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;h3 id=&quot;two-minute-papers&quot;&gt;Two minute papers&lt;/h3&gt;

&lt;p&gt;If you don’t know about Dr Károly Zsolnai-Fehér, let me introduce you to a master storyteller. He (and his team) do an outstanding job on all their videos. He makes catching up on AI developments a breeze, and very enjoyable. For example, have a look at how he talks about OpenAI’s Jukebox AI.&lt;/p&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/6oQ0Obi14rM&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;p&gt;That’s it folks. These are were the trendy papers for the 21&lt;sup&gt;st&lt;/sup&gt; week of 2020.&lt;/p&gt;

&lt;p&gt;I am particularly happy that I had found a lot more demos to append to the papers than all my previous posts.&lt;/p&gt;

&lt;p&gt;These researchers are the absolute best, and you all have been an excellent audience. You all have my heartfelt appreciation.&lt;/p&gt;

&lt;p&gt;Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below!&lt;/p&gt;

&lt;p&gt;Until next time, stay safe!&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">Latest in AI Research - Week 21</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/2020-06-13-lpiair-wk21/oneshotSegmentation.gif" /><media:content medium="image" url="https://waleopakunle.com/images/2020-06-13-lpiair-wk21/oneshotSegmentation.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lumos Maxima</title><link href="https://waleopakunle.com/news/2020/05/23/lumos-maxima" rel="alternate" type="text/html" title="Lumos Maxima" /><published>2020-05-23T00:00:00-05:00</published><updated>2020-05-23T00:00:00-05:00</updated><id>https://waleopakunle.com/news/2020/05/23/lumos-maxima</id><content type="html" xml:base="https://waleopakunle.com/news/2020/05/23/lumos-maxima">&lt;h1 id=&quot;previously-in-the-ai-field&quot;&gt;Previously in the AI field…&lt;/h1&gt;

&lt;p&gt;Hey folks, I am back with a recap of last week in AI! This week, I decided to add on a new section, based on feedback from a fellow data scientist. Be sure to check out the “Job” section for a highlight of some opportunities that are open for machine learning practitioners.&lt;/p&gt;

&lt;p&gt;Working on feedback from last week, all links now open in new tabs. I will be implementing that for all links going forward. Thanks for the feedback, and I hope you enjoy a better reading experience.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;T.O.C&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#new-open-source-tools&quot;&gt;New Open Source Tools&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#cool-projects&quot;&gt;Cool Projects&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#new-events&quot;&gt;New Events&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#new-datasets&quot;&gt;New Datasets&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#resources&quot;&gt;Resources&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#ai-developments&quot;&gt;AI Developments&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#jobs&quot;&gt;Jobs&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;new-open-source-tools&quot;&gt;New Open Source Tools&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/26BRzozg4TCBXv6QU/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tensorflow announced the release of BigTransfer (BiT); a compilation of pre-trained models for easy and effective transfer learning for vision with TF-Hub! For a detailed walkthrough on how to utilize these models and customize it to your computer vision needs, checkout the &lt;a href=&quot;https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html&quot; target=&quot;_blank&quot;&gt;official tensorflow blog post&lt;/a&gt; or try it out for yourselves in &lt;a href=&quot;https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb&quot; target=&quot;_blank&quot;&gt;colab&lt;/a&gt;. If you’re so inclined, checkout their paper on &lt;a href=&quot;https://arxiv.org/pdf/1912.11370&quot; target=&quot;_blank&quot;&gt;Graphical Visual Representation Learning&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/flowtron&quot; target=&quot;_blank&quot;&gt;Flowtron&lt;/a&gt;: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/README.md&quot; target=&quot;_blank&quot;&gt;Transformers&lt;/a&gt; (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over thousands of pretrained models in 100+ languages and deep interoperability between PyTorch &amp;amp; TensorFlow 2.0.
    &lt;ul&gt;
      &lt;li&gt;Features:
        &lt;ul&gt;
          &lt;li&gt;High performance on NLU and NLG task&lt;/li&gt;
          &lt;li&gt;State-of-the-art NLP for everyone&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cmusatyalab/OpenTPOD&quot; target=&quot;_blank&quot;&gt;OpenTPOD&lt;/a&gt;
OpenTPOD is an all-in-one open-source tool for nonexperts to create custom deep neural network object detectors. It is designed to lower the barrier of entry and facilitates the end-to-end authoring workflow of custom object detection using state-of-art deep learning methods. It provides several features via an easy-to-use web interface, including; training data management, data annotation through seamless integration, one-click training / fine-tuning of object detection deep neural networks, one-click model export for interface with Tensorflow Serving, and extensible architecture for easy addition of new deep neural network architectures…&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/UHnNLrD6jTo&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cool-projects&quot;&gt;Cool Projects&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/RyLtUMBdogHvO/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Open Source:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://olivia-ai.org/&quot; target=&quot;_blank&quot;&gt;Olivia&lt;/a&gt;: an open-source chatbot built in Golang using Machine Learning technologies. Its goal is to provide a free and open-source alternative to big services like DialogFlow. You can chat with her by speaking (STT) or writing, she replies with a text message but you can enable her voice (TTS). It currently supports 7 languages and you can try it out &lt;a href=&quot;https://olivia-ai.org/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;
Olivia is organized in modules to facilitate the addition of new capabilities. These modules can be written in Go to execute multiple tasks. The project is entirely open-source from the website to the backend. This gives you the ability to build your own chatbot and contribute to Olivia. Don’t you just love open source?&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/research/publications/groknet-unified-computer-vision-model-trunk-and-embeddings-for-commerce&quot; target=&quot;_blank&quot;&gt;Groknet&lt;/a&gt;: Earlier this week, Facebook announced their deployment of a universal &lt;a href=&quot;https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences/&quot; target=&quot;_blank&quot;&gt;computer vision system designed for shopping&lt;/a&gt;. It can identify fine-grained product attributes across billions of photos — in different categories, such as fashion, auto, and home decor.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/FelipeMarcelino/2048-Gym&quot; target=&quot;_blank&quot;&gt;Using reinforcement learning algorithms to play the game 2048&lt;/a&gt;
This repository is a project about using DQN(Q-Learning) to play the Game 2048 and accelarate and accelerate the environment using Numba). The algorithm used is from Stable Baselines, and the environment is a custom Open AI env.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;new-events&quot;&gt;New Events&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/40KXzKllSl6ve/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.knime.com/knime-analytics-platform-for-data-scientists-webinar-kd-americas?utm_source=kdnuggets&amp;amp;utm_medium=banner&amp;amp;utm_campaign=knime-intro-webinar&quot; target=&quot;_blank&quot;&gt;ETL and Advanced Machine Learning - Open Source, No Code Required&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://hai.stanford.edu/events/covid-ai-road-ahead&quot; target=&quot;_blank&quot;&gt;The Stanford Institute for Human-Centered Artificial Intelligence’s COVID + AI: The Road Ahead&lt;/a&gt;:
Two months into the COVID-19 pandemic, we see curves flattening and cities lifting shelter restrictions. By no means is this pandemic over; and yet, policymakers, researchers, medical practitioners, and industry are starting to grapple with how we operate in a post-COVID world. What is the path forward, economically, medically, and culturally?
Scholars from across disciplines will discuss their research using AI, data science and/or informatics to help us understand how we emerge from this crisis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Live Virtual Event Details&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Date: June 01, 2020 - 09:15am–12:30pm&lt;/p&gt;

&lt;p&gt;Register &lt;a href=&quot;https://www.eventbrite.com/e/covid-ai-the-road-ahead-tickets-105468430916&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cvpr2020.thecvf.com/&quot; target=&quot;_blank&quot;&gt;CVPR 2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CVPR is the premier annual computer vision event comprising the main conference and several co-located workshops and short courses. With its high quality and low cost, it provides an exceptional value for students, academics and industry researchers.
CVPR 2020 will take place virtually from June 14 to June 19th.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/wwdc20/&quot; target=&quot;_blank&quot;&gt;Apple WWDC 2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On June 22, WWDC20 takes off. Get ready for the first global, all-online WWDC by downloading the Apple Developer app to stay notified on all the latest news, with updates for events and sessions. And there’s a lot more to come — starting with the first-ever Swift Student Challenge.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;new-datasets&quot;&gt;New Datasets&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/7rSqa74vJW0Te/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/hsankesara/flickr-image-dataset&quot; target=&quot;_blank&quot;&gt;Flickr 30k Dataset&lt;/a&gt;: The Flickr 30k dataset has over 30,000 images, and each image is labeled with different captions. This dataset is used to build an image caption generator. And this dataset is an upgraded version of Flickr 8k used to build more accurate models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Possible project idea&lt;/strong&gt;:  You can train a deep learning model that is capable of analysing and extracting features from the image and generate as english sentence that describes the image. Next time when your friend send you an image and goes, “Caption this”, you can ask your model what it thinks and get back to your friend with the machine caption.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/katanaml/katana-assistant/blob/master/mlbackend/intents.json&quot; target=&quot;_blank&quot;&gt;Chatbot Intents Dataset&lt;/a&gt;
The dataset for a chatbot is a JSON file that has disparate tags like goodbye, greetings, pharmacy_search, hospital_search, etc. Every tag has a list of patterns that a user can ask, and the chatbot will respond according to that pattern. The dataset is perfect for understanding how chatbot data works.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Possible Project Idea&lt;/strong&gt;: You can build a chatbot or understand the working of a chatbot by modifying and expanding the data with your observations. To build a Chatbot of your own, you need to have a good knowledge of Natural language processing concepts.
&lt;a href=&quot;https://dzone.com/articles/python-chatbot-project-build-your-first-python-pro&quot; target=&quot;_blank&quot;&gt;This article&lt;/a&gt; from DZone could help you get started with that.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/shwetabh123/mall-customers&quot; target=&quot;_blank&quot;&gt;Mall Customers Dataset&lt;/a&gt;
The Mall customers dataset holds the details about people visiting the mall. The dataset has an age, customer id, gender, annual income, and spending score. It gains insights from the data and divides the customers into different groups based on their behaviors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Possible Project Idea&lt;/strong&gt;: Segment the customers based on their gender, age, interest. It is useful in customized marketing. Customer segmentation is an important practice of dividing customers based on individual groups that are similar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://data-flair.training/blogs/r-data-science-project-customer-segmentation/&quot; target=&quot;_blank&quot;&gt;This article&lt;/a&gt; could help you get started with customer segmentation with R or &lt;a href=&quot;https://towardsdatascience.com/customer-segmentation-analysis-with-python-6afa16a38d9e&quot; target=&quot;_blank&quot;&gt;this one&lt;/a&gt;, if you speak the language of the gods.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.google.com/youtube8m/&quot; target=&quot;_blank&quot;&gt;Youtube 8M Dataset&lt;/a&gt;
The youtube 8M dataset is a large scale labeled video dataset that has 6.1 million Youtube video ids, 350,000 hours of video, 2.6 billion audio/visual features, 3862 classes, and 3 avg labels per video. It is used for video classification purposes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Project Idea&lt;/strong&gt;: Video classification can be done by using the dataset, and the model can describe what video is about. A video takes a series of inputs to classify in which category the video belongs. This would be a rather involved project, but if you are interested, &lt;a href=&quot;https://arxiv.org/pdf/1609.08675v1.pdf&quot; target=&quot;_blank&quot;&gt;this paper&lt;/a&gt; could help get you started, along with the following implementations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/google/youtube-8m&quot; target=&quot;_blank&quot;&gt;google/youtube-8m&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/AKASH2907/Content-based-Video-Recommendation&quot; target=&quot;_blank&quot;&gt;AKASH2907/Content-based-Video-Recommendation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/X1LTQuH3i8fza/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Papers I read
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://language-play.github.io/&quot; target=&quot;_blank&quot;&gt;Grounding Language in Play: A scalable approach for controlling robots with natural language&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.10825v1.pdf&quot; target=&quot;_blank&quot;&gt;Instance-aware Image Colorization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Articles:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.nesta.org.uk/report/chinas-approach-to-ai-ethics/&quot; target=&quot;_blank&quot;&gt;AI Ethics in China&lt;/a&gt;
        &lt;blockquote&gt;
          &lt;p&gt;“… It is therefore surprising that the western misconception that China lacks debate around AI ethics seems to prevail, when in fact: i) existing Chinese AI principles largely align with global ones; ii) Chinese discussions enjoy unprecedented government support; and iii) the country is already investigating the technical and social implementation of these principles by exploring how they interact with its distinct cultural heritage.
In this essay I explore AI ethics in China through the lens of some of the key topics discussed in depth in the other essays on this collection – education, healthcare, smart cities and social credit systems – to consider which, if any, ethical issues are unique to China and which should be seen as global concerns.” - Danit Gal, &lt;em&gt;Technology advisor to the UN Secretary General’s High-level Panel on Digital Cooperation and associate fellow at the Leverhulme Centre for the Future of Intelligence at the University&lt;/em&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://blogs.microsoft.com/ai/azure-responsible-machine-learning/?utm_source=msr-social&quot;&gt;Microsoft responsible machine learning capabilities build trust in AI systems, developers say&lt;/a&gt;. Read this article by John Roach to find out what the industry, particularly Micrsosft, is doing regarding ethical AI and model interpretability. Going further in the rabbit hole, I discovered they now have modules that cater for interpretability which I didn’t notice when I was using the service last year. You can check that out in the official &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml&quot; target=&quot;_blank&quot;&gt;Azure Machine Learning docs&lt;/a&gt;.
For an example of how to use this, checkout &lt;a href=&quot;https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model?ocid=AID2463683&amp;amp;wt.mc_id=ai-c9-sejuare&quot; target=&quot;_blank&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://insights.sei.cmu.edu/sei_blog/2020/05/three-risks-in-building-machine-learning-systems.html&quot; target=&quot;_blank&quot;&gt;Three Risks in Building Machine Learning Systems&lt;/a&gt; , Benjamin Cohen&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Read how Waymo is using GANs in modules of their self-driving software, to simulate autonomous vehicle camera and sensor data in this article by &lt;a href=&quot;https://venturebeat.com/2020/05/20/waymo-is-using-ai-to-simulate-autonomous-vehicle-camera-data/&quot; target=&quot;_blank&quot;&gt;Kyle Wiggers on Venturebeat&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;blockquote&gt;
      &lt;p&gt;” In simulation, when a trajectory of a self-driving car and other agents (e.g. other cars, cyclists, and pedestrians) changes, the system generates realistic visual sensor data that helps us model the scene in the updated environment … “   - A Waymo spokesperson.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/sciforce/is-ai-democratization-a-real-thing-12957ca18534&quot; target=&quot;_blank&quot;&gt;Is AI democratization a real thing?&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/this-will-change-the-way-you-look-at-gans-9992af250454&quot; target=&quot;_blank&quot;&gt;This Will Change the Way You Look at GANs&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/@msundarv/entity-linking-a-primary-nlp-task-for-information-extraction-22f9d4b90aa8&quot; target=&quot;_blank&quot;&gt;Entity linking: A primary NLP task for Information Extraction&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/top-10-nice-to-have-data-science-libraries-d155196710ef&quot; target=&quot;_blank&quot;&gt;Top 10 Nice-To-Have Data Science Libraries&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/the-post-grad-survival-guide/building-a-business-from-a-bedroom-98-130-and-11-months-in-7a55774b2a0&quot; target=&quot;_blank&quot;&gt;Building a Business From a Bedroom, $98,130 and 11-Months In&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Free Books:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://link.springer.com/content/pdf/10.1007%2F978-0-387-84858-7.pdf&quot; target=&quot;_blank&quot;&gt;The Elements of Statistical Learning&lt;/a&gt;, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://d2l.ai/d2l-en.pdf&quot; target=&quot;_blank&quot;&gt;Dive Into Deep Learning&lt;/a&gt;
You can also download the entire book in &lt;a href=&quot;https://d2l.ai/chapter_installation/index.html&quot; target=&quot;_blank&quot;&gt;notebook format&lt;/a&gt;, so you can run it locally or on &lt;a href=&quot;https://d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html&quot; target=&quot;_blank&quot;&gt;AWS Sagemaker&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paid Books:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://shop.oreilly.com/product/0636920268505.do&quot; target=&quot;_blank&quot;&gt;High Performance Python, 2nd Edition by Micha Gorelick &amp;amp; Ian Ozsvald&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Notes:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://ermongroup.github.io/cs228-notes/&quot;&gt;Notes from an Introductory course on probablistic graphical models&lt;/a&gt;. They are based on the Stanforf course, CS228.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1DuzClGB5T79yPk-VmETpOVeginpO8gbx?usp=sharing&quot;&gt;My colab notebook&lt;/a&gt; exploring the &lt;a href=&quot;https://github.com/jxcodetw/Adversarial-Colorization-Of-Icons-Based-On-Structure-And-Color-Conditions&quot; target=&quot;_blank&quot;&gt;“Adversarial-Colorization-Of-Icons-Based-On-Structure-And-Color-Conditions”&lt;/a&gt; project. I merely provide comments on the code and an intuition of how to minimally customize the set up. This group of researchers present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours and then, the system colourizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on the collected icon dataset. If you are familiar with Python wx, you can get the user interface which was built with Python Wx &lt;a href=&quot;https://drive.google.com/file/d/1ckhFl6Pmx6ltnYd9-geeFQU1z5F3uW-L/view?usp=sharing&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. You can either get the &lt;a href=&quot;https://drive.google.com/file/d/1B8GjpBPpIxpAEy7cf8xpmA_ESy3CQXLh/view?usp=sharing&quot; target=&quot;_blank&quot;&gt;preprocessed data&lt;/a&gt;, or the &lt;a href=&quot;https://drive.google.com/open?id=1qtfA_fOCVzU8bigJut1LYKOOlTI20f9d&quot; target=&quot;_blank&quot;&gt;unprocessed data&lt;/a&gt; if you would like to train the model yourself. I’m at the phase of understanding and customizing the UI. Perhaps I will be able to add a new feature. &lt;em&gt;fingers-crossed&lt;/em&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/15YgjvL85GJpPfYOTXz9iURkLXqBvKn41?usp=sharing&quot; target=&quot;_blank&quot;&gt;My colab notebook&lt;/a&gt; on building a food classifier with Keras. The model is suffering from acute overfitting as you can see from the loss logs. This was initially meant to be a video tutorial series on how to deploy deep learning models in production, but I wont be completing the video shoot until I get the accuracy up a lot higher than it currently is. This week, I will be tuning hyperparameters and retraining the model. Problem is, it takes too friggin long! Just training on 4 classes took an hour per epoch. I’ll have to look into options to reduce training time as well. Once that headache is done, I’ll be sharing how you can put the model into a flask application, deploy its docker image and manage it with kubernetes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cool Tutorials
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ&quot; target=&quot;_blank&quot;&gt;CMU Neural Nets for NLP 2020 (24 Video Playlist)&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://youtu.be/CyHMukDEllg&quot; target=&quot;_blank&quot;&gt;AI Saturday Lagos - ML Track, Week 10 : Nonlinear Modeling, Cross Validation and Regularization&lt;/a&gt; by the amazing Tejumade Afonja.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/CyHMukDEllg&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/V1GVx4mQ9gQ&quot; target=&quot;_blank&quot;&gt;AI Saturday Lagos - DL Track, Week 10 : Natural Language Processing &amp;amp; RNNs&lt;/a&gt; by the awesome Oreva Ahia.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/V1GVx4mQ9gQ&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/UqPE_OejWsQ&quot; target=&quot;_blank&quot;&gt;Data Engineering 101 by Damilola Ojo, with Data Science Nigeria&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/UqPE_OejWsQ&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/-wr_lNd6y84&quot; target=&quot;_blank&quot;&gt;Docker &amp;amp; Kubernetes Demo in 15 mins: Package &amp;amp; Deploy Your First App&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/-wr_lNd6y84&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Nuggets:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=-wr_lNd6y84&amp;amp;feature=youtu.be&quot; target=&quot;_blank&quot;&gt;Deploying a Go application with Docker images on Kubernetes&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/better-programming/the-only-step-by-step-guide-youll-need-to-build-a-web-scraper-with-python-e79066bd895a&quot;&gt;The Only Step-by-Step Guide You’ll Need to Build a Web Scraper With Python&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://amitness.com/2020/05/data-augmentation-for-nlp/&quot; target=&quot;_blank&quot;&gt;A Visual Survey of Data Augmentation in NLP&lt;/a&gt;
        &lt;blockquote&gt;
          &lt;p&gt;“…I was curious if there were attempts at developing augmentation techniques for NLP and explored the existing literature. In this post, I will give an overview of the current approaches being used for text data augmentation based on my findings..” - Amit Chaudhary, &lt;em&gt;Machine Learning Engineer at Fusemachines&lt;/em&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/everything-you-need-to-know-about-auto-deeplab-googles-latest-on-segmentation-181425d17cd5&quot; target=&quot;_blank&quot;&gt;Everything you need to know about Auto-Deeplab: Google’s latest on Segmentation&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/towards-artificial-intelligence/five-cool-python-libraries-for-data-science-7f1fce402b90&quot; target=&quot;_blank&quot;&gt;Five Cool Python Libraries for Data Science&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://medium.com/deeptek/federated-learning-for-medical-ai-65d0daeafe8b&quot; target=&quot;_blank&quot;&gt;Federated Learning for Medical AI&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/all-about-python-list-comprehension-14dd979ec0d1&quot; target=&quot;_blank&quot;&gt;Python List Comprehension&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ai-developments&quot;&gt;AI Developments&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/2f41Z7bhKGvbG/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Watch how OpenAI and Microsoft apply OpenAI’s language model to achieve code generation.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/y5-wzgIySb4&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Air Force ‘Skyborg’ AI powered drones to take flight alongside warplanes by 2023, because apparently ‘Skynet’ was copyrighted.
    &lt;blockquote&gt;
      &lt;p&gt;The  U.S. Air Force is soliciting the aerospace industry to provide flyable “Skyborg” drones by 2023. The drones will be powered by artificial intelligence, capable of taking off, landing, and performing missions on their own.
Skyborg will not only free manned pilots from dangerous and dull missions but allow the Air Force to add legions of new, unpiloted, cheap planes…&lt;a href=&quot;https://www.popularmechanics.com/military/aviation/a32631612/skyborg-drones-2023/?source=nl&amp;amp;utm_source=nl_pop&amp;amp;utm_medium=email&amp;amp;date=052220&amp;amp;utm_campaign=nl20376989&amp;amp;src=nl&amp;amp;utm_source=reddit.com&quot; target=&quot;_blank&quot;&gt;Read more&lt;/a&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.techradar.com/news/ai-vs-ai-the-next-battle-in-the-cyber-arms-race?utm_source=ONTRAPORT-email-broadcast&amp;amp;utm_medium=ONTRAPORT-email-broadcast&amp;amp;utm_term=&amp;amp;utm_content=Data+Science+Insider%3A+May+22nd%2C+2020&amp;amp;utm_campaign=23052020&quot; target=&quot;_blank&quot;&gt;AI vs. Cyberattacks&lt;/a&gt;
It’s no news that hackers are now taking advantage of machine learning.
Cyberattacks have become part of the modern way of life, with companies investing billions of dollars to protect their data from hackers. As ways to secure systems have become more advanced and innovative, so too have the means of attack. Following the 2017 ‘worms’ attack by WannaCry and NotPetya, which bypassed firewalls and crippled thousands of organisations, companies have turned to AI in order to provide greater protection. Machine algorithms have the advantage of speed and are less labour intensive than traditional security models. However, hackers are now also leveraging ML in order to develop more sophisticated modes of attack. They are employing malicious algorithms that have the ability to evade detection by adapting, learning, and continuously improving. AI-led attacks look likely to feature in the future, with a recent study by Forrester revealing that 88% of security professionals expect AI-driven attacks to become mainstream.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Why this is important?&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…Only AI can fight AI.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With new threats on the horizon, the industry will have to adapt in order to defend itself. Companies are already shifting towards more ‘offensive AI’ systems and we are likely to see even more developments as people fight to protect their most valuable resource - data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.unite.ai/researchers-develop-method-for-artificial-neuronal-networks-to-communicate-with-biological-ones/&quot; target=&quot;_blank&quot;&gt;Researchers Develop Method for Artificial Neuronal Networks to Communicate with Biological Ones&lt;/a&gt;
A group of researchers has developed a way for artificial neuronal networks to communicate with biological neuronal networks. The new development is a big step forward for neuroprosthetic devices, which replace damaged neurons with artificial neuronal circuitry.&lt;/p&gt;

&lt;p&gt;The new method relies on the conversion of artificial electrical spiking signals to a visual pattern. That is then used, via optogenetic stimulation, in order to entrain the biological neurons.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;jobs&quot;&gt;Jobs&lt;/h2&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/QxUiLkCQZEMzS/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Machine Learninig Engineer&lt;/a&gt;; MavTek, Montreal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Machine Learning Engineer&lt;/a&gt; Apple, Vancouver.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Machine Learning Engineer&lt;/a&gt; Paytm, Toronto.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Machine Learning/Data Infrastructure Engineer&lt;/a&gt; Wise Systems, Montreal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Deep Learning R&amp;amp;D Engineer&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.glassdoor.ca/Job/canada-machine-learning-engineer-jobs-SRCH_IL.0,6_IN3_KO7,32.htm?fromAge=7&amp;amp;jl=3552705517&amp;amp;ja=128504383&amp;amp;guid=0000017246b98e83b7d30fec6495cdd5&amp;amp;pos=101&amp;amp;srs=EMAIL_JOB_ALERT&amp;amp;s=224&amp;amp;ao=863973&amp;amp;utm_source=jobalert&amp;amp;utm_medium=email&amp;amp;utm_campaign=jobAlertAlert&amp;amp;utm_content=ja-jobtitle&amp;amp;utm_term=&quot; target=&quot;_blank&quot;&gt;Machine Learning Scientist&lt;/a&gt; Amazon, Montreal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://ai-jobs.net/job/1478-senior-data-analyst/&quot; target=&quot;_blank&quot;&gt;Senior Data Analyst&lt;/a&gt;, Remote or Austin, TX&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://careers.google.com/jobs/results/107074172166775494-software-developer-machine-learning/&quot; target=&quot;_blank&quot;&gt;Software Developer, Machine Learning&lt;/a&gt;, Google, Waterloo, ON, Canada&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;That’s it folks! That’s a review of my week.&lt;/p&gt;

&lt;p&gt;If you would like to collaborate on an interesting AI project, please let me know.&lt;/p&gt;

&lt;p&gt;Until next time, stay safe! Love and l&lt;a href=&quot;http://gen.lib.rus.ec/&quot; target=&quot;_blank&quot;&gt;I&lt;/a&gt;ght, in the name of Professor Dumbledore.&lt;/p&gt;

&lt;center&gt;
    &lt;img width=&quot;400&quot; height=&quot;325&quot; src=&quot;https://media.giphy.com/media/IIJpyNVX37NAI/giphy.gif&quot; frameborder=&quot;5&quot; /&gt;
&lt;/center&gt;</content><author><name>Wale</name></author><summary type="html">Previously in the AI field…</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://media.giphy.com/media/lrQ2T4iMddQhG/giphy.gif" /><media:content medium="image" url="https://media.giphy.com/media/lrQ2T4iMddQhG/giphy.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week Recap</title><link href="https://waleopakunle.com/news/2020/05/16/week-recap" rel="alternate" type="text/html" title="Week Recap" /><published>2020-05-16T00:00:00-05:00</published><updated>2020-05-16T00:00:00-05:00</updated><id>https://waleopakunle.com/news/2020/05/16/week-recap</id><content type="html" xml:base="https://waleopakunle.com/news/2020/05/16/week-recap">&lt;h1 id=&quot;previously-in-the-ai-field&quot;&gt;Previously in the AI field…&lt;/h1&gt;

&lt;p&gt;Hey folks! Here are some of the things I came across this past week in the AI field. Hope this can be informative/interesting in some way to the geeks who read to the end. &lt;em&gt;wink&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-open-source-tools&quot;&gt;New Open Source Tools&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The coolest I came across this week, by far, was &lt;a href=&quot;https://deon.drivendata.org/&quot;&gt;deon&lt;/a&gt;; a cli tool for adding an ethics checklist to your data science project. It’s cool because it shows that the community is really taking the issue of ethics and bias in machine learning algorithms into cognizance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Detectron2: A PyTorch based modular object detection library. They made it available as a library so it can just be imported into your project. This is a really cool way to make an opensource contribution!
Detectron2 has new features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc. You can read more about it on their &lt;a href=&quot;https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/&quot;&gt;blog&lt;/a&gt; or check out the &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;code base&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Codespaces: GitHub just added a new feature folks! You can now write code on Github just like you would on a VS Code editor! With this cool feature, you can code, build, test, debug, and deploy with a complete development environment in your browser. There’s also a plugin for codespaces in VS Code. Seriously, you should check it out and request for early access. You see, it’s still in beta stage. You can do so &lt;a href=&quot;https://github.com/features/codespaces/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Qualcom has opensourced their AI Model Efficiency Toolkit! This is in their bid to provide a simple library plugin for AI developers to utilize for state-of-the-art model efficiency performance and to make artificial intelligence ubiquitous across devices, machines, vehicles, and things. Check out their &lt;a href=&quot;https://www.qualcomm.com/news/onq/2020/05/04/open-sourcing-ai-model-efficiency-toolkit?cmpid=oofyus194234&amp;amp;linkId=87923665&quot;&gt;blog post&lt;/a&gt; and the &lt;a href=&quot;https://github.com/quic/aimet&quot;&gt;code base&lt;/a&gt; for the installation guide, user guide and API documentation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Japanese Company, Preferred Networks, releases their &lt;a href=&quot;https://github.com/pfnet/pytorch-pfn-extras&quot;&gt;First PyTorch Library&lt;/a&gt;. This library will help in Distributed snapshots (reducing the costs of implementing distributed deep learning with automated backup, loading, and generation management of snapshots). Other features include automatic inference of parameter sizes (easier network definitions by automatically inferring the sizes of linear or convolution layer parameters via input sizes) and implementations of convenient extensions and reporters. Let’s hope they successfully merge  these features into the PyTorch base build.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;ML-Agents Unity Package v1.0&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/yIChlf9hcRE&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cool-projects&quot;&gt;Cool Projects&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Here’s an OpenCV digit recognition model by &lt;a href=&quot;https://github.com/jagadishb1409/Digit-recognition&quot;&gt;jagadishb1409&lt;/a&gt;. It’s really neat and would be better still if it were modularized. He uses opencv to create a window on the screen and wait to recognize your input from the screen which are captured by pre-designated keys assigned to mouse actions. An image is created when you draw a “4” on the screen for example and sent to a pre-trained MNIST digit recognizer tensorflow model generated from one of the scripts. A great way to extend this would be to modularize it and maybe make it into a flask application as is done  in this &lt;a href=&quot;(https://github.com/veb-101/mnist-flask_app)&quot;&gt;MNIST-flask_app&lt;/a&gt; by &lt;a href=&quot;https://github.com/veb-101&quot;&gt;Vaihab&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.medseg.ai/&quot;&gt;MedSeg.ai&lt;/a&gt; uses a segmentation model to analyze CT scans. It’s free to use, but not openSource, unfortunately.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/amrha/MLSA&quot;&gt;A Multi-Language Sentiment Analysis tool using flask API&lt;/a&gt; using a catboost model trained on the sentement140 kaggle dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/chonyy/AI-basketball-analysis&quot;&gt;Basketball analysis API&lt;/a&gt; built with flask, tensorflow and opencv. Try it out for yourselves &lt;a href=&quot;https://ai-basketball-analysis.herokuapp.com/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/yemount/pose-animator/&quot;&gt;Online Tool Animates Your Actions in Real Time&lt;/a&gt;. Pose Animator takes a 2D vector illustration and animates its containing curves in real-time based on the recognition result from PoseNet and FaceMesh. Pose Animator basically animates users’ poses and movements from either a camera feed or a static image, and can run on browsers in real time using TensorFlow.js.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;new-events&quot;&gt;New Events&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Competition:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.drivendata.org/competitions/64/hateful-memes/&quot;&gt;The Facebook/Data-Driven “Hateful Memes” Competition&lt;/a&gt;. The goal is to create an algorithm that identifies multimodal hate speech in internet memes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conferences:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://mybuild.microsoft.com/&quot;&gt;Virtual Microsoft build&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;new-datasets&quot;&gt;New Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.catalyzex.com/paper/arxiv:2003.09093&quot;&gt;World largest  real-world masked face dataset&lt;/a&gt;.
The dataset includes 5,000 pictures of 525 people wearing masks and 90,000 images of the same 525 subjects without masks. This can be used in grocery stores and other public places to check if people are wearing masks or not. Conventional facial recognition technology is ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. I’d be mighty interested in building something in this direction with (or without) someone.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Springer has made 65 machine-learning related books available for free… for now. Check out the full list &lt;a href=&quot;https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Papers I read this week:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://openai.com/blog/learning-concepts-with-energy-functions/&quot;&gt;Learning concepts with enery functions&lt;/a&gt; :&lt;/p&gt;

        &lt;blockquote&gt;
          &lt;p&gt;Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/abs/1811.02486&quot;&gt;arxiv&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;blog:  &lt;a href=&quot;https://openai.com/blog/learning-concepts-with-energy-functions/&quot;&gt;openai&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;video: &lt;a href=&quot;https://youtu.be/Cs_j-oNwGgg&quot;&gt;Watch explanatory video on youtube&lt;/a&gt;, or right here:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/Cs_j-oNwGgg&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cooperation Instead of Competition: Harvard &amp;amp; Microsoft Research Optimizes AI-Human Teamwork:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper identifies a possible area for future research on human-machine cooperation as an optimization of team performance when interactions extend beyond the current level of querying humans for answers. This could include settings with more complex, interleaved interactions and those with different levels of human initiative and machine autonomy.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/2005.00582.pdf&quot;&gt;arxiv&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;blog: &lt;a href=&quot;https://medium.com/syncedreview/cooperation-instead-of-competition-harvard-microsoft-research-optimizes-ai-human-teamwork-4f7f89bbbf6f&quot;&gt;Quick read&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Books: This past week, I’ve been joggling reading two books:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Learning OpenCV 4 Computer Vision with Python 3 Joseph Howse and Joe Minichino. You can subscribe to read it on &lt;a href=&quot;https://www.packtpub.com/data/learning-opencv-4-computer-vision-with-python-3-third-edition&quot;&gt;Packt&lt;/a&gt;, or simply go through the code for the book &lt;a href=&quot;https://github.com/PacktPublishing/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition&quot;&gt;here&lt;/a&gt; if you have a good background in Python. I settled on this book after a lot of consideration and exploring other books on OpenCV. What makes it stand out is the detailed explanations and the fact that it is in Python, rather than C++. No regrets so far.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;I had tremendous dificulty installing Python opencv-contrib on my windows computer using anaconda. If you experience this issue,  you may have to settle for OpenCV3. All examples in the book run as per normal so far. You can use the code below to install it in a conda environment.&lt;/span&gt;
&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# You need to install the contrib version of opencv, otherwise you wont have access to some&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# features such as video editing/manipulation.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This was what worked for me after several hours of hair pulling.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# you can use any python version you want actually. I prefer to be one step behind the latest release.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Why, you ask? BUGSSSS!! Bugs, thats why!&lt;/span&gt;
C:&lt;span class=&quot;se&quot;&gt;\U&lt;/span&gt;sers&lt;span class=&quot;se&quot;&gt;\x&lt;/span&gt;yz&amp;gt; conda create &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; flask-env flask &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.7
&lt;span class=&quot;c&quot;&gt;# activate the environment&lt;/span&gt;
C:&lt;span class=&quot;se&quot;&gt;\U&lt;/span&gt;sers&lt;span class=&quot;se&quot;&gt;\x&lt;/span&gt;yz&amp;gt; conda activate flask-env
&lt;span class=&quot;c&quot;&gt;# install opencv-contrib&lt;/span&gt;
C:&lt;span class=&quot;se&quot;&gt;\U&lt;/span&gt;sers&lt;span class=&quot;se&quot;&gt;\x&lt;/span&gt;yz&amp;gt; conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; michael_wild opencv-contrib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Flask Web Development - Miguel Grinberg.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Heard of &lt;a href=&quot;https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world&quot;&gt;Flask Mega Tutorial&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;No?&lt;/p&gt;

&lt;p&gt;Heard of &lt;a href=&quot;https://flask-migrate.readthedocs.io/en/latest/&quot;&gt;Flask-Migrate&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;No?&lt;/p&gt;

&lt;p&gt;Seriously?&lt;/p&gt;

&lt;p&gt;Well, you probably don’t use Flask very often if that’s the case. Let’s change that. I would highly recommend this book if you’re looking to get started, or solidify your foundation, like I am. If you can’t afford to buy it, you can simply follow on his site &lt;a href=&quot;https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world&quot;&gt;here&lt;/a&gt;. It’s almost the same as the book. Alternatively, check out the &lt;a href=&quot;https://github.com/miguelgrinberg/flasky&quot;&gt;code base&lt;/a&gt;. You can get the code for each chapter of the book by ‘git check’-ing the branch tags for each chapter. It’s all very efficiently managed by Miguel.&lt;/p&gt;

&lt;p&gt;Trust me, this guy has the most educative resources by far, when it comes to flask (to the best of my very limited knowledge, of course). But seriously, his book is really awesome. Highly, highly recommend.&lt;/p&gt;

&lt;p&gt;Thanks Miguel!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tutorials
    &lt;ul&gt;
      &lt;li&gt;I also went through this cool tutorial on converting old footage to 4K + 60fps + colorized with AI. You may have guessed it. They threw Jason’s (@citnaj on twitter) DeOldify algorithm into the mix. Really cool.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;iframe width=&quot;600&quot; height=&quot;325&quot; src=&quot;https://www.youtube.com/embed/h-zNjxY-m90&quot; frameborder=&quot;5&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ai-developments&quot;&gt;AI developments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Facebook AI built and deployed a &lt;a href=&quot;https://ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/&quot;&gt;real-time neural text-to-speech system that can process 1 sec of audio in 500 ms, using only CPUs.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.wired.com/story/watsons-creator-teach-ai-new-trick-common-sense/&quot;&gt;Watson’s Creator Wants to Teach AI a New Trick: Common Sense&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Read about a budding use case of artificial intelligence that could very well be a new age Alzheimer’s diagnosis tool &lt;a href=&quot;https://www.wired.com/story/watsons-creator-teach-ai-new-trick-common-sense/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find out how Intel and University of Pennsylvania are using federated learning to detect brain tumors &lt;a href=&quot;https://newsroom.intel.com/news/intel-works-university-pennsylvania-using-privacy-preserving-ai-identify-brain-tumors/#gs.5rq2yc&quot;&gt;here&lt;/a&gt;, while protecting privacy of course.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.technologyreview.com/2020/05/05/1001142/ai-reinforcement-learning-simulate-economy-fairer-tax-policy-income-inequality-recession-pandemic/&quot;&gt;This AI can simulate an economy millions of times to create fairer tax policy&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;That’s it folks. That’s a recap of all AI related content I came across this week. What did I miss? What else is new in the AI field? Let me know in the comments!&lt;/p&gt;

&lt;p&gt;Happy exploration.&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">Previously in the AI field…</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://media.giphy.com/media/DcSE3dQGqAzE4/giphy.gif" /><media:content medium="image" url="https://media.giphy.com/media/DcSE3dQGqAzE4/giphy.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Latest Ai Research Updates</title><link href="https://waleopakunle.com/papers/2020/05/08/latest-ai-research-updates" rel="alternate" type="text/html" title="Latest Ai Research Updates" /><published>2020-05-08T00:00:00-05:00</published><updated>2020-05-08T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/05/08/latest-ai-research-updates</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/05/08/latest-ai-research-updates">&lt;h1 id=&quot;latest-in-ai-research---week-19&quot;&gt;Latest in AI Research - Week 19&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/26FPC5oAdfeFPkQQE/giphy.gif&quot; alt=&quot;R for Research&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hey there! Here is a collation of what’s new in AI research.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  CNN Explainer - Learning Convolutional Neural Networks with Interactive Visualization&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/convlayer_overview_demo.gif&quot; alt=&quot;cnn-explainer&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. Users can interactively visualize and inspect the data transformation and flow of intermediate results in a CNN. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level operations (e.g., mathematical computations) and high-level outcomes (e.g., class predictions). To better understand our tool’s benefits, we conducted a qualitative user study, which shows that CNN Explainer can help users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Explainability&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/poloclub/cnn-explainer&quot;&gt;code&lt;/a&gt; - Tensorflow&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Zijie J&lt;/td&gt;
        &lt;td&gt;Wang&lt;/td&gt;
        &lt;td&gt;Robert Turko&lt;/td&gt;
        &lt;td&gt;Omar Shaikh&lt;/td&gt;
        &lt;td&gt;Haekyu Park&lt;/td&gt;
        &lt;td&gt;Nilaksh Das&lt;/td&gt;
        &lt;td&gt;Fred Hohman&lt;/td&gt;
        &lt;td&gt;Minsuk Kahng&lt;/td&gt;
        &lt;td&gt;Duen Horng Chau&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.15004v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Other Resources:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://poloclub.github.io/cnn-explainer/&quot;&gt;CNN Explainer article&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=HnWIHWFbuUQ&amp;amp;feature=youtu.be&quot;&gt;youtube&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Jukebox: A Generative Model for Music&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/jukebox.jpg&quot; alt=&quot;jukebox&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract:We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Music Generation&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/openai/jukebox/&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Prafulla Dhariwal&lt;/td&gt;
        &lt;td&gt;Heewoo Jun&lt;/td&gt;
        &lt;td&gt;Christine Payne&lt;/td&gt;
        &lt;td&gt;Jong Wook Kim&lt;/td&gt;
        &lt;td&gt;Alec Radford&lt;/td&gt;
        &lt;td&gt;Ilya Sutskever&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://cdn.openai.com/papers/jukebox.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Other Resources:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://openai.com/blog/jukebox/&quot;&gt;OpenAI Blog&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/inconsistency.png&quot; alt=&quot;gnn&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: The graph-based model can help to detect suspicious fraud online. Owing to the development of Graph Neural Networks~(GNNs), prior research work has proposed many GNN-based fraud detection frameworks based on either homogeneous graphs or heterogeneous graphs. These work follow the existing GNN framework by aggregating the neighboring information to learn the node embedding, which lays on the assumption that the neighbors share similar context, features, and relations. However, the inconsistency problem is hardly investigated, i.e., the context inconsistency, feature inconsistency, and relation inconsistency. In this paper, we introduce these inconsistencies and design a new GNN framework, GraphConsis, to tackle the inconsistency problem: (1) for the context inconsistency, we propose to combine the context embeddings with node features, (2) for the feature inconsistency, we design a consistency score to filter the inconsistent neighbors and generate corresponding sampling probability, and (3) for the relation inconsistency, we learn a relation attention weights associated with the sampled nodes. Empirical analysis on four datasets indicates the inconsistency problem is crucial in a fraud detection task. The extensive experiments prove the effectiveness of GraphConsis. We also released a GNN-based fraud detection toolbox with implementations of SOTA models.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Fraud Detection&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/safe-graph/DGFraud&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Zhiwei Liu&lt;/td&gt;
        &lt;td&gt;Yingtong Dou&lt;/td&gt;
        &lt;td&gt;Philip S. Yu&lt;/td&gt;
        &lt;td&gt;Yutong Deng&lt;/td&gt;
        &lt;td&gt;Hao Peng&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2005.00625v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  TLDR - Extreme Summarization of Scientific Documents&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: We introduce TLDR generation for scientific papers, a new automatic summarization task with high source compression, requiring expert background knowledge and complex language understanding. To facilitate research on this task, we introduce SciTLDR, a dataset of 3.9K TLDRs. Furthermore, we introduce a novel annotation protocol for scalably curating additional gold summaries by rewriting peer review comments. We use this protocol to augment our test set, yielding multiple gold TLDRs for evaluation, which is unlike most recent summarization datasets that assume only one valid gold summary. We present a training strategy for adapting pretrained language models that exploits similarities between TLDR generation and the related task of title generation, which outperforms strong extractive and abstractive summarization baselines.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Abstractive text summarization&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/allenai/scitldr&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Isabel Cachola&lt;/td&gt;
        &lt;td&gt;Kyle Lo&lt;/td&gt;
        &lt;td&gt;Arman Cohan&lt;/td&gt;
        &lt;td&gt;Daniel S. Weld&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.15011v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Other resources: &lt;a href=&quot;https://scitldr.apps.allenai.org/&quot;&gt;Demo&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  GIMP-ML - Python Plugins for using Computer Vision Models in GIMP&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/gimp.png&quot; alt=&quot;gimp&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: This paper introduces GIMP-ML, a set of Python plugins for the widely popular GNU Image Manipulation Program (GIMP). It enables the use of recent advances in computer vision to the conventional image editing pipeline in an open-source setting. Applications from deep learning such as monocular depth estimation, semantic segmentation, mask generative adversarial networks, image super-resolution, de-noising and coloring have been incorporated with GIMP through Python-based plugins. Additionally, operations on images such as edge detection and color clustering have also been added. GIMP-ML relies on standard Python packages such as numpy, scikit-image, pillow, pytorch, open-cv, scipy. Apart from these, several image manipulation techniques using these plugins have been compiled and demonstrated in the YouTube &lt;a href=&quot;https://www.youtube.com/playlist?list=PLo9r5wFmpD5dLWTyo6NOiD6BJjhfEOM5t&quot;&gt;playlist&lt;/a&gt; with the objective of demonstrating the use-cases for machine learning based image modification. In addition, GIMP-ML also aims to bring the benefits of using deep learning networks used for computer vision tasks to routine image processing workflows.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;DEPTH ESTIMATION&lt;/td&gt;
        &lt;td&gt;EDGE DETECTION&lt;/td&gt;
        &lt;td&gt;IMAGE SUPER-RESOLUTION&lt;/td&gt;
        &lt;td&gt;MONOCULAR DEPTH ESTIMATION&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/kritiksoman/GIMP-ML&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Kritik Soman&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.13060v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  TAPAS - Weakly Supervised Table Parsing via Pre-training&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/tapas.png&quot; alt=&quot;tapas&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;QUESTION ANSWERING&lt;/td&gt;
        &lt;td&gt;SEMANTIC PARSING&lt;/td&gt;
        &lt;td&gt;TRANSFER LEARNING&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/google-research/tapas&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Jonathan Herzig&lt;/td&gt;
        &lt;td&gt;Paweł Krzysztof Nowak&lt;/td&gt;
        &lt;td&gt;Thomas Müller&lt;/td&gt;
        &lt;td&gt;Francesco Piccinno&lt;/td&gt;
        &lt;td&gt;Julian Martin Eisenschlos&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.02349v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Reinforcement Learning with Augmented Data&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/RLAD.png&quot; alt=&quot;rlad&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;DATA AUGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/MishaLaskin/rad&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Michael Laskin&lt;/td&gt;
        &lt;td&gt;Kimin Lee&lt;/td&gt;
        &lt;td&gt;Adam Stooke&lt;/td&gt;
        &lt;td&gt;Lerrel Pinto&lt;/td&gt;
        &lt;td&gt;Pieter Abbeel&lt;/td&gt;
        &lt;td&gt;Aravind Srinivas&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.14990v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  TTNet - Real-time temporal and spatial video analysis of table tennis&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/latest-in-ai/ttnet.png&quot; alt=&quot;rlad&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
        &lt;td&gt;DECISION MAKING&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;No code implementation yet&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Authors:&lt;/td&gt;
        &lt;td&gt;Roman Voeikov&lt;/td&gt;
        &lt;td&gt;Nikolay Falaleev&lt;/td&gt;
        &lt;td&gt;Ruslan Baikulov&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.09927&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;That’s it folks. These arew what I’ve found most interesting in the past week.&lt;/p&gt;

&lt;p&gt;Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below!&lt;/p&gt;

&lt;p&gt;Until next time, stay safe!&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">Latest in AI Research - Week 19</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/latest-in-ai/convlayer_overview_demo.gif" /><media:content medium="image" url="https://waleopakunle.com/images/latest-in-ai/convlayer_overview_demo.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Latest Papers In Ai Research</title><link href="https://waleopakunle.com/papers/2020/05/02/latest-papers-in-ai-research" rel="alternate" type="text/html" title="Latest Papers In Ai Research" /><published>2020-05-02T00:00:00-05:00</published><updated>2020-05-02T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/05/02/latest-papers-in-ai-research</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/05/02/latest-papers-in-ai-research">&lt;h1 id=&quot;latest-in-ai-research---week-18&quot;&gt;Latest in AI Research - Week 18&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/OnJLRvXvAmvPW/giphy.gif&quot; alt=&quot;R for Research&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hey there! Here is a collation of what’s new in AI research.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Adversarial Latent Autoencoders&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-05-02-latest-in-ai/alae1.gif&quot; alt=&quot;adversarial_latent_autoencoders&quot; /&gt;
&lt;img src=&quot;/images/2020-05-02-latest-in-ai/alae2.gif&quot; alt=&quot;adversarial_latent_autoencoders2&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IMAGE GENERATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/podgorskiy/ALAE&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.04467v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  YOLOv4 - Optimal Speed and Accuracy of Object Detection&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-05-02-latest-in-ai/darknet.png&quot; alt=&quot;darknet&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;DATA AUGMENTATION&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/pjreddie/darknet&quot;&gt;Code with most stars - in C &amp;amp; Cuda&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/AlexeyAB/darknet&quot;&gt;Tensorflow&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.10934v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  NBDT - Neural-Backed Decision Trees&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-05-02-latest-in-ai/neural_backed_decision_trees.png&quot; alt=&quot;neural_backed_decision_trees&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Deep learning is being adopted in settings where accurate and justifiable predictions are required, ranging from finance to medical imaging. While there has been recent work providing post-hoc explanations for model predictions, there has been relatively little work exploring more directly interpretable models that can match state-of-the-art accuracy. Historically, decision trees have been the gold standard in balancing interpretability and accuracy. However, recent attempts to combine decision trees with deep learning have resulted in models that (1) achieve accuracies far lower than that of modern neural networks (e.g. ResNet) even on small datasets (e.g. MNIST), and (2) require significantly different architectures, forcing practitioners pick between accuracy and interpretability. We forgo this dilemma by creating Neural-Backed Decision Trees (NBDTs) that (1) achieve neural network accuracy and (2) require no architectural changes to a neural network. NBDTs achieve accuracy within 1% of the base neural network on CIFAR10, CIFAR100, TinyImageNet, using recently state-of-the-art WideResNet; and within 2% of EfficientNet on ImageNet. This yields state-of-the-art explainable models on ImageNet, with NBDTs improving the baseline by ~14% to 75.30% top-1 accuracy. Furthermore, we show interpretability of our model’s decisions both qualitatively and quantitatively via a semi-automatic process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/alvinwan/neural-backed-decision-trees&quot;&gt;Pytorch code&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/alvinwan/neural-backed-decision-trees/blob/master/examples/load_pretrained_nbdts.ipynb&quot;&gt;Colab&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/abs/2004.00221v1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  ResNeSt - Split-Attention Networks&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-05-02-latest-in-ai/gluon_demo.gif&quot; alt=&quot;gluon&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: While image classification models have recently continued to advance, most downstream applications such as object detection and semantic segmentation still employ ResNet variants as the backbone network due to their simple and modular structure. We present a simple and modular Split-Attention block that enables attention across feature-map groups. By stacking these Split-Attention blocks ResNet-style, we obtain a new ResNet variant which we call ResNeSt. Our network preserves the overall ResNet structure to be used in downstream tasks straightforwardly without introducing additional computational costs. ResNeSt models outperform other networks with similar model complexities. For example, ResNeSt-50 achieves 81.13% top-1 accuracy on ImageNet using a single crop-size of 224x224, outperforming previous best ResNet variant by more than 1% accuracy. This improvement also helps downstream tasks including object detection, instance segmentation and semantic segmentation. For example, by simply replace the ResNet-50 backbone with ResNeSt-50, we improve the mAP of Faster-RCNN on MS-COCO from 39.3% to 42.3% and the mIoU for DeeplabV3 on ADE20K from 42.1% to 45.1%.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;IMAGE CLASSIFICATION&lt;/td&gt;
        &lt;td&gt;INSTANCE SEGMENTATION&lt;/td&gt;
        &lt;td&gt;OBJECT DETECTION&lt;/td&gt;
        &lt;td&gt;SEMANTIC SEGMENTATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/dmlc/gluon-cv&quot;&gt;Tensorflow code&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/zhanghang1989/PyTorch-Encoding&quot;&gt;Pytorch&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.08955v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Lite Transformer with Long-Short Range Attention&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-05-02-latest-in-ai/lite_transformer.png&quot; alt=&quot;lite_transformer.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;ABSTRACTIVE TEXT SUMMARIZATION&lt;/td&gt;
        &lt;td&gt;AUTOML&lt;/td&gt;
        &lt;td&gt;LANGUAGE MODELLING&lt;/td&gt;
        &lt;td&gt;MACHINE TRANSLATION&lt;/td&gt;
        &lt;td&gt;QUANTIZATION&lt;/td&gt;
        &lt;td&gt;QUESTION ANSWERING&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/mit-han-lab/lite-transformer&quot;&gt;Pytorch code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.11886v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  MASS - Masked Sequence to Sequence Pre-training for Language Generation
&lt;img src=&quot;/images/2020-05-02-latest-in-ai/mass.png&quot; alt=&quot;mass.png&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;CONVERSATIONAL RESPONSE GENERATION&lt;/td&gt;
        &lt;td&gt;MACHINE TRANSLATION&lt;/td&gt;
        &lt;td&gt;TEXT GENERATION&lt;/td&gt;
        &lt;td&gt;TEXT SUMMARIZATION&lt;/td&gt;
        &lt;td&gt;UNSUPERVISED MACHINE TRANSLATION&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/MASS&quot;&gt;Pytorch code (Microsoft)&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/MPNet&quot;&gt;Pytorch code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1905.02450v5.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;That’s it, that’s what’s trending in AI research these days. What do you think?&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">Latest in AI Research - Week 18</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/2020-05-02-latest-in-ai/alae1.gif" /><media:content medium="image" url="https://waleopakunle.com/images/2020-05-02-latest-in-ai/alae1.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ganspace</title><link href="https://waleopakunle.com/papers/2020/04/26/ganspace" rel="alternate" type="text/html" title="Ganspace" /><published>2020-04-26T00:00:00-05:00</published><updated>2020-04-26T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/04/26/ganspace</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/04/26/ganspace">&lt;h1 id=&quot;ganspace---discovering-interpretable-gan-controls&quot;&gt;&lt;strong&gt;GANSpace - Discovering Interpretable GAN Controls&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;quick-summary&quot;&gt;Quick Summary&lt;/h2&gt;

&lt;h2 id=&quot;what-do-they-improve&quot;&gt;What do they improve?&lt;/h2&gt;

&lt;p&gt;Efficiency and ease of controlling the output of two very popular GAN architectures; the StyleGAN and BigGAN.&lt;/p&gt;

&lt;h2 id=&quot;what-older-methods-are-they-improving-on&quot;&gt;What older methods are they improving on?&lt;/h2&gt;

&lt;p&gt;Continuous control of GANs using some form of supervision, e.g., augmenting GANs with with image labels at training time.&lt;/p&gt;

&lt;h2 id=&quot;issues-with-existing-methods-that-necessitate-this-improvement&quot;&gt;Issues with existing methods that necessitate this improvement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Training in older methods is very expensive.&lt;/li&gt;
  &lt;li&gt;Limitation in style and variational control in most existing GAN architectures.&lt;/li&gt;
  &lt;li&gt;Users usually have little to no control over the direction of the generated output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;which-of-the-issues-are-they-improvingsolving&quot;&gt;Which of the issues are they improving/solving?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training expense:
The components found by the PCA make it possible to control layerwise styling without the need for retraining. The paper demonstrates a way to control BigGANs output style mixing without retraining.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Limitations in styling:
The components found by PCA reveal that a lot of features of the inputs could be manipulated by manipulating the decomposed components responsible for those features. For example, different components were found to be responsible for large scale geometric configuration and viewpoint such as zoom level, angle and so on, while successive components control the finer details and overall appearance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Control of styling:
They identify the most important principal axes directions of the creative process. As it turns out, these axes at the initial layers control very specific features of the output such as gender, age, background, rotation, etc. It also turns out that these axes can be targeted efficiently by the user.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-do-they-propose-to-do-differently&quot;&gt;What do they propose to do differently?&lt;/h2&gt;

&lt;p&gt;Rather than using a supervised approach of augmenting the GAN with labeled images at training time to control the direction of generation, they propose that the controls should be determined after the GAN has been trained. The directions for the control should be determined by deducing the principal axis direction post-training.&lt;/p&gt;

&lt;h2 id=&quot;do-their-experiments-show-an-improvement&quot;&gt;Do their experiments show an improvement?&lt;/h2&gt;

&lt;p&gt;Yes. They provide a user interface in which users can explore the principal directions interactively and visualize the impact that the modification of the components have on the output. They were able to achieve principal decomposition of the latent space and use the components to visualize and interactively edit the output.&lt;/p&gt;

&lt;h2 id=&quot;key-achievements-of-the-paper&quot;&gt;Key achievements of the paper&lt;/h2&gt;

&lt;p&gt;Made BigGAN behave like StyleGAN with an inexpensive modification that enabled layerwise edits. They achieved this by allowing the Skip-z connections to vary individually between layers in BigGANs. as is the case in StyleGANs.&lt;/p&gt;

&lt;h2 id=&quot;future-research-areas&quot;&gt;Future research areas&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Effect of the application of other unsupervised techniques other than PCA, to activations.&lt;/li&gt;
  &lt;li&gt;Modifying activation data arrangements before PCA application&lt;/li&gt;
  &lt;li&gt;Improving other existing GAN architectures with PCA&lt;/li&gt;
  &lt;li&gt;Discovery of more editing controls in GANs&lt;/li&gt;
  &lt;li&gt;Applying PCA technique to GANs with audio input.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-business-applications&quot;&gt;Possible business applications&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Social applications for generative art, with more user controls&lt;/li&gt;
  &lt;li&gt;Photo editing applications&lt;/li&gt;
  &lt;li&gt;Research tool for segmentation tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more-on-the-ganspace&quot;&gt;More on the GANSpace&lt;/h2&gt;

&lt;p&gt;If you know anything about GANs, you know that first of all, they are notoriously difficult to train due to their instability (e.g mode collapse, etc.) , and you might also know that they could take an awful long time to train as well! And many times, you may not have the resource to get to the end of your training, let alone control the outputs at test time.
Anyway, these guys from Alto University, Adobe Research and Nvidia,  did an awesome job. They found a way to efficiently visualize and control the kinds of styles that two flavors of GANs produce. Their experiments cover StyleGAN and BigGAN, but I can imagine that the concepts they lay out in this paper can be extended to other GAN flavors as well, albeit with a little modification and creativity.&lt;/p&gt;

&lt;p&gt;In order to explain why it is interesting (and important) to be able to control the direction that the generator takes in generating these output images/videos/audios, imagine that you had an image of yourself, which you took yesterday when you had your hair cut at home. You see, due to the lockdown, you had your haircut done by your sibling, because you know, what could go wrong?&lt;/p&gt;

&lt;p&gt;Kikiki.&lt;/p&gt;

&lt;p&gt;Imagine that you really, really, really miss your long hair and forgot to take a picture of it just before you cut it and your looks went horribly wrong. Imagine also, that you somehow don’t have an image of your past self for some absurd reason… Now, imagine that you download my beautiful app, that my future intelligent self will build that incorporates a novel GAN that I have trained and deployed for your convenience. Suppose that this “great app” lets you make alternate images of yourself. For example, you could generate images of yourself wearing glasses, 10 years from now, smiling with your favorite vacation spot in the background, and with your long hair that you miss so much… just imagine. Now imagine that the selection of any of these variations was entirely random. Meaning you have no direct control over the type, length and color of hair that you want and could end up with an end image looking bald and with extraordinarily large earrings.&lt;/p&gt;

&lt;p&gt;Oh dear.&lt;/p&gt;

&lt;p&gt;On a scale of 1 to 10 seconds, at what speed would you uninstall the application??&lt;/p&gt;

&lt;p&gt;That’s what I thought.&lt;/p&gt;

&lt;p&gt;Don’t worry, the app my future self builds won’t be so horrendously silly.&lt;/p&gt;

&lt;p&gt;Visualizing and understanding what goes on in GANs is indeed still an open, tough problem. Maybe if someone at OpenAI reads this, they’ll consider investing in a BigGAN “adversarial organism” that’s compatible with Microscope, which they just made &lt;a href=&quot;/ai-news/2020/04/14/openai-microscope&quot;&gt;open source recently&lt;/a&gt;. Maybe.&lt;/p&gt;

&lt;p&gt;Apparently, lots of people have tried their hands at visualizing GANs. Most existing methods prior to this paper used some form of supervision (supplying labels) to guide the direction of generation of the output. This may sound easy, but I assure you they are computationally expensive.&lt;/p&gt;

&lt;p&gt;So, what exactly do these researchers do differently?&lt;/p&gt;

&lt;p&gt;So you know how GANs are (usually) fed random latent vectors z (noise) during training and then they somehow generate an image very similar to one in our training data? Well, how does the GAN, specifically the generator,  know which direction to pursue in its “creative” process? This is the first problem they addressed. The approach they took should be known to most of you, especially if you are just starting out in the field of data science. I certainly had to brush up on it to understand how they went about it in this paper.  It’s called Principal Component Analysis (PCA). In case you don’t know about it, it is a method commonly used to reduce the dimensionality of large vectors that have some form of correlation, while maintaining the variation in the data as much as possible. You can read more about PCA &lt;a href=&quot;https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/&quot;&gt;here&lt;/a&gt;.
In essence, they used PCA to find the most important spatial directions to explore during the “creative” process of the GAN (StyleGAN and BigGAN in this case).&lt;/p&gt;

&lt;p&gt;The way I understand the goal of applying the PCA is that they attempted to find the principal axes of synthesis in each layer of perceptrons that the latent vector (z) activates in the training process.&lt;/p&gt;

&lt;p&gt;Now, because the way the StyleGAN is trained is slightly different from the way the BigGAN is trained, the method for obtaining the Principal Activation direction (the most important axes to be explored while starting from the random latent vector input) is different for each of these GAN architectures.&lt;/p&gt;

&lt;p&gt;To the initiated, you will recall that the input as well as the intermediate layers in the BigGAN take in latent vectors z and Skip-z as inputs respectively, while the class label remains constant. In contrast, in the StyleGAN, the input is kept constant while the output is controlled by a non-linear vector z as input to its intermediate layers.&lt;/p&gt;

&lt;p&gt;To the uninitiated, a basic GAN consists of a probability distribution p(z) from which a latent vector z is sampled, a neural network G(z) that generates novel  images, and a discriminator network D that tries to make G slip up (or improve, depending on how you look at it). The end result is to generate an image (or video, or audio) that looks (or sounds) very convincing. In fact, it should look so convincing that the unsuspecting observer would assume the end result was sampled from the initial training distribution, &lt;em&gt;p(z)&lt;/em&gt;. In a nutshell, that is the overview of the structure and purpose of a GAN.
Due to the latent vector distribution not being learnt in BigGAN, and there not being a way to parameterize the input image like in the StyleGAN, a roundabout method of obtaining the Priority Activation directions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Take N samples of the latent vectors&lt;/li&gt;
  &lt;li&gt;Allow complete forward propagation through the model, so as to produce N activation vectors.&lt;/li&gt;
  &lt;li&gt;Compute PCA from the N activation vectors&lt;/li&gt;
  &lt;li&gt;Compute PCA coordinates of each activation by computing the dot product of the transpose of the low-rank basis matrix &lt;em&gt;V&lt;/em&gt; and the difference between the previous layers input and the data mean.&lt;/li&gt;
  &lt;li&gt;Transfer the basis to latent space by linear regression.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Meanwhile, the way to achieve this in StyleGAN is fairly straightforward. Principal Activation Direction discovery in  the StyleGAN has the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random sampling of latent vectors&lt;/li&gt;
  &lt;li&gt;Compute the styling weights for each layer&lt;/li&gt;
  &lt;li&gt;Compute PCA on the weight values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By using PCA on these activations, they discovered that:
The principal components discovered in earlier layers are responsible for large-scale variations like gender expression and head rotation, while components further in the layers generally control details and overall appearance.
BigGAN components appear to be class invariant, at least for earlier layers. That is, the PCA component for zooming in a class such as “German Shepherd” is the same for a class “Poodle”, for example. The interpretation of the components deeper in the network may have varying interpretations across classes.
Quality and generality depends on the dataset to a large degree.
By modifying each of these components across the layers, various changes can be made to the final output image, without the need for a laborious retraining procedure. In BigGAN, this is achieved by varying the Skip-z inputs separately from the latent vector z inputs between layers. This is the key change in making the BigGAN behave in a way similar to the StyleGAN.&lt;/p&gt;

&lt;p&gt;I think this paper is very interesting because it profers an efficient way to control GANs, and the applications I can think about that involve this are especially interesting. I look forward to more advances in this thinking direction.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Authors of the awesome paper: Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris&lt;/p&gt;

&lt;p&gt;link to paper: &lt;a href=&quot;https://arxiv.org/pdf/2004.02546v1.pdf&quot;&gt;arxiv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;code: &lt;a href=&quot;https://github.com/harskish/ganspace&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;colab walk-through: coming soon&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Let me know if you liked this summary, and if you have a paper you would like me to append to my “read’n summarize” list. 
Until next time, keep home and stay safe!&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">GANSpace - Discovering Interpretable GAN Controls</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/2020-04-27-ganspace/ganspace1.png" /><media:content medium="image" url="https://waleopakunle.com/images/2020-04-27-ganspace/ganspace1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Latest Papers In Ai Research</title><link href="https://waleopakunle.com/papers/2020/04/21/latest-papers-in-ai-research" rel="alternate" type="text/html" title="Latest Papers In Ai Research" /><published>2020-04-21T00:00:00-05:00</published><updated>2020-04-21T00:00:00-05:00</updated><id>https://waleopakunle.com/papers/2020/04/21/latest-papers-in-ai-research</id><content type="html" xml:base="https://waleopakunle.com/papers/2020/04/21/latest-papers-in-ai-research">&lt;h1 id=&quot;latest-in-ai-research---week-17&quot;&gt;Latest in AI Research - Week 17&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/26FPC5oAdfeFPkQQE/giphy.gif&quot; alt=&quot;R for Research&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hey there! It’s another Tuesday, and here is a collation of what’s new in AI research.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Learning to see in the dark&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-04-21-latest-in-ai/learning-t-see-in-the-dark.png&quot; alt=&quot;learning_to_see_in_the_dark&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract:Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Deblurring&lt;/td&gt;
        &lt;td&gt;Denoising&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/cchen156/Learning-to-See-in-the-Dark&quot;&gt;code1&lt;/a&gt; - Python 2.7, Tensorflow&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch&quot;&gt;code2&lt;/a&gt; - Pytorch&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1805.01934v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  DeepFactors: Real-Time Probabilistic Dense Monocular SLAM&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-04-21-latest-in-ai/df_teaser2.gif&quot; alt=&quot;df_teaser&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/jczarnowski/DeepFactors&quot;&gt;code&lt;/a&gt; - C++&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://github.com/jczarnowski/DeepFactors&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Weight Poisoning Attacks on Pre-trained Models&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-04-21-latest-in-ai/evil_bert_weight_poisoning.jpg&quot; alt=&quot;evil_bert&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract:Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct weight poisoning’’ attacks where pre-trained weights are injected with vulnerabilities that expose backdoors’’ after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Sentiment Analysis&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/neulab/RIPPLe&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/2004.06660v1.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Ntgcecq.png&quot; alt=&quot;Architechture of different off-policy agents&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Atari Games&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/google-research/batch_rl&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1709.06009v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Footprints and Free Space from a Single Color Image&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2020-04-21-latest-in-ai/footprints_and_free_space.gif&quot; alt=&quot;footprints_and_free_space&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Understanding the shape of a scene from a single color image is a formidable computer vision task. However, most methods aim to predict the geometry of surfaces that are visible to the camera, which is of limited use when planning paths for robots or augmented reality agents. Such agents can only move when grounded on a traversable surface, which we define as the set of classes which humans can also walk over, such as grass, footpaths and pavement. Models which predict beyond the line of sight often parameterize the scene with voxels or meshes, which can be expensive to use in machine learning frameworks. We introduce a model to predict the geometry of both visible and occluded traversable surfaces, given a single RGB image as input. We learn from stereo video sequences, using camera poses, per-frame depth and semantic segmentation to form training data, which is used to supervise an image-to-image network. We train models from the KITTI driving dataset, the indoor Matterport dataset, and from our own casually captured stereo footage. We find that a surprisingly low bar for spatial coverage of training scenes is required. We validate our algorithm against a range of strong baselines, and include an assessment of our predictions for a path-planning task.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Semantic Segmentation&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/nianticlabs/footprints&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  An Optimistic Perspective on Offline Reinforcement Learning&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Ntgcecq.png&quot; alt=&quot;Architechture of different off-policy agents&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this replay dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. The results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Atari games&lt;/td&gt;
        &lt;td&gt;Q-Learning&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/google-research/batch_rl&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1907.04543v3.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  AI Feynman: a Physics-Inspired Method for Symbolic Regression&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: A core challenge for both physics and artificial intellicence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult test set, we improve the state of the art success rate from 15% to 90%.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/SJ001/AI-Feynman&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1905.11481v2.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Title:  Few-Shot NLG with Pre-Trained Language Model&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract: Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of \textit{few-shot natural language generation}. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Task:&lt;/td&gt;
        &lt;td&gt;Few Shot learning&lt;/td&gt;
        &lt;td&gt;Language Modelling&lt;/td&gt;
        &lt;td&gt;Text Generation&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Implementation:&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://github.com/czyssrs/Few-Shot-NLG&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the paper &lt;a href=&quot;https://arxiv.org/pdf/1904.09521v3.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: All papers and their related information are scraped from &lt;a href=&quot;https://paperswithcode.com/&quot;&gt;Paperswithcode&lt;/a&gt;, a super awesome site!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below!&lt;/p&gt;

&lt;p&gt;Until next time, stay safe!&lt;/p&gt;</content><author><name>Wale</name></author><summary type="html">Latest in AI Research - Week 17</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://waleopakunle.com/images/2020-04-21-latest-in-ai/ours_1.gif" /><media:content medium="image" url="https://waleopakunle.com/images/2020-04-21-latest-in-ai/ours_1.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>