{
  
    
        "post0": {
            "title": "Latest Papers In Ai Research Wk21",
            "content": "Latest in AI Research - Week 21 . . Hey there! Here is a collation of what’s new in AI research. . Title: Adversarial Colorization Of Icons Based On Structure And Color Conditions . | . . Abstract: We present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours, whereas our system colorizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on our collected icon dataset. One condition requires the generated image and the drawn contour to possess a similar contour, while the other anticipates the image and the referenced icon to be similar in color style. Accordingly, the generator takes a contour image and a man-made icon image to colorize the contour, and then the discriminators determine whether the result fulfills the two conditions. The trained network is able to colorize icons demanded by designers and greatly reduces their workload. For the evaluation, we compared our dual conditional GAN to several state-of-the-art techniques. Experiment results demonstrate that our network is over the previous networks. Finally, we will provide the source code, icon dataset, and trained network for public use. . Task: | COLORIZATION | . Implementation: | Pytorch | . Read the paper here . . . Title: DeepFaceFlow - In-the-wild Dense 3D Facial Motion Estimation . | . . Abstract: Dense 3D facial motion capture from only monocular in-the-wild pairs of RGB images is a highly challenging problem with numerous applications, ranging from facial expression recognition to facial reenactment. In this work, we propose DeepFaceFlow, a robust, fast, and highly-accurate framework for the dense estimation of 3D non-rigid facial flow between pairs of monocular images. Our DeepFaceFlow framework was trained and tested on two very large-scale facial video datasets, one of them of our own collection and annotation, with the aid of occlusion-aware and 3D-based loss function. We conduct comprehensive experiments probing different aspects of our approach and demonstrating its improved performance against state-of-the-art flow and 3D reconstruction methods. Furthermore, we incorporate our framework in a full-head state-of-the-art facial video synthesis method and demonstrate the ability of our method in better representing and capturing the facial dynamics, resulting in a highly-realistic facial video synthesis. Given registered pairs of images, our framework generates 3D flow maps at ~60 fps. . Task: | 3D RECONSTRUCTION | FACIAL EXPRESSION RECOGNITION | MOTION CAPTURE | MOTION ESTIMATION | . Implementation: . . The group has only released a peper as at the time of writing this post. The dataset used and code implementations are still in the works and you can [stay updated here.](https://github.com/mrkoujan/DeepFaceFlow) Read the paper here . . . Title: EfficientPS - Efficient Panoptic Segmentation . | . Demo: . Abstract: Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date. . Below is a short video that summarises this awesome piece of work. You will get a visual understanding of what image segmentation is about, the architecture used to achieve this, as well as see it in action: . Task: | INSTANCE SEGMENTATION | PANOPTIC SEGMENTATION | SEMANTIC SEGMENTATION | . Implementation: . . This piece is licenced under the GPL-3 License. It is currently the state of the art (SOTA) for Panoptic Segmentation based on the _Mapillary val_ dataset. Due to the inherent nature of the GPL-3 License, you need to contact the authors to get the source code. However, you can read a short article on its usage, architecture as well as see several demos [here](http://panoptic.cs.uni-freiburg.de/) and see the License [here](https://github.com/DeepSceneSeg/EfficientPS). . Links in alert boxes do not open into a new window... yet. Work in progress. Read the paper here . Current SOTA for Panoptic Segmentation on Mapillary val . . . Title: Order-Aware Generative Modeling Using the 3D-Craft Dataset . | . . Abstract: In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available. . Task: | HOUSE GENERATION | . Implementation: | Facebook Research’s Pytorch Code | . Read the paper here . . . Title: U2-Net - Going Deeper with Nested U-Structure for Salient Object Detection . | . . Abstract: In this paper, we design a simple yet powerful deep network architecture, U2-Net, for salient object detection (SOD). The architecture of our U2-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U2-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U2-Net† (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: here . Task: | IMAGE CLASSIFICATION | OBJECT DETECTION | SALIENT OBJECT DETECTION | . Implementation: | code | . Read the paper here . This code has been used in this AR Project by Cyril . . . Title: Surfboard: Audio Feature Extraction for Modern Machine Learning . | . . Abstract: We introduce Surfboard, an open-source Python library for extracting audio features with application to the medical domain. Surfboard is written with the aim of addressing pain points of existing libraries and facilitating joint use with modern machine learning frameworks. The package can be accessed both programmatically in Python and via its command line interface, allowing it to be easily integrated within machine learning workflows. It builds on state-of-the-art audio analysis packages and offers multiprocessing support for processing large workloads. We review similar frameworks and describe Surfboard’s architecture, including the clinical motivation for its features. Using the mPower dataset, we illustrate Surfboard’s application to a Parkinson’s disease classification task, highlighting common pitfalls in existing research. The source code is opened up to the research community to facilitate future audio research in the clinical domain. . Task: | AUDIO FEATURE EXTRACTION | . Implementation: | code | . Read the paper here . . . Title: Single-Stage Semantic Segmentation from Image Labels . | . . Abstract: Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage − training one segmentation network on image labels − which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods. . Task: | SEMANTIC SEGMENTATION | . Implementation: | code | . Read the paper here . . . Title: Flowtron - an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis . | . . Abstract: In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at [Github]](https://github.com/NVIDIA/flowtron) . Task: | TEXT-TO-SPEECH SYNTHESIS | STYLE TRANSFER | SPEECH SYNTHESIS | . Implementation: | NVIDIA Pytorch code | . Read the paper here . . . Title: Variational quantum Gibbs state preparation with a truncated Taylor series . | . Abstract: The preparation of quantum Gibbs state is an essential part of quantum computation and has wide-ranging applications in various areas, including quantum simulation, quantum optimization, and quantum machine learning. In this paper, we propose variational hybrid quantum-classical algorithms for quantum Gibbs state preparation. We first utilize a truncated Taylor series to evaluate the free energy and choose the truncated free energy as the loss function. Our protocol then trains the parameterized quantum circuits to learn the desired quantum Gibbs state. Notably, this algorithm can be implemented on near-term quantum computers equipped with parameterized quantum circuits. By performing numerical experiments, we show that shallow parameterized circuits with only one additional qubit can be trained to prepare the Ising chain and spin chain Gibbs states with a fidelity higher than 95%. In particular, for the Ising chain model, we find that a simplified circuit ansatz with only one parameter and one additional qubit can be trained to realize a 99% fidelity in Gibbs state preparation at inverse temperatures larger than 2. . Task: | QUANTUM MACHINE LEARNING | . Implementation: | Github, Chinese | Github, English Translated | . Read the paper here . . . Title: Neural Controlled Differential Equations for Irregular Time Series . | . Abstract: Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of emph{controlled differential equations}. The resulting emph{neural controlled differential equation} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models. . Task: | IRREGULAR TIME SERIES | TIME SERIES | . Implementation: | code | . Read the paper here . . . Other ways to catch up on research . Henry AI Labs . Truth be told, I thought to create this section because Henry does what he does exceedingly well. In fact, I look forward to his posts every week. Here’s the one he released for the corresponding 21st week of the year 2020: . Here he describes what could potentially help me automate my “paper summary” adventures: . Two minute papers . If you don’t know about Dr Károly Zsolnai-Fehér, let me introduce you to a master storyteller. He (and his team) do an outstanding job on all their videos. He makes catching up on AI developments a breeze, and very enjoyable. For example, have a look at how he talks about OpenAI’s Jukebox AI. . . . That’s it folks. These are were the trendy papers for the 21st week of 2020. . I am particularly happy that I had found a lot more demos to append to the papers than all my previous posts. . These researchers are the absolute best, and you all have been an excellent audience. You all have my heartfelt appreciation. . Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below! . Until next time, stay safe! .",
            "url": "https://waleopakunle.com/papers/2020/06/12/latest-papers-in-ai-research-wk21",
            "relUrl": "/papers/2020/06/12/latest-papers-in-ai-research-wk21",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Stack Gan Exposition",
            "content": "Face Aging Using Conditional GAN . StackGAN is a two-stage network. Each stage has two generators and two discriminators. StackGAN is made up of many networks, which are as follows: . Stack-I GAN: text encoder, Conditioning Augmentation network, generator network, discriminator network, embedding compressor network . | Stack-II GAN: text encoder, Conditioning Augmentation network, generator network, discriminator network, embedding compressor network . | . *** INSERT IMAGE OF STACK-GAN ARCHITECTURE *** . The text encoder network . The sole purpose of the text encoder network is to convert a text description (t) to a text embedding . (). I won’t train the text encoder network. I will be working with pre-trained text embeddings, but you can train yours by using the steps discussed in this paper: https://arxiv.org/pdf/1605.05395.pdf. The text encoder network encodes a sentence to a 1,024-dimensional text embedding. The text encoder network is common to both of the stages . The conditioning augmentation block . A conditioning augmentation (CA) network samples random latent variables  from a distribution, which is represented as  . Advantages: . It adds randomness to the network. . | It makes the generator network robust by capturing various objects with various poses and appearances.  . | It produces more image-text pairs. With a higher number of image-text pairs, we can train a robust network that can handle perturbations . | . STAGE-1 . STAGE-II . TERMS TO READ UP ON . diagonal covariance matrix . | upsampling . | .",
            "url": "https://waleopakunle.com/2020/06/12/STACK-GAN-Exposition.html",
            "relUrl": "/2020/06/12/STACK-GAN-Exposition.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Face Aging Cgan Exposition",
            "content": "Face Aging Using Conditional GAN . cGANs are a type of GAN that are conditioned on some extra information. We feed the extra information y to the generator as an additional input layer. In vanilla GANs, there is no control over the category of the generated images. When we add a condition y to the generator, we can generate images of a specific category, using y, which might be any kind of data, such as a class label or integer data. Vanilla GANs can learn only one category and it is extremely difficult to architect GANs for multiple categories. A cGAN, however, can be used to generate multi-modal models with different conditions for different categories. . The architecture of a cGAN is shown in the following diagram: . ** INSERT IMAGE OF ARCHITECTURE ** . The training objective function for cGANs can be expressed as follows: . . Here, G is the generator network and D is the discriminator network. The loss for the discriminator is and the loss for the generator is . We can say the G(z | y) is modeling the distribution of our data given z and y. Here, z is a prior noise distribution | . The architecture of a cGAN for face aging is slightly more complicated. The Age-cGan consists of four networks: an encoder, the FaceNet, a generator network, and a discriminator network. With the encoder, we learn the inverse mapping of input face images and the age condition with the latent vector . FaceNet is a face recognition network that learns the difference between an input image x and a reconstructed image . We have a generator network, which takes a hidden representation consisting of a face image and a condition vector and generates an image. The discriminator network is to discriminate between the real images and the fake images.  . The problem with cGANs is that they can’t learn the task of inverse mapping an input image x with attributes y to a latent vector z. The solution to this problem is to use an encoder network. We can train an encoder network to approximate the inverse mapping of the input images x. . The encoder network . The primary goal of the encoder network is to generate a latent vector of the provided images. Basically, it takes an image of a dimension of (64, 64, 3) and converts it into a 100-dimensional vector. The encoder network is a deep convolutional neural network. The network contains four convolutional blocks and two dense layers. Each convolutional block contains a convolutional layer, a batch normalization layer, and an activation function. In each convolutional block, each convolutional layer is followed by a batch normalization layer, except the first convolutional layer . Face recognition network . The primary goal of the face recognition network is to recognize a person’s identity in a given image. For our task, we will be using the pre-trained or ResNet-50 model without fully connected layers. The pre-trained or ResNet-50 network, once provided with an image, returns the corresponding embedding. The extracted embeddings for the real image and the reconstructed image can be calculated by calculating the Euclidean distance of the embeddings . The generator network . The primary goal of the generator is to generate an image of a dimension of (64, 64, 3). It takes a 100-dimensional latent vector and some extra information, y, and tries to generate realistic images. The generator network is a deep convolutional neural network too. It is made up of dense, upsampling, and convolutional layers. It takes two input values: a noise vector and a conditioning value. The conditioning value is the additional information provided to the network. For the Age-cGAN, this will be the age.  . The discriminator network . The primary goal of the discriminator network is to identify whether the provided image is fake or real. It does this by passing the image through a series of downsampling layers and some classification layers. In other words, it predicts whether the image is real or fake. Like the other networks, the discriminator network is another deep convolutional network. It contains several convolutional blocks. Each convolutional block contains a convolutional layer, a batch normalization layer, and an activation function, apart from the first convolutional block, which doesn’t have the batch normalization layer.  . Aging-CGAN TRAINING . The training of the Age-cGAN is made up of three stages: . Conditional GAN training: In this stage, we train the generator network and the discriminator network. . | Initial latent vector approximation: In this stage, we train the encoder network. . | Latent vector optimization: In this stage, we optimize both the encoder and the generator network. . | *** CREATE YOUR OWN DIAGRAM OF AGING-CGAN TRAINING AND PLACE HERE *** . Conditional GAN training . In this stage, we train the generator network and the discriminator network. Once trained, the generator network can generate blurred images of a face. This stage is similar to training a vanilla GAN, in which we train both networks simultaneously.  . The training objective function . ** HANDWRITTEN IMAGE OF THE EQUATION AND EXPLAIN IT ** . Initial latent vector approximation . Initial latent vector approximation is a method to approximate a latent vector to optimize the reconstruction of face images. To approximate a latent vector, we have an encoder network. We train the encoder network on the generated images and real images. Once trained, the encoder network will start generating latent vectors from the learned distribution. The training objective function for training the encoder network is the Euclidean distance loss. . Latent vector optimization . During latent vector optimization, we optimize the encoder network and the generator network simultaneously. The equation we use for latent vector optimization is as follows: . . FR is the face recognition network. This equation indicates that the Euclidean distance between the real image and the reconstructed images should be minimal. In this stage, we try to minimize the distance to maximize identity preservation. .",
            "url": "https://waleopakunle.com/2020/06/12/Face-Aging-cGAN-Exposition.html",
            "relUrl": "/2020/06/12/Face-Aging-cGAN-Exposition.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Lumos Maxima",
            "content": "Previously in the AI field… . Hey folks, I am back with a recap of last week in AI! This week, I decided to add on a new section, based on feedback from a fellow data scientist. Be sure to check out the “Job” section for a highlight of some opportunities that are open for machine learning practitioners. . Working on feedback from last week, all links now open in new tabs. I will be implementing that for all links going forward. Thanks for the feedback, and I hope you enjoy a better reading experience. . . T.O.C . New Open Source Tools . | Cool Projects . | New Events . | New Datasets . | Resources . | AI Developments . | Jobs . | . New Open Source Tools . . . Tensorflow announced the release of BigTransfer (BiT); a compilation of pre-trained models for easy and effective transfer learning for vision with TF-Hub! For a detailed walkthrough on how to utilize these models and customize it to your computer vision needs, checkout the official tensorflow blog post or try it out for yourselves in colab. If you’re so inclined, checkout their paper on Graphical Visual Representation Learning. . | Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. . | Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over thousands of pretrained models in 100+ languages and deep interoperability between PyTorch &amp; TensorFlow 2.0. Features: High performance on NLU and NLG task | State-of-the-art NLP for everyone | . | . | OpenTPOD OpenTPOD is an all-in-one open-source tool for nonexperts to create custom deep neural network object detectors. It is designed to lower the barrier of entry and facilitates the end-to-end authoring workflow of custom object detection using state-of-art deep learning methods. It provides several features via an easy-to-use web interface, including; training data management, data annotation through seamless integration, one-click training / fine-tuning of object detection deep neural networks, one-click model export for interface with Tensorflow Serving, and extensible architecture for easy addition of new deep neural network architectures… | . . Cool Projects . . . Open Source: . Olivia: an open-source chatbot built in Golang using Machine Learning technologies. Its goal is to provide a free and open-source alternative to big services like DialogFlow. You can chat with her by speaking (STT) or writing, she replies with a text message but you can enable her voice (TTS). It currently supports 7 languages and you can try it out here Olivia is organized in modules to facilitate the addition of new capabilities. These modules can be written in Go to execute multiple tasks. The project is entirely open-source from the website to the backend. This gives you the ability to build your own chatbot and contribute to Olivia. Don’t you just love open source? . | Groknet: Earlier this week, Facebook announced their deployment of a universal computer vision system designed for shopping. It can identify fine-grained product attributes across billions of photos — in different categories, such as fashion, auto, and home decor. . | Using reinforcement learning algorithms to play the game 2048 This repository is a project about using DQN(Q-Learning) to play the Game 2048 and accelarate and accelerate the environment using Numba). The algorithm used is from Stable Baselines, and the environment is a custom Open AI env. . | . | . . New Events . . . ETL and Advanced Machine Learning - Open Source, No Code Required . | The Stanford Institute for Human-Centered Artificial Intelligence’s COVID + AI: The Road Ahead: Two months into the COVID-19 pandemic, we see curves flattening and cities lifting shelter restrictions. By no means is this pandemic over; and yet, policymakers, researchers, medical practitioners, and industry are starting to grapple with how we operate in a post-COVID world. What is the path forward, economically, medically, and culturally? Scholars from across disciplines will discuss their research using AI, data science and/or informatics to help us understand how we emerge from this crisis. . | . Live Virtual Event Details . Date: June 01, 2020 - 09:15am–12:30pm . Register here. . CVPR 2020 | . CVPR is the premier annual computer vision event comprising the main conference and several co-located workshops and short courses. With its high quality and low cost, it provides an exceptional value for students, academics and industry researchers. CVPR 2020 will take place virtually from June 14 to June 19th. . Apple WWDC 2020 | . On June 22, WWDC20 takes off. Get ready for the first global, all-online WWDC by downloading the Apple Developer app to stay notified on all the latest news, with updates for events and sessions. And there’s a lot more to come — starting with the first-ever Swift Student Challenge. . . New Datasets . . . Flickr 30k Dataset: The Flickr 30k dataset has over 30,000 images, and each image is labeled with different captions. This dataset is used to build an image caption generator. And this dataset is an upgraded version of Flickr 8k used to build more accurate models. | . Possible project idea: You can train a deep learning model that is capable of analysing and extracting features from the image and generate as english sentence that describes the image. Next time when your friend send you an image and goes, “Caption this”, you can ask your model what it thinks and get back to your friend with the machine caption. . Chatbot Intents Dataset The dataset for a chatbot is a JSON file that has disparate tags like goodbye, greetings, pharmacy_search, hospital_search, etc. Every tag has a list of patterns that a user can ask, and the chatbot will respond according to that pattern. The dataset is perfect for understanding how chatbot data works. | . Possible Project Idea: You can build a chatbot or understand the working of a chatbot by modifying and expanding the data with your observations. To build a Chatbot of your own, you need to have a good knowledge of Natural language processing concepts. This article from DZone could help you get started with that. . Mall Customers Dataset The Mall customers dataset holds the details about people visiting the mall. The dataset has an age, customer id, gender, annual income, and spending score. It gains insights from the data and divides the customers into different groups based on their behaviors. | . Possible Project Idea: Segment the customers based on their gender, age, interest. It is useful in customized marketing. Customer segmentation is an important practice of dividing customers based on individual groups that are similar. . This article could help you get started with customer segmentation with R or this one, if you speak the language of the gods. . Youtube 8M Dataset The youtube 8M dataset is a large scale labeled video dataset that has 6.1 million Youtube video ids, 350,000 hours of video, 2.6 billion audio/visual features, 3862 classes, and 3 avg labels per video. It is used for video classification purposes. | . Project Idea: Video classification can be done by using the dataset, and the model can describe what video is about. A video takes a series of inputs to classify in which category the video belongs. This would be a rather involved project, but if you are interested, this paper could help get you started, along with the following implementations: . google/youtube-8m . | AKASH2907/Content-based-Video-Recommendation . | . Resources . . . Papers I read Grounding Language in Play: A scalable approach for controlling robots with natural language | Instance-aware Image Colorization | . | Articles: . AI Ethics in China “… It is therefore surprising that the western misconception that China lacks debate around AI ethics seems to prevail, when in fact: i) existing Chinese AI principles largely align with global ones; ii) Chinese discussions enjoy unprecedented government support; and iii) the country is already investigating the technical and social implementation of these principles by exploring how they interact with its distinct cultural heritage. In this essay I explore AI ethics in China through the lens of some of the key topics discussed in depth in the other essays on this collection – education, healthcare, smart cities and social credit systems – to consider which, if any, ethical issues are unique to China and which should be seen as global concerns.” - Danit Gal, Technology advisor to the UN Secretary General’s High-level Panel on Digital Cooperation and associate fellow at the Leverhulme Centre for the Future of Intelligence at the University . | Microsoft responsible machine learning capabilities build trust in AI systems, developers say. Read this article by John Roach to find out what the industry, particularly Micrsosft, is doing regarding ethical AI and model interpretability. Going further in the rabbit hole, I discovered they now have modules that cater for interpretability which I didn’t notice when I was using the service last year. You can check that out in the official Azure Machine Learning docs. For an example of how to use this, checkout this repo. . | Three Risks in Building Machine Learning Systems , Benjamin Cohen . | Read how Waymo is using GANs in modules of their self-driving software, to simulate autonomous vehicle camera and sensor data in this article by Kyle Wiggers on Venturebeat. | . ” In simulation, when a trajectory of a self-driving car and other agents (e.g. other cars, cyclists, and pedestrians) changes, the system generates realistic visual sensor data that helps us model the scene in the updated environment … “ - A Waymo spokesperson. . Is AI democratization a real thing? . | This Will Change the Way You Look at GANs . | Entity linking: A primary NLP task for Information Extraction . | Top 10 Nice-To-Have Data Science Libraries . | Building a Business From a Bedroom, $98,130 and 11-Months In . | . | Free Books: The Elements of Statistical Learning, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman | Dive Into Deep Learning You can also download the entire book in notebook format, so you can run it locally or on AWS Sagemaker. | . | Paid Books: . High Performance Python, 2nd Edition by Micha Gorelick &amp; Ian Ozsvald. | . | Notes: Notes from an Introductory course on probablistic graphical models. They are based on the Stanforf course, CS228. . | My colab notebook exploring the “Adversarial-Colorization-Of-Icons-Based-On-Structure-And-Color-Conditions” project. I merely provide comments on the code and an intuition of how to minimally customize the set up. This group of researchers present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps. Designers are tasked with drawing contours and then, the system colourizes contours in different styles. This goal is achieved by training a dual conditional generative adversarial network (GAN) on the collected icon dataset. If you are familiar with Python wx, you can get the user interface which was built with Python Wx here. You can either get the preprocessed data, or the unprocessed data if you would like to train the model yourself. I’m at the phase of understanding and customizing the UI. Perhaps I will be able to add a new feature. fingers-crossed . | My colab notebook on building a food classifier with Keras. The model is suffering from acute overfitting as you can see from the loss logs. This was initially meant to be a video tutorial series on how to deploy deep learning models in production, but I wont be completing the video shoot until I get the accuracy up a lot higher than it currently is. This week, I will be tuning hyperparameters and retraining the model. Problem is, it takes too friggin long! Just training on 4 classes took an hour per epoch. I’ll have to look into options to reduce training time as well. Once that headache is done, I’ll be sharing how you can put the model into a flask application, deploy its docker image and manage it with kubernetes. . | . | Cool Tutorials CMU Neural Nets for NLP 2020 (24 Video Playlist) . | AI Saturday Lagos - ML Track, Week 10 : Nonlinear Modeling, Cross Validation and Regularization by the amazing Tejumade Afonja. . | . | . AI Saturday Lagos - DL Track, Week 10 : Natural Language Processing &amp; RNNs by the awesome Oreva Ahia. | . Data Engineering 101 by Damilola Ojo, with Data Science Nigeria | . Docker &amp; Kubernetes Demo in 15 mins: Package &amp; Deploy Your First App | . Nuggets: Deploying a Go application with Docker images on Kubernetes. . | The Only Step-by-Step Guide You’ll Need to Build a Web Scraper With Python . | A Visual Survey of Data Augmentation in NLP “…I was curious if there were attempts at developing augmentation techniques for NLP and explored the existing literature. In this post, I will give an overview of the current approaches being used for text data augmentation based on my findings..” - Amit Chaudhary, Machine Learning Engineer at Fusemachines . | Everything you need to know about Auto-Deeplab: Google’s latest on Segmentation . | Five Cool Python Libraries for Data Science . | Federated Learning for Medical AI . | Python List Comprehension | . | . . AI Developments . . . Watch how OpenAI and Microsoft apply OpenAI’s language model to achieve code generation. | . Air Force ‘Skyborg’ AI powered drones to take flight alongside warplanes by 2023, because apparently ‘Skynet’ was copyrighted. The U.S. Air Force is soliciting the aerospace industry to provide flyable “Skyborg” drones by 2023. The drones will be powered by artificial intelligence, capable of taking off, landing, and performing missions on their own. Skyborg will not only free manned pilots from dangerous and dull missions but allow the Air Force to add legions of new, unpiloted, cheap planes…Read more. . | AI vs. Cyberattacks It’s no news that hackers are now taking advantage of machine learning. Cyberattacks have become part of the modern way of life, with companies investing billions of dollars to protect their data from hackers. As ways to secure systems have become more advanced and innovative, so too have the means of attack. Following the 2017 ‘worms’ attack by WannaCry and NotPetya, which bypassed firewalls and crippled thousands of organisations, companies have turned to AI in order to provide greater protection. Machine algorithms have the advantage of speed and are less labour intensive than traditional security models. However, hackers are now also leveraging ML in order to develop more sophisticated modes of attack. They are employing malicious algorithms that have the ability to evade detection by adapting, learning, and continuously improving. AI-led attacks look likely to feature in the future, with a recent study by Forrester revealing that 88% of security professionals expect AI-driven attacks to become mainstream. | . Why this is important?: . “…Only AI can fight AI.” . With new threats on the horizon, the industry will have to adapt in order to defend itself. Companies are already shifting towards more ‘offensive AI’ systems and we are likely to see even more developments as people fight to protect their most valuable resource - data. . Researchers Develop Method for Artificial Neuronal Networks to Communicate with Biological Ones A group of researchers has developed a way for artificial neuronal networks to communicate with biological neuronal networks. The new development is a big step forward for neuroprosthetic devices, which replace damaged neurons with artificial neuronal circuitry. . The new method relies on the conversion of artificial electrical spiking signals to a visual pattern. That is then used, via optogenetic stimulation, in order to entrain the biological neurons. . . Jobs . . . Machine Learninig Engineer; MavTek, Montreal. . | Machine Learning Engineer Apple, Vancouver. . | Machine Learning Engineer Paytm, Toronto. . | Machine Learning/Data Infrastructure Engineer Wise Systems, Montreal. . | Deep Learning R&amp;D Engineer . | Machine Learning Scientist Amazon, Montreal. . | Senior Data Analyst, Remote or Austin, TX . | Software Developer, Machine Learning, Google, Waterloo, ON, Canada . | . . That’s it folks! That’s a review of my week. . If you would like to collaborate on an interesting AI project, please let me know. . Until next time, stay safe! Love and lIght, in the name of Professor Dumbledore. .",
            "url": "https://waleopakunle.com/news/2020/05/23/lumos-maxima",
            "relUrl": "/news/2020/05/23/lumos-maxima",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Week Recap",
            "content": "Previously in the AI field… . Hey folks! Here are some of the things I came across this past week in the AI field. Hope this can be informative/interesting in some way to the geeks who read to the end. wink . New Open Source Tools . The coolest I came across this week, by far, was deon; a cli tool for adding an ethics checklist to your data science project. It’s cool because it shows that the community is really taking the issue of ethics and bias in machine learning algorithms into cognizance. . | Detectron2: A PyTorch based modular object detection library. They made it available as a library so it can just be imported into your project. This is a really cool way to make an opensource contribution! Detectron2 has new features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc. You can read more about it on their blog or check out the code base. . | Codespaces: GitHub just added a new feature folks! You can now write code on Github just like you would on a VS Code editor! With this cool feature, you can code, build, test, debug, and deploy with a complete development environment in your browser. There’s also a plugin for codespaces in VS Code. Seriously, you should check it out and request for early access. You see, it’s still in beta stage. You can do so here. . | Qualcom has opensourced their AI Model Efficiency Toolkit! This is in their bid to provide a simple library plugin for AI developers to utilize for state-of-the-art model efficiency performance and to make artificial intelligence ubiquitous across devices, machines, vehicles, and things. Check out their blog post and the code base for the installation guide, user guide and API documentation. . | Japanese Company, Preferred Networks, releases their First PyTorch Library. This library will help in Distributed snapshots (reducing the costs of implementing distributed deep learning with automated backup, loading, and generation management of snapshots). Other features include automatic inference of parameter sizes (easier network definitions by automatically inferring the sizes of linear or convolution layer parameters via input sizes) and implementations of convenient extensions and reporters. Let’s hope they successfully merge these features into the PyTorch base build. . | Check out the ML-Agents Unity Package v1.0 . | . . Cool Projects . Here’s an OpenCV digit recognition model by jagadishb1409. It’s really neat and would be better still if it were modularized. He uses opencv to create a window on the screen and wait to recognize your input from the screen which are captured by pre-designated keys assigned to mouse actions. An image is created when you draw a “4” on the screen for example and sent to a pre-trained MNIST digit recognizer tensorflow model generated from one of the scripts. A great way to extend this would be to modularize it and maybe make it into a flask application as is done in this MNIST-flask_app by Vaihab. . | MedSeg.ai uses a segmentation model to analyze CT scans. It’s free to use, but not openSource, unfortunately. . | A Multi-Language Sentiment Analysis tool using flask API using a catboost model trained on the sentement140 kaggle dataset. . | Basketball analysis API built with flask, tensorflow and opencv. Try it out for yourselves here . | Online Tool Animates Your Actions in Real Time. Pose Animator takes a 2D vector illustration and animates its containing curves in real-time based on the recognition result from PoseNet and FaceMesh. Pose Animator basically animates users’ poses and movements from either a camera feed or a static image, and can run on browsers in real time using TensorFlow.js. . | . . New Events . Competition: The Facebook/Data-Driven “Hateful Memes” Competition. The goal is to create an algorithm that identifies multimodal hate speech in internet memes. | . | Conferences: Virtual Microsoft build. | . | . . New Datasets . World largest real-world masked face dataset. The dataset includes 5,000 pictures of 525 people wearing masks and 90,000 images of the same 525 subjects without masks. This can be used in grocery stores and other public places to check if people are wearing masks or not. Conventional facial recognition technology is ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. I’d be mighty interested in building something in this direction with (or without) someone. | . . Resources . Springer has made 65 machine-learning related books available for free… for now. Check out the full list here . | Papers I read this week: . Learning concepts with enery functions : . Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities. . | paper: arxiv | blog: openai | video: Watch explanatory video on youtube, or right here: | . | . Cooperation Instead of Competition: Harvard &amp; Microsoft Research Optimizes AI-Human Teamwork: . The paper identifies a possible area for future research on human-machine cooperation as an optimization of team performance when interactions extend beyond the current level of querying humans for answers. This could include settings with more complex, interleaved interactions and those with different levels of human initiative and machine autonomy. . | paper: arxiv | blog: Quick read | . | Books: This past week, I’ve been joggling reading two books: . Learning OpenCV 4 Computer Vision with Python 3 Joseph Howse and Joe Minichino. You can subscribe to read it on Packt, or simply go through the code for the book here if you have a good background in Python. I settled on this book after a lot of consideration and exploring other books on OpenCV. What makes it stand out is the detailed explanations and the fact that it is in Python, rather than C++. No regrets so far. | . | . . I had tremendous dificulty installing Python opencv-contrib on my windows computer using anaconda. If you experience this issue, you may have to settle for OpenCV3. All examples in the book run as per normal so far. You can use the code below to install it in a conda environment. # You need to install the contrib version of opencv, otherwise you wont have access to some # features such as video editing/manipulation. # This was what worked for me after several hours of hair pulling. # you can use any python version you want actually. I prefer to be one step behind the latest release. # Why, you ask? BUGSSSS!! Bugs, thats why! C: Users xyz&gt; conda create -n flask-env flask python=3.7 # activate the environment C: Users xyz&gt; conda activate flask-env # install opencv-contrib C: Users xyz&gt; conda install -c michael_wild opencv-contrib . Flask Web Development - Miguel Grinberg. | . Heard of Flask Mega Tutorial? . No? . Heard of Flask-Migrate? . No? . Seriously? . Well, you probably don’t use Flask very often if that’s the case. Let’s change that. I would highly recommend this book if you’re looking to get started, or solidify your foundation, like I am. If you can’t afford to buy it, you can simply follow on his site here. It’s almost the same as the book. Alternatively, check out the code base. You can get the code for each chapter of the book by ‘git check’-ing the branch tags for each chapter. It’s all very efficiently managed by Miguel. . Trust me, this guy has the most educative resources by far, when it comes to flask (to the best of my very limited knowledge, of course). But seriously, his book is really awesome. Highly, highly recommend. . Thanks Miguel! . Tutorials I also went through this cool tutorial on converting old footage to 4K + 60fps + colorized with AI. You may have guessed it. They threw Jason’s (@citnaj on twitter) DeOldify algorithm into the mix. Really cool. | . | . . AI developments . Facebook AI built and deployed a real-time neural text-to-speech system that can process 1 sec of audio in 500 ms, using only CPUs. . | Watson’s Creator Wants to Teach AI a New Trick: Common Sense . | Read about a budding use case of artificial intelligence that could very well be a new age Alzheimer’s diagnosis tool here. . | Find out how Intel and University of Pennsylvania are using federated learning to detect brain tumors here, while protecting privacy of course. . | This AI can simulate an economy millions of times to create fairer tax policy. . | . . That’s it folks. That’s a recap of all AI related content I came across this week. What did I miss? What else is new in the AI field? Let me know in the comments! . Happy exploration. .",
            "url": "https://waleopakunle.com/news/2020/05/16/week-recap",
            "relUrl": "/news/2020/05/16/week-recap",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Latest Ai Research Updates",
            "content": "Latest in AI Research - Week 19 . . Hey there! Here is a collation of what’s new in AI research. . Title: CNN Explainer - Learning Convolutional Neural Networks with Interactive Visualization . | . . Abstract: Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. Users can interactively visualize and inspect the data transformation and flow of intermediate results in a CNN. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level operations (e.g., mathematical computations) and high-level outcomes (e.g., class predictions). To better understand our tool’s benefits, we conducted a qualitative user study, which shows that CNN Explainer can help users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques. . Task: | Explainability | . Implementation: | code - Tensorflow | . Authors: | Zijie J | Wang | Robert Turko | Omar Shaikh | Haekyu Park | Nilaksh Das | Fred Hohman | Minsuk Kahng | Duen Horng Chau | . Read the paper here . Other Resources: | CNN Explainer article | youtube | . . Title: Jukebox: A Generative Model for Music . | . . Abstract:We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code. . Task: | Music Generation | . Implementation: | code | . Authors: | Prafulla Dhariwal | Heewoo Jun | Christine Payne | Jong Wook Kim | Alec Radford | Ilya Sutskever | . Read the paper here . Other Resources: | OpenAI Blog | . . Title: Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection . | . . Abstract: The graph-based model can help to detect suspicious fraud online. Owing to the development of Graph Neural Networks~(GNNs), prior research work has proposed many GNN-based fraud detection frameworks based on either homogeneous graphs or heterogeneous graphs. These work follow the existing GNN framework by aggregating the neighboring information to learn the node embedding, which lays on the assumption that the neighbors share similar context, features, and relations. However, the inconsistency problem is hardly investigated, i.e., the context inconsistency, feature inconsistency, and relation inconsistency. In this paper, we introduce these inconsistencies and design a new GNN framework, GraphConsis, to tackle the inconsistency problem: (1) for the context inconsistency, we propose to combine the context embeddings with node features, (2) for the feature inconsistency, we design a consistency score to filter the inconsistent neighbors and generate corresponding sampling probability, and (3) for the relation inconsistency, we learn a relation attention weights associated with the sampled nodes. Empirical analysis on four datasets indicates the inconsistency problem is crucial in a fraud detection task. The extensive experiments prove the effectiveness of GraphConsis. We also released a GNN-based fraud detection toolbox with implementations of SOTA models. . Task: | Fraud Detection | . Implementation: | code | . Authors: | Zhiwei Liu | Yingtong Dou | Philip S. Yu | Yutong Deng | Hao Peng | . Read the paper here . . Title: TLDR - Extreme Summarization of Scientific Documents . | . Abstract: We introduce TLDR generation for scientific papers, a new automatic summarization task with high source compression, requiring expert background knowledge and complex language understanding. To facilitate research on this task, we introduce SciTLDR, a dataset of 3.9K TLDRs. Furthermore, we introduce a novel annotation protocol for scalably curating additional gold summaries by rewriting peer review comments. We use this protocol to augment our test set, yielding multiple gold TLDRs for evaluation, which is unlike most recent summarization datasets that assume only one valid gold summary. We present a training strategy for adapting pretrained language models that exploits similarities between TLDR generation and the related task of title generation, which outperforms strong extractive and abstractive summarization baselines. . Task: | Abstractive text summarization | . Implementation: | code | . Authors: | Isabel Cachola | Kyle Lo | Arman Cohan | Daniel S. Weld | . Read the paper here . Other resources: Demo . . Title: GIMP-ML - Python Plugins for using Computer Vision Models in GIMP . | . . Abstract: This paper introduces GIMP-ML, a set of Python plugins for the widely popular GNU Image Manipulation Program (GIMP). It enables the use of recent advances in computer vision to the conventional image editing pipeline in an open-source setting. Applications from deep learning such as monocular depth estimation, semantic segmentation, mask generative adversarial networks, image super-resolution, de-noising and coloring have been incorporated with GIMP through Python-based plugins. Additionally, operations on images such as edge detection and color clustering have also been added. GIMP-ML relies on standard Python packages such as numpy, scikit-image, pillow, pytorch, open-cv, scipy. Apart from these, several image manipulation techniques using these plugins have been compiled and demonstrated in the YouTube playlist with the objective of demonstrating the use-cases for machine learning based image modification. In addition, GIMP-ML also aims to bring the benefits of using deep learning networks used for computer vision tasks to routine image processing workflows. . Task: | DEPTH ESTIMATION | EDGE DETECTION | IMAGE SUPER-RESOLUTION | MONOCULAR DEPTH ESTIMATION | SEMANTIC SEGMENTATION | . Implementation: | code | . Authors: | Kritik Soman | . Read the paper here . . Title: TAPAS - Weakly Supervised Table Parsing via Pre-training . | . . Abstract: Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art. . Task: | QUESTION ANSWERING | SEMANTIC PARSING | TRANSFER LEARNING | . Implementation: | code | . Authors: | Jonathan Herzig | Paweł Krzysztof Nowak | Thomas Müller | Francesco Piccinno | Julian Martin Eisenschlos | . Read the paper here . . Title: Reinforcement Learning with Augmented Data . | . . Abstract: Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. . Task: | DATA AUGMENTATION | . Implementation: | code | . Authors: | Michael Laskin | Kimin Lee | Adam Stooke | Lerrel Pinto | Pieter Abbeel | Aravind Srinivas | . Read the paper here . . Title: TTNet - Real-time temporal and spatial video analysis of table tennis . | . . Abstract: Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. . Task: | SEMANTIC SEGMENTATION | DECISION MAKING | . Implementation: | No code implementation yet | . Authors: | Roman Voeikov | Nikolay Falaleev | Ruslan Baikulov | . Read the paper here . . That’s it folks. These arew what I’ve found most interesting in the past week. . Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below! . Until next time, stay safe! .",
            "url": "https://waleopakunle.com/papers/2020/05/08/latest-ai-research-updates",
            "relUrl": "/papers/2020/05/08/latest-ai-research-updates",
            "date": " • May 8, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Latest Papers In Ai Research",
            "content": "Latest in AI Research - Week 18 . . Hey there! Here is a collation of what’s new in AI research. . Title: Adversarial Latent Autoencoders . | . . Abstract: Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture. . Task: | IMAGE GENERATION | . Implementation: | code | . Read the paper here . . . Title: YOLOv4 - Optimal Speed and Accuracy of Object Detection . | . . Abstract: There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. . Task: | DATA AUGMENTATION | OBJECT DETECTION | . Implementation: | Code with most stars - in C &amp; Cuda | Tensorflow | . Read the paper here . . . Title: NBDT - Neural-Backed Decision Trees . | . . Abstract: Deep learning is being adopted in settings where accurate and justifiable predictions are required, ranging from finance to medical imaging. While there has been recent work providing post-hoc explanations for model predictions, there has been relatively little work exploring more directly interpretable models that can match state-of-the-art accuracy. Historically, decision trees have been the gold standard in balancing interpretability and accuracy. However, recent attempts to combine decision trees with deep learning have resulted in models that (1) achieve accuracies far lower than that of modern neural networks (e.g. ResNet) even on small datasets (e.g. MNIST), and (2) require significantly different architectures, forcing practitioners pick between accuracy and interpretability. We forgo this dilemma by creating Neural-Backed Decision Trees (NBDTs) that (1) achieve neural network accuracy and (2) require no architectural changes to a neural network. NBDTs achieve accuracy within 1% of the base neural network on CIFAR10, CIFAR100, TinyImageNet, using recently state-of-the-art WideResNet; and within 2% of EfficientNet on ImageNet. This yields state-of-the-art explainable models on ImageNet, with NBDTs improving the baseline by ~14% to 75.30% top-1 accuracy. Furthermore, we show interpretability of our model’s decisions both qualitatively and quantitatively via a semi-automatic process. . Implementation: | Pytorch code | Colab | . Read the paper here . . . Title: ResNeSt - Split-Attention Networks . | . . Abstract: While image classification models have recently continued to advance, most downstream applications such as object detection and semantic segmentation still employ ResNet variants as the backbone network due to their simple and modular structure. We present a simple and modular Split-Attention block that enables attention across feature-map groups. By stacking these Split-Attention blocks ResNet-style, we obtain a new ResNet variant which we call ResNeSt. Our network preserves the overall ResNet structure to be used in downstream tasks straightforwardly without introducing additional computational costs. ResNeSt models outperform other networks with similar model complexities. For example, ResNeSt-50 achieves 81.13% top-1 accuracy on ImageNet using a single crop-size of 224x224, outperforming previous best ResNet variant by more than 1% accuracy. This improvement also helps downstream tasks including object detection, instance segmentation and semantic segmentation. For example, by simply replace the ResNet-50 backbone with ResNeSt-50, we improve the mAP of Faster-RCNN on MS-COCO from 39.3% to 42.3% and the mIoU for DeeplabV3 on ADE20K from 42.1% to 45.1%. . Task: | IMAGE CLASSIFICATION | INSTANCE SEGMENTATION | OBJECT DETECTION | SEMANTIC SEGMENTATION | . Implementation: | Tensorflow code | Pytorch | . Read the paper here . . . Title: Lite Transformer with Long-Short Range Attention . | . . Abstract: Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. . Task: | ABSTRACTIVE TEXT SUMMARIZATION | AUTOML | LANGUAGE MODELLING | MACHINE TRANSLATION | QUANTIZATION | QUESTION ANSWERING | . Implementation: | Pytorch code | . Read the paper here . . . Title: MASS - Masked Sequence to Sequence Pre-training for Language Generation . | . Abstract: Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model. . Task: | CONVERSATIONAL RESPONSE GENERATION | MACHINE TRANSLATION | TEXT GENERATION | TEXT SUMMARIZATION | UNSUPERVISED MACHINE TRANSLATION | . Implementation: | Pytorch code (Microsoft) | Pytorch code | . Read the paper here . . That’s it, that’s what’s trending in AI research these days. What do you think? .",
            "url": "https://waleopakunle.com/papers/2020/05/02/latest-papers-in-ai-research",
            "relUrl": "/papers/2020/05/02/latest-papers-in-ai-research",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Ganspace",
            "content": "GANSpace - Discovering Interpretable GAN Controls . Quick Summary . What do they improve? . Efficiency and ease of controlling the output of two very popular GAN architectures; the StyleGAN and BigGAN. . What older methods are they improving on? . Continuous control of GANs using some form of supervision, e.g., augmenting GANs with with image labels at training time. . Issues with existing methods that necessitate this improvement . Training in older methods is very expensive. | Limitation in style and variational control in most existing GAN architectures. | Users usually have little to no control over the direction of the generated output. | . Which of the issues are they improving/solving? . Training expense: The components found by the PCA make it possible to control layerwise styling without the need for retraining. The paper demonstrates a way to control BigGANs output style mixing without retraining. . | Limitations in styling: The components found by PCA reveal that a lot of features of the inputs could be manipulated by manipulating the decomposed components responsible for those features. For example, different components were found to be responsible for large scale geometric configuration and viewpoint such as zoom level, angle and so on, while successive components control the finer details and overall appearance. . | Control of styling: They identify the most important principal axes directions of the creative process. As it turns out, these axes at the initial layers control very specific features of the output such as gender, age, background, rotation, etc. It also turns out that these axes can be targeted efficiently by the user. . | . What do they propose to do differently? . Rather than using a supervised approach of augmenting the GAN with labeled images at training time to control the direction of generation, they propose that the controls should be determined after the GAN has been trained. The directions for the control should be determined by deducing the principal axis direction post-training. . Do their experiments show an improvement? . Yes. They provide a user interface in which users can explore the principal directions interactively and visualize the impact that the modification of the components have on the output. They were able to achieve principal decomposition of the latent space and use the components to visualize and interactively edit the output. . Key achievements of the paper . Made BigGAN behave like StyleGAN with an inexpensive modification that enabled layerwise edits. They achieved this by allowing the Skip-z connections to vary individually between layers in BigGANs. as is the case in StyleGANs. . Future research areas . Effect of the application of other unsupervised techniques other than PCA, to activations. | Modifying activation data arrangements before PCA application | Improving other existing GAN architectures with PCA | Discovery of more editing controls in GANs | Applying PCA technique to GANs with audio input. | . Possible business applications . Social applications for generative art, with more user controls | Photo editing applications | Research tool for segmentation tasks | . More on the GANSpace . If you know anything about GANs, you know that first of all, they are notoriously difficult to train due to their instability (e.g mode collapse, etc.) , and you might also know that they could take an awful long time to train as well! And many times, you may not have the resource to get to the end of your training, let alone control the outputs at test time. Anyway, these guys from Alto University, Adobe Research and Nvidia, did an awesome job. They found a way to efficiently visualize and control the kinds of styles that two flavors of GANs produce. Their experiments cover StyleGAN and BigGAN, but I can imagine that the concepts they lay out in this paper can be extended to other GAN flavors as well, albeit with a little modification and creativity. . In order to explain why it is interesting (and important) to be able to control the direction that the generator takes in generating these output images/videos/audios, imagine that you had an image of yourself, which you took yesterday when you had your hair cut at home. You see, due to the lockdown, you had your haircut done by your sibling, because you know, what could go wrong? . Kikiki. . Imagine that you really, really, really miss your long hair and forgot to take a picture of it just before you cut it and your looks went horribly wrong. Imagine also, that you somehow don’t have an image of your past self for some absurd reason… Now, imagine that you download my beautiful app, that my future intelligent self will build that incorporates a novel GAN that I have trained and deployed for your convenience. Suppose that this “great app” lets you make alternate images of yourself. For example, you could generate images of yourself wearing glasses, 10 years from now, smiling with your favorite vacation spot in the background, and with your long hair that you miss so much… just imagine. Now imagine that the selection of any of these variations was entirely random. Meaning you have no direct control over the type, length and color of hair that you want and could end up with an end image looking bald and with extraordinarily large earrings. . Oh dear. . On a scale of 1 to 10 seconds, at what speed would you uninstall the application?? . That’s what I thought. . Don’t worry, the app my future self builds won’t be so horrendously silly. . Visualizing and understanding what goes on in GANs is indeed still an open, tough problem. Maybe if someone at OpenAI reads this, they’ll consider investing in a BigGAN “adversarial organism” that’s compatible with Microscope, which they just made open source recently. Maybe. . Apparently, lots of people have tried their hands at visualizing GANs. Most existing methods prior to this paper used some form of supervision (supplying labels) to guide the direction of generation of the output. This may sound easy, but I assure you they are computationally expensive. . So, what exactly do these researchers do differently? . So you know how GANs are (usually) fed random latent vectors z (noise) during training and then they somehow generate an image very similar to one in our training data? Well, how does the GAN, specifically the generator, know which direction to pursue in its “creative” process? This is the first problem they addressed. The approach they took should be known to most of you, especially if you are just starting out in the field of data science. I certainly had to brush up on it to understand how they went about it in this paper. It’s called Principal Component Analysis (PCA). In case you don’t know about it, it is a method commonly used to reduce the dimensionality of large vectors that have some form of correlation, while maintaining the variation in the data as much as possible. You can read more about PCA here and here. In essence, they used PCA to find the most important spatial directions to explore during the “creative” process of the GAN (StyleGAN and BigGAN in this case). . The way I understand the goal of applying the PCA is that they attempted to find the principal axes of synthesis in each layer of perceptrons that the latent vector (z) activates in the training process. . Now, because the way the StyleGAN is trained is slightly different from the way the BigGAN is trained, the method for obtaining the Principal Activation direction (the most important axes to be explored while starting from the random latent vector input) is different for each of these GAN architectures. . To the initiated, you will recall that the input as well as the intermediate layers in the BigGAN take in latent vectors z and Skip-z as inputs respectively, while the class label remains constant. In contrast, in the StyleGAN, the input is kept constant while the output is controlled by a non-linear vector z as input to its intermediate layers. . To the uninitiated, a basic GAN consists of a probability distribution p(z) from which a latent vector z is sampled, a neural network G(z) that generates novel images, and a discriminator network D that tries to make G slip up (or improve, depending on how you look at it). The end result is to generate an image (or video, or audio) that looks (or sounds) very convincing. In fact, it should look so convincing that the unsuspecting observer would assume the end result was sampled from the initial training distribution, p(z). In a nutshell, that is the overview of the structure and purpose of a GAN. Due to the latent vector distribution not being learnt in BigGAN, and there not being a way to parameterize the input image like in the StyleGAN, a roundabout method of obtaining the Priority Activation directions: . Take N samples of the latent vectors | Allow complete forward propagation through the model, so as to produce N activation vectors. | Compute PCA from the N activation vectors | Compute PCA coordinates of each activation by computing the dot product of the transpose of the low-rank basis matrix V and the difference between the previous layers input and the data mean. | Transfer the basis to latent space by linear regression. | . Meanwhile, the way to achieve this in StyleGAN is fairly straightforward. Principal Activation Direction discovery in the StyleGAN has the following steps: . Random sampling of latent vectors | Compute the styling weights for each layer | Compute PCA on the weight values | . By using PCA on these activations, they discovered that: The principal components discovered in earlier layers are responsible for large-scale variations like gender expression and head rotation, while components further in the layers generally control details and overall appearance. BigGAN components appear to be class invariant, at least for earlier layers. That is, the PCA component for zooming in a class such as “German Shepherd” is the same for a class “Poodle”, for example. The interpretation of the components deeper in the network may have varying interpretations across classes. Quality and generality depends on the dataset to a large degree. By modifying each of these components across the layers, various changes can be made to the final output image, without the need for a laborious retraining procedure. In BigGAN, this is achieved by varying the Skip-z inputs separately from the latent vector z inputs between layers. This is the key change in making the BigGAN behave in a way similar to the StyleGAN. . I think this paper is very interesting because it profers an efficient way to control GANs, and the applications I can think about that involve this are especially interesting. I look forward to more advances in this thinking direction. . . Authors of the awesome paper: Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris . link to paper: arxiv . code: Github . colab walk-through: coming soon . . Let me know if you liked this summary, and if you have a paper you would like me to append to my “read’n summarize” list. Until next time, keep home and stay safe! .",
            "url": "https://waleopakunle.com/papers/2020/04/26/ganspace",
            "relUrl": "/papers/2020/04/26/ganspace",
            "date": " • Apr 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Latest Papers In Ai Research",
            "content": "Latest in AI Research - Week 17 . . Hey there! It’s another Tuesday, and here is a collation of what’s new in AI research. . Title: Learning to see in the dark . | . . Abstract:Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg . Task: | Deblurring | Denoising | . Implementation: | code1 - Python 2.7, Tensorflow | code2 - Pytorch | . Read the paper here . . Title: DeepFactors: Real-Time Probabilistic Dense Monocular SLAM . | . . Abstract: The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry. . Task: | . Implementation: | code - C++ | . Read the paper here . . Title: Weight Poisoning Attacks on Pre-trained Models . | . . Abstract:Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct weight poisoning’’ attacks where pre-trained weights are injected with vulnerabilities that expose backdoors’’ after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. . Task: | Sentiment Analysis | . Implementation: | code | . Read the paper here . . Title: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents . | . . Abstract: The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open. . Task: | Atari Games | . Implementation: | code | . Read the paper here . . Title: Footprints and Free Space from a Single Color Image . | . . Abstract: Understanding the shape of a scene from a single color image is a formidable computer vision task. However, most methods aim to predict the geometry of surfaces that are visible to the camera, which is of limited use when planning paths for robots or augmented reality agents. Such agents can only move when grounded on a traversable surface, which we define as the set of classes which humans can also walk over, such as grass, footpaths and pavement. Models which predict beyond the line of sight often parameterize the scene with voxels or meshes, which can be expensive to use in machine learning frameworks. We introduce a model to predict the geometry of both visible and occluded traversable surfaces, given a single RGB image as input. We learn from stereo video sequences, using camera poses, per-frame depth and semantic segmentation to form training data, which is used to supervise an image-to-image network. We train models from the KITTI driving dataset, the indoor Matterport dataset, and from our own casually captured stereo footage. We find that a surprisingly low bar for spatial coverage of training scenes is required. We validate our algorithm against a range of strong baselines, and include an assessment of our predictions for a path-planning task. . Task: | Semantic Segmentation | . Implementation: | code | . Read the paper here . . Title: An Optimistic Perspective on Offline Reinforcement Learning . | . . Abstract: Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this replay dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. The results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced. . Task: | Atari games | Q-Learning | . Implementation: | code | . Read the paper here . . Title: AI Feynman: a Physics-Inspired Method for Symbolic Regression . | . . Abstract: A core challenge for both physics and artificial intellicence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult test set, we improve the state of the art success rate from 15% to 90%. . Task: | . Implementation: | code | . Read the paper here . . Title: Few-Shot NLG with Pre-Trained Language Model . | . Abstract: Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of textit{few-shot natural language generation}. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. . Task: | Few Shot learning | Language Modelling | Text Generation | . Implementation: | code | . Read the paper here . . Disclaimer: All papers and their related information are scraped from Paperswithcode, a super awesome site! . . Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below! . Until next time, stay safe! .",
            "url": "https://waleopakunle.com/papers/2020/04/21/latest-papers-in-ai-research",
            "relUrl": "/papers/2020/04/21/latest-papers-in-ai-research",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Latest Papers In Ai Research",
            "content": "Latest in AI Research - Week 16 . Hey there folks! Here is a list of the latest in deep learning research, along with abstract extracts! . . Title: 3D Photography using Context-aware Layered Depth Inpainting . | . Abstract: We propose a method for converting a single RGB-D input image into a 3D photo - a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner. The resulting 3D photos can be efficiently rendered with motion parallax using standard graphics engines. We validate the effectiveness of our method on a wide range of challenging everyday scenes and show fewer artifacts compared with the state of the arts. . Task: | Novel View Synthesis | . Implementation: | code | colab coming soon. | . Read the paper here . . Title: EfficientDet: Scalable and Efficient Object Detection . | . Abstract: Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. . Task: | AutoML | Object Detection | . Implementation: | code | Colab coming soon | . Read the paper here . . Title: Longformer: The Long-Document Transformer . | . Abstract: Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. . Task: | Language Modeling | . Implementation: | code | . Read the paper here . . Title: A Simple Baseline for Multi-Object Tracking . | . Abstract: There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problems. It remarkably outperforms the state-of-the-arts on the public datasets at 30 fps. We hope this baseline could inspire and help evaluate new ideas in this field. . Task: | Multi-object tracking | Object detection | Object tracking | . Implementation: | code | . Read the paper here . . Title: Learning to Explore using Active Neural SLAM . | . Abstract: This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM’. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge. . Task: | Point goal Navigation | . Implementation: | code | . Read the paper here . . Title: Top-1 Solution of Multi-Moments in Time Challenge 2019 . | . Abstract: In this technical report, we briefly introduce the solutions of our team ‘Efficient’ for the Multi-Moments in Time challenge in ICCV 2019. We first conduct several experiments with popular Image-Based action recognition methods TRN, TSN, and TSM. Then a novel temporal interlacing network is proposed towards fast and accurate recognition. Besides, the SlowFast network and its variants are explored. Finally, we ensemble all the above models and achieve 67.22 % on the validation set and 60.77 % on the test set, which ranks 1st on the final leaderboard. In addition, we release a new code repository for video understanding which unifies state-of-the-art 2D and 3D methods based on PyTorch. . Task: | Video understanding | . Implementation: | code | . Read the paper here . . Title: Temporal Interlacing Network . | . Abstract: For a long time, the vision community tries to learn the spatio-temporal representation by combining convolutional neural network together with various temporal models, such as the families of Markov chain, optical flow, RNN and temporal convolution. However, these pipelines consume enormous computing resources due to the alternately learning process for spatial and temporal information. One natural question is whether we can embed the temporal information into the spatial one so the information in the two domains can be jointly learned once-only. In this work, we answer this question by presenting a simple yet powerful operator – temporal interlacing network (TIN). Instead of learning the temporal features, TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa. A differentiable interlacing target can be learned to control the interlacing process. In this way, a heavy temporal model is replaced by a simple interlacing operator. We theoretically prove that with a learnable interlacing target, TIN performs equivalently to the regularized temporal convolution network (r-TCN), but gains 4% more accuracy with 6x less latency on 6 challenging benchmarks. . Task: | Optical flow estimation | Video Understanding | . Implementation: | code1 | code2 | . Read the paper here . . Title: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization . | . Abstract: Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks. . Task: | Cross-lingual Transfer | . Implementation: | code | . Read the paper here . . Disclaimer: All papers and their related information are scraped from Paperswithcode, a super awesome site! . . So, tell me. Which of these catch your eye? Which would you like for me to discuss and implement next? Let me know in the comment section! . Right now, my money is on 3D Photography. It sounds really interesting, doesn’t it? .",
            "url": "https://waleopakunle.com/papers/2020/04/15/latest-papers-in-ai-research",
            "relUrl": "/papers/2020/04/15/latest-papers-in-ai-research",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Openai Microscope",
            "content": "OpenAI Releases Microscope . . Image source: OpenAI . Have you ever wondered how you could interprete how the neurons in a deep learning network come about their classifications? Are you a researcher focused on shining the light on the black box of deep learning neurons? Are you an avid deep learning enthusiast like myself who would like to understand what makes your model draw a conclusion that the picture you passed to it was actualy a cup? Or do you just want to make fancy deep-dream styled artwork? . Well, OpenAI has just released the first version of an awesome toolkit that makes you do just that and more! The tool comes in a bid to interprete and understand the behavior of the thousands of neurons that make up deep learning models. . According to the OpenAI blogpost, . Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems. . How do you use the Microscope? . Quoting the “About” section here, . The OpenAI Microscope is based on two concepts, a location in a model and a technique. Metaphorically, the location is where you point the microscope, the technique is what lens you affix to it. . Our models are composed of a graph of “nodes” (the neural network layers), which are connected to each other through “edges.” Each op contains hundreds of “units”, which are roughly analogous to neurons. Most of the techniques we use are useful only at a specific resolution. For instance, feature visualization can only be pointed at a “unit”, not its parent “node”. . The “lenses” refer to the Vis/channel and Vis/neuron settings applied to the layers in the settings section. . Here is one such visualization, using the resnet V2 50 architecture… err, “model organism”. Here, I selected neurons of the clock visual logits of the architecture: . . Image source: OpenAI . The shapes displayed by each neuron have some similarities with the dataset being explored. The visualizations included in the Microscope include feature visualizations, DeepDream, dataset examples (images that cause neurons to fire strongly), and synthetic tuning curves (neuroscience-inspired plots of how units respond to synthetic image families). Each technique is explained when selected. More features will be added over time of course. . In this initial release, they made a collection of visualizations of every significant layer and neuron of eight vision model architectures, which they term “model organisms”. . Microscope offers a way for researchers to collaborate in a way that was previously notoriously difficult. With this tool, collaborators can make reference to particular neurons, investigate neural intreractions in detail and share findings easily, with really eye-catching visuals. . . Image source: OpenAI . The underlying models employed in Microscope (did I mention that there are 8 of them?) are already open source on lucid, but Microscope makes these models and the neurons linkable. Since researchers can make references to particular neurons, researchers all over the world can immediately scrutinize another researcher’s claims, and make personal explorartions as well. . There are a few drawbacks to the Microscope though, although I believe these are only transient inconveniences. . They have only implemented 8 models. While these are among the more popular models, the diversity is a bit lacking. Again, this is only temporary and is due to the sheer complexity of setting these up in distributed jobs. I expect that there will be a huge spike in the amount of model organisms available in coming months as the community soaks this release in. | Can’t use your custom model… yet. This was a big blow to me. When I read the introduction to the launch, I had already started anticipating configuring the Microscope to visualize how GANs come about their outputs. It seems that’s a pipe-dream for now. The community is stuck using relatively older models till the tool is upgraded. The reason for this is that the tooling for developing these distributed jobs isn’t available to everyone just yet. | Asides from these draw backs, it does look like a particularly promising and powerful tool. And it produces some really slick visuals! This tool is a major improvement in the field of model interpretability. Conceptualizing features and neuron interactions is fundamental to evolve our understanding of increasingly complex deep learning models. . Y’all should check it out! let me know what you think about this tool in the comments! Don’t forget to tag me if you find really cool visuals! . Until next time, stay safe! .",
            "url": "https://waleopakunle.com/ai-news/2020/04/14/openai-microscope",
            "relUrl": "/ai-news/2020/04/14/openai-microscope",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Background Matting Colab Notebook",
            "content": "toc: true | badges: true | author: Wale | categories: [jupyter, Implementation] | image: /images/2020-03-09-matting/media/example-of-inputs.png | . First of all, let&#39;s clone their hard work. . !git clone https://github.com/senguptaumd/Background-Matting.git . Cloning into &#39;Background-Matting&#39;... remote: Enumerating objects: 10, done. remote: Counting objects: 100% (10/10), done. remote: Compressing objects: 100% (10/10), done. remote: Total 124 (delta 2), reused 1 (delta 0), pack-reused 114 Receiving objects: 100% (124/124), 59.16 MiB | 28.03 MiB/s, done. Resolving deltas: 100% (58/58), done. . Now, we shimmy into the correct working directory like so: . %cd /content/Background-Matting/ . /content/Background-Matting . Create directories for storing the different models . !mkdir /content/Background-Matting/Models/ !mkdir /content/Background-Matting/Models/real-fixed-cam !mkdir /content/Background-Matting/Models/real-hand-held . Now in order to make this snappy, we need to get the pretrained models. That&#39;s much easier and faster than trainng it from scratch (also, the code for training isn&#39;t available yet. lol). . The researchers have graciously made their models available in google drive here. . I figured it would be quite cumbersome if you had to download the weights locally, then upload them to your google drive only for you to have to mount the drive, wouldn&#39;t it? Exactly. So, we won&#39;t be doing that. Instead, we will use gdown to get the files using thir shareable links. . So here is what you&#39;re gonna do. . Go to the google drive link in the previous cell. | Open Models/real_fixed_cam to reveal netG_epoch_12.pth | Right click on the file and copy the shareable link. | It should look like this : https://drive.google.com/open?id=1yiNsSkPYoBZ55fSQ1iwb1io9QL_PcR2i . change the &quot;open&quot; in the link to &quot;uc&quot;. Save the link somewhere you can easily reach it. | Now install &quot;gdown&quot; so we can download the pretrained weights . !pip install gdown . Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.38.0) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;gdown) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;gdown) (2020.4.5.1) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;gdown) (1.24.3) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;gdown) (2.8) . Pass in our edited link to gdown so it can work its magic, like so: . !gdown https://drive.google.com/uc?id=1yiNsSkPYoBZ55fSQ1iwb1io9QL_PcR2i . Downloading... From: https://drive.google.com/uc?id=1yiNsSkPYoBZ55fSQ1iwb1io9QL_PcR2i To: /content/Background-Matting/netG_epoch_12.pth 71.8MB [00:01, 38.4MB/s] . That downloaded the model for real-fixed-cams to the working directory. But it isn&#39;t where we would like it to be just yet. To do that, we use shutil. shutil is inbuilt with python and can be used for manipulating files and folders like so: . import shutil shutil.move(&quot;/content/Background-Matting/netG_epoch_12.pth&quot;, &quot;Models/real-fixed-cam&quot;) . &#39;Models/real-fixed-cam/netG_epoch_12.pth&#39; . There. Now we just have to do the same thing for the model for hand held cameras. The weights for the hand held camera version can be found in Models/real_hand_held. You can download them like so: . !gdown https://drive.google.com/uc?id=13HckO9fPAKYocdB_CAC5n8uyM3xQ2MpG . Downloading... From: https://drive.google.com/uc?id=13HckO9fPAKYocdB_CAC5n8uyM3xQ2MpG To: /content/Background-Matting/netG_epoch_12.pth 71.8MB [00:00, 74.3MB/s] . ... move with shutil... . shutil.move(&quot;/content/Background-Matting/netG_epoch_12.pth&quot;, &quot;Models/real-hand-held&quot;) . &#39;Models/real-hand-held/netG_epoch_12.pth&#39; . If you checkout the &quot;getting started&quot; section oftheir repo, they say to use tensorflow 1.14. But you see, I&#39;m a progressive, and I like making things break so I can fix em. So, I&#39;m gonna stick with 2.2. Here&#39;s how you check your tensorflow version: . %tensorflow_version 2.x import tensorflow as tf print(tf.__version__) . 2.2.0-rc2 . Install the requirements in the requirements.txt file. Can you smell the errors yet? No? . !pip install -r /content/Background-Matting/requirements.txt !pip install scipy==1.4.1 !pip install folium==0.2.1 !pip install imgaug==0.2.6 . Collecting numpy==1.17.0 Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB) |████████████████████████████████| 20.4MB 56.6MB/s Collecting opencv-python==3.4.5.20 Downloading https://files.pythonhosted.org/packages/85/e1/d3eed618272f4b746339af1a84b2511e79c1708d88a9195cf25d743fa614/opencv_python-3.4.5.20-cp36-cp36m-manylinux1_x86_64.whl (25.4MB) |████████████████████████████████| 25.4MB 124kB/s Collecting scikit-image==0.14.2 Downloading https://files.pythonhosted.org/packages/24/06/d560630eb9e36d90d69fe57d9ff762d8f501664ce478b8a0ae132b3c3008/scikit_image-0.14.2-cp36-cp36m-manylinux1_x86_64.whl (25.3MB) |████████████████████████████████| 25.3MB 94kB/s Collecting scipy==1.2.1 Downloading https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (24.8MB) |████████████████████████████████| 24.8MB 141kB/s Requirement already satisfied: networkx&gt;=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (2.4) Requirement already satisfied: PyWavelets&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (1.1.1) Requirement already satisfied: dask[array]&gt;=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (2.12.0) Requirement already satisfied: pillow&gt;=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (7.0.0) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (1.12.0) Requirement already satisfied: cloudpickle&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (1.3.0) Requirement already satisfied: matplotlib&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (3.2.1) Requirement already satisfied: decorator&gt;=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx&gt;=1.8-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (4.4.2) Requirement already satisfied: toolz&gt;=0.7.3; extra == &#34;array&#34; in /usr/local/lib/python3.6/dist-packages (from dask[array]&gt;=1.0.0-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.2-&gt;-r /content/Background-Matting/requirements.txt (line 3)) (0.10.0) ERROR: tensorflow 2.2.0rc2 has requirement scipy==1.4.1; python_version &gt;= &#34;3&#34;, but you&#39;ll have scipy 1.2.1 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: albumentations 0.1.12 has requirement imgaug&lt;0.2.7,&gt;=0.2.5, but you&#39;ll have imgaug 0.2.9 which is incompatible. Installing collected packages: numpy, opencv-python, scipy, scikit-image Found existing installation: numpy 1.18.2 Uninstalling numpy-1.18.2: Successfully uninstalled numpy-1.18.2 Found existing installation: opencv-python 4.1.2.30 Uninstalling opencv-python-4.1.2.30: Successfully uninstalled opencv-python-4.1.2.30 Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Found existing installation: scikit-image 0.16.2 Uninstalling scikit-image-0.16.2: Successfully uninstalled scikit-image-0.16.2 Successfully installed numpy-1.17.0 opencv-python-3.4.5.20 scikit-image-0.14.2 scipy-1.2.1 . Collecting scipy==1.4.1 Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB) |████████████████████████████████| 26.1MB 117kB/s Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy==1.4.1) (1.17.0) ERROR: albumentations 0.1.12 has requirement imgaug&lt;0.2.7,&gt;=0.2.5, but you&#39;ll have imgaug 0.2.9 which is incompatible. Installing collected packages: scipy Found existing installation: scipy 1.2.1 Uninstalling scipy-1.2.1: Successfully uninstalled scipy-1.2.1 Successfully installed scipy-1.4.1 . Collecting folium==0.2.1 Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB) |████████████████████████████████| 71kB 5.4MB/s Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1) (2.11.1) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2-&gt;folium==0.2.1) (1.1.1) Building wheels for collected packages: folium Building wheel for folium (setup.py) ... done Created wheel for folium: filename=folium-0.2.1-cp36-none-any.whl size=79979 sha256=7aa5b194ce4609067279cd3ae6406466c75ab2da8bb45f414332b991adf6c246 Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5 Successfully built folium Installing collected packages: folium Found existing installation: folium 0.8.3 Uninstalling folium-0.8.3: Successfully uninstalled folium-0.8.3 Successfully installed folium-0.2.1 Collecting imgaug==0.2.6 Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB) |████████████████████████████████| 634kB 8.3MB/s Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.6) (1.4.1) Requirement already satisfied: scikit-image&gt;=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.6) (0.14.2) Requirement already satisfied: numpy&gt;=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.6) (1.17.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.6) (1.12.0) Requirement already satisfied: pillow&gt;=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (7.0.0) Requirement already satisfied: PyWavelets&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (1.1.1) Requirement already satisfied: cloudpickle&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (1.3.0) Requirement already satisfied: matplotlib&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (3.2.1) Requirement already satisfied: networkx&gt;=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (2.4) Requirement already satisfied: dask[array]&gt;=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (2.12.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (1.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (2.4.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (2.8.1) Requirement already satisfied: decorator&gt;=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx&gt;=1.8-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (4.4.2) Requirement already satisfied: toolz&gt;=0.7.3; extra == &#34;array&#34; in /usr/local/lib/python3.6/dist-packages (from dask[array]&gt;=1.0.0-&gt;scikit-image&gt;=0.11.0-&gt;imgaug==0.2.6) (0.10.0) Building wheels for collected packages: imgaug Building wheel for imgaug (setup.py) ... done Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=f8135573b05255cf51a51ca7212636d142500dec9d053325de2dd179406b80a3 Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0 Successfully built imgaug Installing collected packages: imgaug Found existing installation: imgaug 0.2.9 Uninstalling imgaug-0.2.9: Successfully uninstalled imgaug-0.2.9 Successfully installed imgaug-0.2.6 . Go ahead and restart the runtime, and then change back to the working directory. Trust me. It&#39;ll work. . %cd /content/Background-Matting/ . /content/Background-Matting . !pwd . /content/Background-Matting . You can optionally download the Adobe trained weights, but I have commented them out because I didn&#39;t use them. Uncomment them if you want to be adventurous when the training scripts are released. . #!gdown https://drive.google.com/uc?id=11POaBmHUkSwJbLZVa8rCcnfwTDzZh6Kj . Downloading... From: https://drive.google.com/uc?id=11POaBmHUkSwJbLZVa8rCcnfwTDzZh6Kj To: /content/Background-Matting/netG_epoch_44.pth 71.8MB [00:03, 20.1MB/s] . #!gdown https://drive.google.com/uc?id=1c-Sd8fGk0uFUI1qxCzuLe24OQeDAdwp6 . Downloading... From: https://drive.google.com/uc?id=1c-Sd8fGk0uFUI1qxCzuLe24OQeDAdwp6 To: /content/Background-Matting/net_epoch_64.pth 71.8MB [00:02, 31.6MB/s] . .... shutil them... . #import shutil # Need to import it again because it got cleared when we restarted runtime #models = [&quot;netG_epoch_44.pth&quot;, &quot;net_epoch_64.pth&quot;] #for m in models: # shutil.move(m, &quot;Models/&quot;) . Now, lets fetch other peopl&#39;s work. We need to create segmentation masks along the way. Rather than build and train one from scratch, we follow the best practice of not reinventing the wheel: . LEVERAGE ON OTHER PEOPLE&#39;S WORK RESPECTFULY!!!! . !git clone https://github.com/tensorflow/models.git . Cloning into &#39;models&#39;... remote: Enumerating objects: 7, done. remote: Counting objects: 100% (7/7), done. remote: Compressing objects: 100% (7/7), done. remote: Total 33949 (delta 0), reused 7 (delta 0), pack-reused 33942 Receiving objects: 100% (33949/33949), 512.20 MiB | 36.94 MiB/s, done. Resolving deltas: 100% (21866/21866), done. Checking out files: 100% (2491/2491), done. . !pwd . /content/Background-Matting . Tricky parts . Remember those errors I said were gonna come? Well, here they are. If we don&#39;t modify their scripts, we are gonna run into a hell of a lot of headache. In order to explain the changes to you, I have turned on the numbering of lines of code. I&#39;ll reference the changes I made based on the line numbers. . I can al most here some smart alec going: &quot;But how do we edit .py files in google colab?!&quot; Yes, yes. 2 days ago, I was just like you. Completely clueless. But we thank Google for, well, Google! . In order to edit the .py files without having to download it to your local machine, use a simple trick. . Call up the .py file, test_segmentation_deeplab.py in our case, like this:%pycat test_segmentation_deeplab.py . Copy the cintent of whatever pops up. Paste all of it into a cell in colab. Make edits as appropriate In this case, I changed how tensorflow was imported, so the version 2.2 we are using is compatible with their 1.14 code. Then, I disabled the version 2 behavior, like so:12 # import tensorflow as tf 13 import tensorflow.compat.v1 as tf 14 tf.disable_v2_behavior() . Also had errors with line 34. Changed it from:34 graph_def = tf.GraphDef.FromString(file_handle.read()) to: . 34 graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read()) . Aaaaaand line 146 chenged from:146 tf.gfile.makedirs(model_dir) . to: . 146 tf.io.gfile.makedirs(model_dir) Remove the file with defective code. . !rm test_segmentation_deeplab.py . Add :%%writefile test_segmentation_deeplab.py to the top of the cell containing the code you edited. . Alright, let&#39;s go ahead and implement all that gibberish. sheesh! . #If you run into error for the segmentation, do the following: #%pycat test_segmentation_deeplab.py . !rm test_segmentation_deeplab.py . %%writefile test_segmentation_deeplab.py import os from io import BytesIO import tarfile import tempfile from six.moves import urllib import numpy as np from PIL import Image import cv2, pdb, glob, argparse #import tensorflow as tf import tensorflow.compat.v1 as tf tf.disable_v2_behavior() class DeepLabModel(object): &quot;&quot;&quot;Class to load deeplab model and run inference.&quot;&quot;&quot; INPUT_TENSOR_NAME = &#39;ImageTensor:0&#39; OUTPUT_TENSOR_NAME = &#39;SemanticPredictions:0&#39; INPUT_SIZE = 513 FROZEN_GRAPH_NAME = &#39;frozen_inference_graph&#39; def __init__(self, tarball_path): #&quot;&quot;&quot;Creates and loads pretrained deeplab model.&quot;&quot;&quot; self.graph = tf.Graph() graph_def = None # Extract frozen graph from tar archive. tar_file = tarfile.open(tarball_path) for tar_info in tar_file.getmembers(): if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name): file_handle = tar_file.extractfile(tar_info) graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read()) break tar_file.close() if graph_def is None: raise RuntimeError(&#39;Cannot find inference graph in tar archive.&#39;) with self.graph.as_default(): tf.import_graph_def(graph_def, name=&#39;&#39;) self.sess = tf.Session(graph=self.graph) def run(self, image): &quot;&quot;&quot;Runs inference on a single image. Args: image: A PIL.Image object, raw input image. Returns: resized_image: RGB image resized from original input image. seg_map: Segmentation map of `resized_image`. &quot;&quot;&quot; width, height = image.size resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height) target_size = (int(resize_ratio * width), int(resize_ratio * height)) resized_image = image.convert(&#39;RGB&#39;).resize(target_size, Image.ANTIALIAS) batch_seg_map = self.sess.run( self.OUTPUT_TENSOR_NAME, feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]}) seg_map = batch_seg_map[0] return resized_image, seg_map def create_pascal_label_colormap(): &quot;&quot;&quot;Creates a label colormap used in PASCAL VOC segmentation benchmark. Returns: A Colormap for visualizing segmentation results. &quot;&quot;&quot; colormap = np.zeros((256, 3), dtype=int) ind = np.arange(256, dtype=int) for shift in reversed(range(8)): for channel in range(3): colormap[:, channel] |= ((ind &gt;&gt; channel) &amp; 1) &lt;&lt; shift ind &gt;&gt;= 3 return colormap def label_to_color_image(label): &quot;&quot;&quot;Adds color defined by the dataset colormap to the label. Args: label: A 2D array with integer type, storing the segmentation label. Returns: result: A 2D array with floating type. The element of the array is the color indexed by the corresponding element in the input label to the PASCAL color map. Raises: ValueError: If label is not of rank 2 or its value is larger than color map maximum entry. &quot;&quot;&quot; if label.ndim != 2: raise ValueError(&#39;Expect 2-D input label&#39;) colormap = create_pascal_label_colormap() if np.max(label) &gt;= len(colormap): raise ValueError(&#39;label value too large.&#39;) return colormap[label] parser = argparse.ArgumentParser(description=&#39;Deeplab Segmentation&#39;) parser.add_argument(&#39;-i&#39;, &#39;--input_dir&#39;, type=str, required=True,help=&#39;Directory to save the output results. (required)&#39;) args=parser.parse_args() dir_name=args.input_dir; ## setup #################### LABEL_NAMES = np.asarray([ &#39;background&#39;, &#39;aeroplane&#39;, &#39;bicycle&#39;, &#39;bird&#39;, &#39;boat&#39;, &#39;bottle&#39;, &#39;bus&#39;, &#39;car&#39;, &#39;cat&#39;, &#39;chair&#39;, &#39;cow&#39;, &#39;diningtable&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;motorbike&#39;, &#39;person&#39;, &#39;pottedplant&#39;, &#39;sheep&#39;, &#39;sofa&#39;, &#39;train&#39;, &#39;tv&#39; ]) FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1) FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP) MODEL_NAME = &#39;xception_coco_voctrainval&#39; # @param [&#39;mobilenetv2_coco_voctrainaug&#39;, &#39;mobilenetv2_coco_voctrainval&#39;, &#39;xception_coco_voctrainaug&#39;, &#39;xception_coco_voctrainval&#39;] _DOWNLOAD_URL_PREFIX = &#39;http://download.tensorflow.org/models/&#39; _MODEL_URLS = { &#39;mobilenetv2_coco_voctrainaug&#39;: &#39;deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz&#39;, &#39;mobilenetv2_coco_voctrainval&#39;: &#39;deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz&#39;, &#39;xception_coco_voctrainaug&#39;: &#39;deeplabv3_pascal_train_aug_2018_01_04.tar.gz&#39;, &#39;xception_coco_voctrainval&#39;: &#39;deeplabv3_pascal_trainval_2018_01_04.tar.gz&#39;, } _TARBALL_NAME = _MODEL_URLS[MODEL_NAME] model_dir = &#39;deeplab_model&#39; if not os.path.exists(model_dir): tf.io.gfile.makedirs(model_dir) download_path = os.path.join(model_dir, _TARBALL_NAME) print(&#39;downloading model to %s, this might take a while...&#39; % download_path) if not os.path.exists(download_path): urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path) print(&#39;download completed! loading DeepLab model...&#39;) MODEL = DeepLabModel(download_path) print(&#39;model loaded successfully!&#39;) ####################################################################################### list_im=glob.glob(dir_name + &#39;/*_img.png&#39;); list_im.sort() for i in range(0,len(list_im)): image = Image.open(list_im[i]) res_im,seg=MODEL.run(image) seg=cv2.resize(seg.astype(np.uint8),image.size) mask_sel=(seg==15).astype(np.float32) name=list_im[i].replace(&#39;img&#39;,&#39;masksDL&#39;) cv2.imwrite(name,(255*mask_sel).astype(np.uint8)) str_msg=&#39; nDone: &#39; + dir_name print(str_msg) . Writing test_segmentation_deeplab.py . If you somehow run into errors you can&#39;t fix, just copy the cell into yours. I don&#39;t mind. . run the edited segmentation script now. . !python test_segmentation_deeplab.py -i sample_data/input . 2020-04-15 19:48:33.445318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version. Instructions for updating: non-resource variables are not supported in the long term downloading model to deeplab_model/deeplabv3_pascal_trainval_2018_01_04.tar.gz, this might take a while... download completed! loading DeepLab model... 2020-04-15 19:48:51.202558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-04-15 19:48:51.252306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.252872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P4 computeCapability: 6.1 coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s 2020-04-15 19:48:51.252927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-15 19:48:51.252993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-04-15 19:48:51.253026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-04-15 19:48:51.253057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-04-15 19:48:51.253088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-04-15 19:48:51.274806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-04-15 19:48:51.274892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-15 19:48:51.275004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.275666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.276203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-04-15 19:48:51.276578: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F 2020-04-15 19:48:51.289443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2000125000 Hz 2020-04-15 19:48:51.289670: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2949800 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-15 19:48:51.289695: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-04-15 19:48:51.408024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.408536: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2949640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-04-15 19:48:51.408562: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Tesla P4, Compute Capability 6.1 2020-04-15 19:48:51.409606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.409965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P4 computeCapability: 6.1 coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s 2020-04-15 19:48:51.410018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-15 19:48:51.410040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-04-15 19:48:51.410057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-04-15 19:48:51.410073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-04-15 19:48:51.410090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-04-15 19:48:51.410128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-04-15 19:48:51.410143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-15 19:48:51.410223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.410630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:51.410948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-04-15 19:48:51.414184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-04-15 19:48:57.779545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-04-15 19:48:57.779619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0 2020-04-15 19:48:57.779653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N 2020-04-15 19:48:57.781950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:57.782432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-04-15 19:48:57.782910: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0. 2020-04-15 19:48:57.782967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6966 MB memory) -&gt; physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1) model loaded successfully! 2020-04-15 19:48:59.655699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-04-15 19:49:04.081540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 Done: sample_data/input . Preprocess your images, so the backgrounds are aligned. This is especially important if you did not use a fixed camera on a tripod. . !python test_pre_process.py -i sample_data/input . Done: sample_data/input . We need to perform similar steps to what we did for the segmentation script to the background matting script. . #%pycat test_background-matting_image.py . !rm test_background-matting_image.py . You need to edit the following: . #os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;4&quot; . to: . os.environ[&quot;CUDA_DEVICE_ORDER&quot;]=&quot;PCI_BUS_ID&quot; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot; . This just selects the CUDA devices to be used for the matting process. It&#39;s a real headache if you don&#39;t change it. Trust me. . Dont forget to remove the defective file and write your changes by adding . %%writefile test_background-matting_image.py . to the top of the edited file. . %%writefile test_background-matting_image.py from __future__ import print_function import os, glob, time, argparse, pdb, cv2 #import matplotlib.pyplot as plt import numpy as np from skimage.measure import label import torch import torch.nn as nn from torch.autograd import Variable import torch.backends.cudnn as cudnn from functions import * from networks import ResnetConditionHR torch.set_num_threads(1) os.environ[&quot;CUDA_DEVICE_ORDER&quot;]=&quot;PCI_BUS_ID&quot; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot; print(&#39;CUDA Device: &#39; + os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]) &quot;&quot;&quot;Parses arguments.&quot;&quot;&quot; parser = argparse.ArgumentParser(description=&#39;Background Matting.&#39;) parser.add_argument(&#39;-m&#39;, &#39;--trained_model&#39;, type=str, default=&#39;real-fixed-cam&#39;,choices=[&#39;real-fixed-cam&#39;, &#39;real-hand-held&#39;, &#39;syn-comp-adobe&#39;],help=&#39;Trained background matting model&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output_dir&#39;, type=str, required=True,help=&#39;Directory to save the output results. (required)&#39;) parser.add_argument(&#39;-i&#39;, &#39;--input_dir&#39;, type=str, required=True,help=&#39;Directory to load input images. (required)&#39;) parser.add_argument(&#39;-tb&#39;, &#39;--target_back&#39;, type=str,help=&#39;Directory to load the target background.&#39;) parser.add_argument(&#39;-b&#39;, &#39;--back&#39;, type=str,default=None,help=&#39;Captured background image. (only use for inference on videos with fixed camera&#39;) args=parser.parse_args() #input model model_main_dir=&#39;Models/&#39; + args.trained_model + &#39;/&#39;; #input data path data_path=args.input_dir if os.path.isdir(args.target_back): args.video=True print(&#39;Using video mode&#39;) else: args.video=False print(&#39;Using image mode&#39;) #target background path back_img10=cv2.imread(args.target_back); back_img10=cv2.cvtColor(back_img10,cv2.COLOR_BGR2RGB); #Green-screen background back_img20=np.zeros(back_img10.shape); back_img20[...,0]=120; back_img20[...,1]=255; back_img20[...,2]=155; #initialize network fo=glob.glob(model_main_dir + &#39;netG_epoch_*.pth&#39;) model_name1=fo[0] netM=ResnetConditionHR(input_nc=(3,3,1,4),output_nc=4,n_blocks1=7,n_blocks2=3) netM=nn.DataParallel(netM) netM.load_state_dict(torch.load(model_name1)) netM.cuda(); netM.eval() cudnn.benchmark=True reso=(512,512) #input reoslution to the network #load captured background for video mode, fixed camera if args.back is not None: bg_im0=cv2.imread(args.back); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB); #Create a list of test images test_imgs = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and f.endswith(&#39;_img.png&#39;)] test_imgs.sort() #output directory result_path=args.output_dir if not os.path.exists(result_path): os.makedirs(result_path) for i in range(0,len(test_imgs)): filename = test_imgs[i] #original image bgr_img = cv2.imread(os.path.join(data_path, filename)); bgr_img=cv2.cvtColor(bgr_img,cv2.COLOR_BGR2RGB); if args.back is None: #captured background image bg_im0=cv2.imread(os.path.join(data_path, filename.replace(&#39;_img&#39;,&#39;_back&#39;))); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB); #segmentation mask rcnn = cv2.imread(os.path.join(data_path, filename.replace(&#39;_img&#39;,&#39;_masksDL&#39;)),0); if args.video: #if video mode, load target background frames #target background path back_img10=cv2.imread(os.path.join(args.target_back,filename.replace(&#39;_img.png&#39;,&#39;.png&#39;))); back_img10=cv2.cvtColor(back_img10,cv2.COLOR_BGR2RGB); #Green-screen background back_img20=np.zeros(back_img10.shape); back_img20[...,0]=120; back_img20[...,1]=255; back_img20[...,2]=155; #create multiple frames with adjoining frames gap=20 multi_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4)) idx=[i-2*gap,i-gap,i+gap,i+2*gap] for t in range(0,4): if idx[t]&lt;0: idx[t]=len(test_imgs)+idx[t] elif idx[t]&gt;=len(test_imgs): idx[t]=idx[t]-len(test_imgs) file_tmp=test_imgs[idx[t]] bgr_img_mul = cv2.imread(os.path.join(data_path, file_tmp)); multi_fr_w[...,t]=cv2.cvtColor(bgr_img_mul,cv2.COLOR_BGR2GRAY); else: ## create the multi-frame multi_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4)) multi_fr_w[...,0] = cv2.cvtColor(bgr_img,cv2.COLOR_BGR2GRAY); multi_fr_w[...,1] = multi_fr_w[...,0] multi_fr_w[...,2] = multi_fr_w[...,0] multi_fr_w[...,3] = multi_fr_w[...,0] #crop tightly bgr_img0=bgr_img; bbox=get_bbox(rcnn,R=bgr_img0.shape[0],C=bgr_img0.shape[1]) crop_list=[bgr_img,bg_im0,rcnn,back_img10,back_img20,multi_fr_w] crop_list=crop_images(crop_list,reso,bbox) bgr_img=crop_list[0]; bg_im=crop_list[1]; rcnn=crop_list[2]; back_img1=crop_list[3]; back_img2=crop_list[4]; multi_fr=crop_list[5] #process segmentation mask kernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)) kernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)) rcnn=rcnn.astype(np.float32)/255; rcnn[rcnn&gt;0.2]=1; K=25 zero_id=np.nonzero(np.sum(rcnn,axis=1)==0) del_id=zero_id[0][zero_id[0]&gt;250] if len(del_id)&gt;0: del_id=[del_id[0]-2,del_id[0]-1,*del_id] rcnn=np.delete(rcnn,del_id,0) rcnn = cv2.copyMakeBorder( rcnn, 0, K + len(del_id), 0, 0, cv2.BORDER_REPLICATE) rcnn = cv2.erode(rcnn, kernel_er, iterations=10) rcnn = cv2.dilate(rcnn, kernel_dil, iterations=5) rcnn=cv2.GaussianBlur(rcnn.astype(np.float32),(31,31),0) rcnn=(255*rcnn).astype(np.uint8) rcnn=np.delete(rcnn, range(reso[0],reso[0]+K), 0) #convert to torch img=torch.from_numpy(bgr_img.transpose((2, 0, 1))).unsqueeze(0); img=2*img.float().div(255)-1 bg=torch.from_numpy(bg_im.transpose((2, 0, 1))).unsqueeze(0); bg=2*bg.float().div(255)-1 rcnn_al=torch.from_numpy(rcnn).unsqueeze(0).unsqueeze(0); rcnn_al=2*rcnn_al.float().div(255)-1 multi_fr=torch.from_numpy(multi_fr.transpose((2, 0, 1))).unsqueeze(0); multi_fr=2*multi_fr.float().div(255)-1 with torch.no_grad(): img,bg,rcnn_al, multi_fr =Variable(img.cuda()), Variable(bg.cuda()), Variable(rcnn_al.cuda()), Variable(multi_fr.cuda()) input_im=torch.cat([img,bg,rcnn_al,multi_fr],dim=1) alpha_pred,fg_pred_tmp=netM(img,bg,rcnn_al,multi_fr) al_mask=(alpha_pred&gt;0.95).type(torch.cuda.FloatTensor) # for regions with alpha&gt;0.95, simply use the image as fg fg_pred=img*al_mask + fg_pred_tmp*(1-al_mask) alpha_out=to_image(alpha_pred[0,...]); #refine alpha with connected component labels=label((alpha_out&gt;0.05).astype(int)) try: assert( labels.max() != 0 ) except: continue largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1 alpha_out=alpha_out*largestCC alpha_out=(255*alpha_out[...,0]).astype(np.uint8) fg_out=to_image(fg_pred[0,...]); fg_out=fg_out*np.expand_dims((alpha_out.astype(float)/255&gt;0.01).astype(float),axis=2); fg_out=(255*fg_out).astype(np.uint8) #Uncrop R0=bgr_img0.shape[0];C0=bgr_img0.shape[1] alpha_out0=uncrop(alpha_out,bbox,R0,C0) fg_out0=uncrop(fg_out,bbox,R0,C0) #compose back_img10=cv2.resize(back_img10,(C0,R0)); back_img20=cv2.resize(back_img20,(C0,R0)) comp_im_tr1=composite4(fg_out0,back_img10,alpha_out0) comp_im_tr2=composite4(fg_out0,back_img20,alpha_out0) cv2.imwrite(result_path+&#39;/&#39;+filename.replace(&#39;_img&#39;,&#39;_out&#39;), alpha_out0) cv2.imwrite(result_path+&#39;/&#39;+filename.replace(&#39;_img&#39;,&#39;_fg&#39;), cv2.cvtColor(fg_out0,cv2.COLOR_BGR2RGB)) cv2.imwrite(result_path+&#39;/&#39;+filename.replace(&#39;_img&#39;,&#39;_compose&#39;), cv2.cvtColor(comp_im_tr1,cv2.COLOR_BGR2RGB)) cv2.imwrite(result_path+&#39;/&#39;+filename.replace(&#39;_img&#39;,&#39;_matte&#39;).format(i), cv2.cvtColor(comp_im_tr2,cv2.COLOR_BGR2RGB)) print(&#39;Done: &#39; + str(i+1) + &#39;/&#39; + str(len(test_imgs))) . Writing test_background-matting_image.py . Test to see if you really have GPU available. (Of course you do if you&#39;re on colab. But you may want to do this if you are working locally). . You could also run this in colab. Who knows, you may have forgotten to turn the GPU on. . from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) . [name: &#34;/device:CPU:0&#34; device_type: &#34;CPU&#34; memory_limit: 268435456 locality { } incarnation: 4202033667829564289 , name: &#34;/device:XLA_CPU:0&#34; device_type: &#34;XLA_CPU&#34; memory_limit: 17179869184 locality { } incarnation: 9216759723142865038 physical_device_desc: &#34;device: XLA_CPU device&#34; , name: &#34;/device:XLA_GPU:0&#34; device_type: &#34;XLA_GPU&#34; memory_limit: 17179869184 locality { } incarnation: 10291174574456229518 physical_device_desc: &#34;device: XLA_GPU device&#34; , name: &#34;/device:GPU:0&#34; device_type: &#34;GPU&#34; memory_limit: 7304675328 locality { bus_id: 1 links { } } incarnation: 8325660143807418765 physical_device_desc: &#34;device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1&#34; ] . FINALLLYYYYY You can n ow run the matting script. Pass in the arguments as appropriate. You can check the test_background-matting_image.py script to get the arguments available. . For example, you can change the type of background targeted by changing . -tb sample_data/background/0001.png . to . -tb sample_data/background/myNewBackground.png . You get the idea. . !python test_background-matting_image.py -m real-hand-held -i sample_data/input/ -o sample_data/output/background0/ -tb sample_data/background/0001.png . CUDA Device: 0 Using image mode Done: 1/5 Done: 2/5 Done: 3/5 Done: 4/5 Done: 5/5 . !python test_background-matting_image.py -m real-hand-held -i sample_data/input/ -o sample_data/output/background1/ -tb sample_data/background/0002.png . CUDA Device: 0 Using image mode Done: 1/5 Done: 2/5 Done: 3/5 Done: 4/5 Done: 5/5 . An example with my own background. Add your own background to try it out. . #!python test_background-matting_image.py -m real-hand-held -i sample_data/input/ -o sample_data/output/background_test/ -tb sample_data/background/test_back.png . CUDA Device: 0 Using image mode Done: 1/5 Done: 2/5 Done: 3/5 Done: 4/5 Done: 5/5 . Here is a little script to help visualize my sample pictures. You can go ahead to select the rest in background0 as appropriate. . NOTE: Colab does not support cv2.imshow() anymore. Apparently, it makes Colab crash. So the alternative is to use cv2_imshow from google.colab.patches. . import cv2 from google.colab.patches import cv2_imshow import glob cv_img = [] for img in sorted(glob.glob(&#39;/content/Background-Matting/sample_data/output/background0/wale2*.png&#39;)): n= cv2.imread(img) cv_img.append(n) cv2_imshow(n) . It&#39;s a little wobbly arround the fingers, but I guess it&#39;ll do for a good meme. Let&#39;s see the other backgrounds then. . cv_img2 = [] for img in sorted(glob.glob(&#39;/content/Background-Matting/sample_data/output/background1/wale2*.png&#39;)): n= cv2.imread(img) cv_img2.append(n) cv2_imshow(n) . cv_img3 = [] for img in sorted(glob.glob(&#39;/content/Background-Matting/sample_data/output/background_test/wale2*.png&#39;)): n= cv2.imread(img) cv_img3.append(n) cv2_imshow(n) . Yikes! . If you ever want to add pictures of you or your friends, you&#39;ll need to do the following: . Provide 3 pictures: i. One containing the subject(you or your friend). Save this image like this: my_image_img.png or my_friend_img.png. Dont forget the _img.png. Place this image in sample_data/input | ii. Image of the background without the subject (use _back.png extension) eg: my_background_back.png. Dont forget the _back.png extension. The script will look for this. Place it in sample_data/input as well. . iii. You need to provide a target background, where you want the subject to be placed. Place this in sample_data/background. You can name it anything you want. Just make sure the extension is .png Alternatively, you could use one of the default backgrounds in the folder. But I challenge you to be creative. . Next, after you must have saved the images, don&#39;t forget to run the segmentation and preprocessing scripts again. The following is what you need to run: . !python test_segmentation_deeplab.py -i sample_data/input !python test_pre_process.py -i sample_data/input !python test_background-matting_image.py -m real-hand-held -i sample_data/input/ -o sample_data/output/ -tb sample_data/background/0001.png . Don&#39;t forget to change the target background to whatever you want. . That&#39;s it guys. You made it to the end. I hope you find this helpful. This is like a hobby to me, so if you are a fellow hobbyist and create some cool pictures off of this, please let me know in the comments or in the mail. . Most importantly, STAY SAFE, so you can read more of my geeky blog posts. :-) .",
            "url": "https://waleopakunle.com/2020/04/14/Background-Matting-Implementation.html",
            "relUrl": "/2020/04/14/Background-Matting-Implementation.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Latest In Ai Research",
            "content": "Here is a compilation of my reading list for the past week as well as a personal completion tracker. . . Paper Title Paper Source Code Read Implemented . Background Matting | https://arxiv.org/pdf/2004.00626v1.pdf | https://github.com/senguptaumd/Background-Matting | :heavy_check_mark: | :x: | . NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis | https://arxiv.org/pdf/2003.08934v1.pdf | https://github.com/yenchenlin/nerf-pytorch | :x: | :x: | . Tracking Objects as Points | https://arxiv.org/pdf/2004.01177v1.pdf | https://github.com/xingyizhou/CenterTrack | :x: | :x: | . Deep Residual Learning for Image Recognition | https://arxiv.org/pdf/1512.03385v1.pdf | https://github.com/tensorflow/models/tree/master/research/deeplab | :x: | :x: | . Weight Uncertainty in Neural Networks | https://arxiv.org/pdf/1505.05424v2.pdf | https://github.com/piEsposito/blitz-bayesian-deep-learning | :x: | :x: | . Multi-Interest Network with Dynamic Routing for Recommendation at Tmall | https://arxiv.org/pdf/1904.08030v1.pdf | https://github.com/shenweichen/deepmatch | :x: | :x: | . Neural Collaborative Filtering | https://arxiv.org/pdf/1708.05031.pdf | https://github.com/shenweichen/deepmatch | :x: | :x: | . TResNet: High Performance GPU-Dedicated Architecture | https://arxiv.org/pdf/2003.13630v1.pdf | https://github.com/mrT23/TResNet | :x: | :x: | . Strip Pooling: Rethinking Spatial Pooling for Scene Parsing | https://arxiv.org/pdf/2003.13328v1.pdf | https://github.com/Andrew-Qibin/SPNet | :x: | :x: | .",
            "url": "https://waleopakunle.com/news/2020/04/12/latest-in-ai-research",
            "relUrl": "/news/2020/04/12/latest-in-ai-research",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Background Matting Making The World Your Green Screen",
            "content": "Background Matting — Making the world your green screen . . ​ Source: giphy.com . Link to my colab notebook: Background matting Implementation . Have you ever wished to replace the background of a picture you took and just did not want to go through the hassle of using a professional studio? Are you a graphics designer or movie director and are just sick of having to use green screens for your video recordings? Well, if that happens to be you for some reason, I have great news for you! You can now easily replace the background of your images and videos with ease, depending on you or your team’s level of technical experience. You can now do this from the comfort of your home using a fixed or handheld camera, thanks to the work of a computer vision research team at the University of Washington. In their paper released on the 1st of April, they describe a novel architecture they came up with which lets you do just that! Now, all you need to make this a reality is a couple of GPUs, a picture(or video) of the background you want (without you in it) as well as a picture(or video) of you in the same background, all taken from your handheld mobile device. Their method is state of the art, and in my opinion, provides images that are comparable to professional images. . In this post, I am going to be giving you the key ideas from their paper without getting too technical. I will walk you through the core ideas of their paper, possible lucrative applications of the technology, as well as other resources to help you dive further in. I will also be providing a notebook set up for you to try your hands on this novel architecture, and make new mattes of yourself and your friends. . Paper Summary . Which issue/problem are they improving/solving? . The problem they attempt to solve is the problem of matting a foreground to a different background, while maintaining the integrity and quality of the image. In order to achieve this, they modeled the problem using this equation: . C = F * α + B *(1-α) . . . i. . This is called the matting equation. . Before I proceed, let me break down the equation so you fully understand what is going on. . First of all, what is matting? . Matting is the process of separating an image into foreground (F) and background (B) so you can composite the foreground onto a new background. This is the key technique behind the green screen effect that is widely used in video production, graphics, and consumer apps. . Now, what are foreground, alpha and background in the matting equation? . “Foreground” refers to the content of the image, that is, the human in the image, who is to be transferred to a different background for example. . “Background” here refers to the part of a picture, scene, or design that forms a setting for the main figures or objects, or appears furthest from the viewer. . “Alpha” refers to the mixing coefficient. It can also be thought of as the transparency of the subject being captured in the image or scene. . Now let’s look at that matting equation again: . C = F * α + B *(1-α) . . . i. . Precisely, they attempt to solve for the Foreground (F), Background (B) and transparency (α) for each pixel in a given image. . If you are familiar with computer vision, you may know that an image consists of three channels (R, G and B). The presence of these channels imply that the number of unknowns increases significantly. In this case, there are now 7 unknowns, from only 3 observations per pixel. My understanding of this is the following: . Colored Foreground (with the channels): 3 unknowns . Colored Background (has 3 channels): 3 unknowns . Mixture factor/ transparency (α): 1 unknown. . What do they improve? . Most existing matting methods of creating mattes require a green screen background or a manually created trimap to produce a good matte. Other methods rely on segmentation. Automatic, trimap-free methods are beginning to appear, but are not of comparable quality. Instead, the group from the University of Washington propose taking an additional photo of the (static) background just before or after the subject is in frame, and using this photo to perform background matting. Essentially, they reduce the . What older methods are they improving on? . Context Aware Matting (CAM) which simultaneously predicts the alpha and the foreground, thus solving the complete matting problem, but is not robust to faulty trimaps. | Sampling based methods which use sampling to build the color statistics of the known foreground and background, and then solve for the matte in the ‘unknown’ region. | Propagation-based approaches aim to propagate the alpha matte from the foreground and the background region into the ‘unknown’ region to solve the matting equation. Both sampling and propagation approaches are traditional approaches. | Matting with known natural background: Some older methods solved matting with a natural background by simple background subtraction and thresholding but the results were very sensitive to the threshold and produced binary mattes. | . . Difference between segmentation and the background matting procedure. . Source: Vivek Jarayam’s blog post on Towards Data Science . Issues with existing methods that necessitate this improvement . Previous methods took too long | Were not robust enough (e.g, to faulty trimaps) | Binary mattes were produced in some cases. | Background subtraction methods consider shadows and so do not produce alpha mattes. The problem with this is that a background image with and without a shadow are inherently different and could produce undesirable results. | . What do they propose to do differently? . Use casually taken images of the background with and without the subject. | . . The image capture process. Source: Soumyadip Sengupta et al. . Use a context switching block to switch between scenarios. | Use a GAN architecture to refine the matting output. | . Limitations of their method? . Well, first of all, the procedure requires two images. | Second, they require a static background and minute camera motion; their method would not perform well on backgrounds with people walking through (photo-bombers) or with a camera that moves far from the background capture position. | Their approach specializes exclusively on human subjects. | This approach will fail for transparent objects, waterfalls, or backgrounds containing moving humans (in the case of image inputs). | . Core ideas of the paper . Contributions introduced by the paper . The first trimap-free automatic matting algorithm that utilizes a casually captured background. | A novel matting architecture (Context Switching Block) to select among input cues. | A self-supervised adversarial training to improve mattes on real images. | Experimental comparisons to a variety of competing methods on a wide range of inputs (handheld, fixed camera, indoor, outdoor), demonstrating the relative success of our approach. | . . An overview of the background matting architecture. . Source: Soumyadip Sengupta et al. . The network architecture . Inputs and outputs | . There are actually 4 input images to the network, two of which are supplied by the user and the other two are generated off of the user supplied images. The user’s input to the system is expected to be: . A 512×512 image or video “I” of a person in front of a static, natural background | An image of just the background “B”. | Basically, all the user has to do is to take a picture using a smartphone camera and then step away from the frame to capture the background, like in the images below. . . Source: Vivek Jarayam’s blog post on Towards Data Science . The augmentation script then generates a soft segmentation (S) of the provided image, I, that contains the subject. “S” is derived by using Person Segmentation. . In addition, in the event that an image is passed rather than a video, the 4th input to be generated by the script is 4 concatenations “M” of the single frame I , converted to grayscale. However, if a video is passed instead of an image, the 4th input generated will consist of 4 concatenations of two frames before and two frames after the input video I. This is done to account for the motion cues. . This concatenated object then have their features selectively combined and passed on to Residual Blocks which then feed into the decoders that decode the Foreground and α-matte. . Training procedure | . The training of the network consists of two parts: . Supervised training . | This is done using 4 encoders managed by a Context Switching block of selectors and 3 Residual Blocks and Decoders, trained on the Adobe dataset. . . Supervised portion of the architecture. Source: Vivek Jarayam’s blog post on Towards Data Science . The purpose of the Context Switching block in the supervised training is to manage the interaction between the 4 groups of input cues [Image, Background, Segmentation and Motion] from the image and prior encoders and combine the features efficiently. They observed that the Context Switching Block architecture helped to generalize to real data. . The output of the supervised training are the foreground image “F” as well as alpha matte α. . Remember the matting equation we talked about? These are the unknowns they are trying to decipher. F and α are then laid over various backgrounds from the Adobe dataset. However the outputs at this stage were discovered to not be refined enough. That’s where the other part of the system comes in. . An unsupervised training using a Generative Adversarial Network. . | . Self-Supervised portion of the architecture. Source: Vivek Jarayam’s blog post on Towards Data Science . In order to produce refined images, the output of the supervised training is fed into the unsupervised architecture, consisting of a discriminator which has been trained to identify what real images look like, and a generator which tries to come up with new images using the outputs of the supervised network. The discriminator, which knows what real images look like, then scores the images produced by the generator in a teacher-student kind of scenario; a min-max game. The least square error approach used in LS-GAN framework is adopted and the generator tries to minimize the error, to fool the generator into assigning an image a “Real” value, while the discriminator, which is essentially a classifier, tries to spot the errors in the image produced and assign it a “Real” or “Fake” label. . The result of this is that the generator learns to produce increasingly better images. The errors of copied background, and lack of quality of the images produced in the supervised arm of the architecture are gradually overcome and the matte which now has a new background cannot be categorized as a fake image. . To get a feel of the adversarial training losses and more details about the architecture used, please consult the paper, or better still the source code showing their implementations. . Advantages of the new method introduced . Speed of producing new matting with varying backgrounds. | Gets rid of the need to use a trimap altogether. A casual image in a natural background is sufficient. | . Key achievements of the paper . It is my understanding that because this is a novel architecture and approach, there was no numerical benchmark available for this research. Hence, they conducted user studies for their real data testing. What they did was use previously available methods that make use of trimaps to generate mattes, and then take a survey of users to find out which they thought was best. . . Summary of survey results. Source: Soumyadip Sengupta et al. . Future research areas . Making the method work for subjects other than human. | Addressing the problem that subject has to remain static. | Working on transparent subject, such as glass and waterfalls. | Eradicate the need for taking more than one photograph. | Possible business applications . The movie industry for scene generation. They may no longer have to use green screens for recording videos. Now, they could use any background, and simply matte the characters into a background of their choice. | Professional photography, to generate images of people in diverse locations/backgrounds. | . Conclusion . In this post, I have gone over a proposed background matting technique that enables casual capture of high quality foreground+alpha mattes in natural settings, presented by researchers from Washington University. Their method requires the photographer to take a shot with a (human) subject and without, not moving much between shots. This approach avoids using a green screen or painstakingly constructing a detailed trimap as typically needed for high matting quality. They have developed a deep learning framework trained on synthetic-composite data and then adapted to real data using an adversarial network. . For even more details such as the nitty gritty of the network architecture, training dataset, hyperparameters and such, I suggest you read their official paper. . Resources . Link to their paper . | Link to their inference code base . | Link to their blog post and project page: blog | project page | . | Link to my colab notebook: Background matting Implementation | New Concepts . Trimap | . As the name suggests, a trimap of an image I is a partition containing three sets of regions - F, B and M. The set F contains foreground pixel information, set B contains background pixel information and M is an intermediate region separating F from B. You could think of trimap as a mask that contains a white region that defines F, a black region that defines B and a gray region that defines M, which is often a region of uncertainty (some parameters are undefined too, depending on the application) . Random affine transformations | . Affine transformation is a linear mapping method that preserves points, straight lines, and planes. Sets of parallel lines remain parallel after an affine transformation. . The affine transformation technique is typically used to correct for geometric distortions or deformations that occur with non-ideal camera angles. For example, satellite imagery uses affine transformations to correct for wide angle lens distortion, panorama stitching, and image registration. Transforming and fusing the images to a large, flat coordinate system is desirable to eliminate distortion. This enables easier interactions and calculations that don’t require accounting for image distortion. - Mathworks.com .",
            "url": "https://waleopakunle.com/papers/2020/04/12/background-matting-making-the-world-your-green-screen",
            "relUrl": "/papers/2020/04/12/background-matting-making-the-world-your-green-screen",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "from pathlib import Path loadpy = Path(&#39;load_covid_data.py&#39;) if not loadpy.exists(): ! wget https://github.com/machine-learning-apps/covid19-dashboard/blob/master/_notebooks/load_covid_data.py . C: Anaconda3 lib site-packages ipykernel_launcher.py:14: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike . df_50.columns . Index([&#39;country&#39;, &#39;state&#39;, &#39;confirmed&#39;, &#39;type&#39;, &#39;critical_estimate&#39;, &#39;days_since_10&#39;, &#39;recovered&#39;, &#39;deaths&#39;], dtype=&#39;object&#39;) . Analysis for Africa . C: Anaconda3 lib site-packages ipykernel_launcher.py:14: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike .",
            "url": "https://waleopakunle.com/covid-eda-project/",
            "relUrl": "/covid-eda-project/",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Progressively Grown Gans",
            "content": "A Summary of Progressive Growing of GANs for improved quality, stability and variation (Pro-GANs) . Before we begin, I thought it would be more insightful (and interesting) if you could first see for yourself what the algorithm produced in this paper is capable of. If you’re interested, please check this youtube video out. . Original Abstract . We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024x1024. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8:80 in unsupervised CIFAR10. . Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset. . My Summary . What did they contribute? . The NVIDIA team improved the stability, resolution (image quality) and variation of generated images. They also propose a new metric for evaluating generated images. . What older methods were they improving on? . Resolution (Image quality) | . Durugkar et al. (2016) who use one generator and multiple discriminators concurrently. . Ghosh et al. (2017) who use multiple generators and one discriminator. . Wang et al. (2017), who use multiple discriminators that operate on different spatial resolutions. . Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017 define a generator and discriminator for each level of an image pyramid (Hierarchical GANs). . Variation | . Metz et al., 2016 who unroll the discriminator to regularize its updates. . Zhao et al., 2017 who use a “repelling regularizer” that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. . Ghosh et al. (2017) who use multiple generators. . Stability | . Ioffe &amp; Szegedy, 2015; Salimans &amp; Kingma, 2016; Ba et al., 2016 and many others tend to use a variant of batch normalization in the generator and discriminator to discourage the escalation of unhealthy competition between the networks. . Issues with existing methods that necessitate this improvement . Autoregressive models have limited applicability because they are slow to evaluate and do not have latent representation although they produce sharp images. Variational autoencoders (VAEs) are easy to train but tend to produce blurry results despite recent advances and this is due to restrictions in the model architecture. GANs produce sharp images, but they do so in fairly small resolutions and somewhat limited variation. Also, training GANs remains notoriously unstable despite recent advances. . Which of the issues were they improving/solving? . Their primary contribution was to develop a training methodology for the training of GANs in which the resolution of the resulting output images could be progressively increased from an initial low-resolution images. They achieved this by progressively adding layers to the networks. . What did they propose to do differently and why? . Change: Use a single generator and discriminator network that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. . Reason: In previous approaches to training, the networks had to learn both the large-scale structure and fine scale detail of image distributions and this understandably took a long period of time. . Result: . | . Stabilized training sufficiently to enable the reliable synthesis of high resolution images. . | Reduced training time (2 - 6 times faster depending on the output resolution). . | Change: Dynamically setting the weights and scale it at runtime rather than careful initialization . Reason: Previous methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. . Result: Dynamically setting the weights ensures that learning speed is the equal for all weights. . | Change: Normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. . Reason: To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition. . Result: prevents the escalation of signal magnitudes very effectively when needed. . | Change: Considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid representations of generated and target images, starting at a low-pass resolution of 16 x 16 pixels. . Reason: Previous methods of evaluating generative performance work reliably in finding large scale mode collapse, but fail to react well to smaller effects such as loss of variation in colors and textures. They also do not directly assess image quality in terms of similarity to the training set. . Result: A new approach for evaluating generated images based on a combination of older methods, including sampling from a Laplacian pyramid and sliced Wasserstein distance (SWD) for estimating statistical similarities. A smaller SWD at a given Laplacian layer indicates that training and generated image samples appear similar in both appearance and variation at the pyramids spatial resolution. . | Change: Use minibatch standard variation to increase variation. . Reason: GANs tend to capture only a subset of variations present in training data and other approaches to tackle this problem are rather cumbersome. . Result: In their experiment, including mini batch standard variation improved the sliced Wasserstein loss. . | . Did their experiments show an improvement? . source: Prograssive growing of GANs for Improved quality, stability and variation. . As can be seen from their results that compare how different training configurations compare in terms of their SWD losses and structural similarities (MS-SIM) of generated images from different datasets (CELEBA and LSUN), progressive GANs have a significantly improved performance. Removing any of the teams new contributions makes the network perform in a handicapped manner as can be seen with and without minibatch standard deviation in (e) and (e*) respectively. . source: Prograssive growing of GANs for Improved quality, stability and variation. . Core Ideas . Older methods had notable weaknesses which were improved on in the paper, including training speed, stability in training and resolution of output images. . | This paper introduced a new methodology for generating high resolution images by progressively increasing the layers in the network. They also tackled mode collapse by using pixel-wise feature normalization and equalized learning rates for all weights. Finally, they increased variation by introducing minibatch standard deviation and proposed a new way for evaluating generated images. . | Advantages of the new method introduced: . | . Decreased training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2–6 times faster, depending on the final output resolution. . | More stable GAN training. . | Images are one step closer to photorealism. . | . What the community says about the paper? . #2 Best Image Generation model for CIFAR10 dataset | . Future research areas . Conditional Pro-GANs. . | Increasing resolution threshold. . | Increasing training stability (i.e decreasing the chances of mode collapse) Notably, some work seems to have been done on that here . | . Possible business and other applications . Applications of Pro-GAN to critical domains, such as this one. Where a Pro-GAN architecture was used to generate high resolution synthesized training data for computer-assisted diagnosis, or this one used to generate Gastritis data. . | Generating high resolution example datasets for computer vision tasks. . | Generate high resolution logo (or of anything, really) images for creatives. . | . P.S: If you think of any I may have omitted, please reach out to me. wink . Implementations . You can find the original implementation by Tero Karras here. Alternatively, you can follow this link for a quick start to the colab notebook I set up using his implementation and pretrained network, so you can run it interactively. Create a copy for yourself and hack away. . New Concepts / Terminology . The Inception Score, or IS for short, is an objective metric for evaluating the quality of generated images, specifically synthetic images output by generative adversarial network models. You can read more about how it is calculated and its application here. . Average pooling: Pooling refers to techniques used to downscale / reduce the size of an image gotten from a preceding convolutional layer, in such a way that the information in the image is retained without distortion. Hence, average pooling is just one of the versions of pooling available. You can read all about pooling in this post here. . He’s initializer: A weight initialization method that takes into account the size of the previous layer. The weights are still random, but differ in range depending on the size of the previous layer. . Laplacian pyramid: Used to reconstruct an upsampled image from an image lower in the pyramid (with less resolution). Please checkout this post by Kang and Atul for a very nice explanation and description of the implementation in OpenCV. . Sliced Wasserstein distance (SWD): A modification of the Wasserstein loss function used for increasing the stability of training GANs. You can read the rather involved paper that introduced it here. It leverages on the rather attractive feature of Wasserstein distance while making interesting modifications. . Least Squares Generative Adversarial Network (LSGAN): it’s an extension to the GAN architecture that addresses the problem of vanishing gradients and loss saturation. If you would like to read more aboout it, including how to go about implementing it, check out this blog post. . WGAN-GP or Wasserstein GAN Gradient penalty: It is a modification of the cost function used in WGANs. You can read this article to learn about the limitations of weight clipping in WGANs and the slight modification that makes WGAN-GP approach more desirable. . Training resources used . Network Architecture source: Prograssive growing of GANs for Improved quality, stability and variation. . | Hardware / Compute 8 Tesla V100 GPUs, trained for 4 days to achieve convergence. | | Datasets CELEBA-HQ - A dataset containing HD images of celebrities. 30,000 images were used in total. | LSUN - It contains around one million labeled images for each of 10 scene categories and 20 object categories. | CIFAR10 - It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. | MNIST - A database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. | | Hyperparameters Optimizer: Adam | | . Activation function: leaky ReLU, leakiness of 0.2 . | No learning rate decay or rampdown. . | ncritic = 1 . | Small weight added to discriminator output. . | Adaptive minibatch size depending on the current output resolution. . | Authors of the original paper: Tero Karras (NVIDIA), Timo Aila (NVIDIA), Samuli Laine (NVIDIA), Jaakko Lehtinen (NVIDIA and Aalto University) . That’s it for this post. Thank you for your time! . Please let me know in the comments: . Any new AI research papers you would like me to review. . | Suggestions regarding the content that might need more explanation. . | Anything you want really. . |",
            "url": "https://waleopakunle.com/papers/2020/03/09/progressively-grown-gans",
            "relUrl": "/papers/2020/03/09/progressively-grown-gans",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://waleopakunle.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Introduction . Hi there! In case it was not obvious enough from all the name tags, my name is Wale Opakunle. I am a deep learning practitioner, machine learning engineer and data journalist, well, most of the time. Other times, I am just a learning machine confounded by the velocity at which the tech space is accelerating. . I reside in Lagos, Nigeria and I have a first degree in Electrical and Electronics Engineering from the University of Lagos, right down by the Lagos lagoon. . It was there, in my final year in 2018 that I awoke my passion for artificial intelligence. I saw, and still see, numerous untapped paths in which the technology could be harnessed to change my country. In light of this, I began to voraciously read, watch and listen to all materials that so much as mentioned how to architect, produce and distribute products embedded with artificial intelligence. . I dare say that I am still a long way of from understanding all the intricacies - if that is even a thing. However, I have come to learn a whole lot by putting in the effort and I am determined to give back to the community that has helped me grow so much. . It is my deepest desire that I create an awareness of the possibilities that developments in artificial intelligence affords us and by so doing, infect as many as possible within my community with this passion and knowledge, that we may bring about the change we so much desire to see in my country, for ourselves and for posterity. . . Vision for my blog . In light of the above, my blog is dedicated to do the following: . Discuss State of The Art (SOTA) research papers within my field of interest, their implementations as well as possible impacts they could have if harnessed in Nigeria. . | Discuss projects I work on that could be helpful to the community. . | Discuss the various advances in Artificial Intelligence. . | Spurn the audience to contribute to building AI tools (based on all discussions above) for the improvement of Nigeria. . | Contribute to the decentralization and dissemination of AI knowledge in Nigeria. . | Should our interests and passions align, and you wish to collaborate, please do not hesitate to reach out to me. .",
          "url": "https://waleopakunle.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "AI News",
          "content": "This page contains a selection of news related to artificial intelligence around the world. . OpenAI Opensources Microscope .",
          "url": "https://waleopakunle.com/ai-news",
          "relUrl": "/ai-news",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Contact",
          "content": "Subscribe to my mailing list. . No spams, ever . I use Mailchimp as my marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp&#39;s privacy practices here. . . Reach out to me . Quick small favor: . Have you spotted an error I made in my bants? . Do you have suggestions on which papers you would like reviewed next? Would you like to collaborate on a deep learning project? . Please send your message below and I will reply as soon as possible! . . Thank you! I really value your feedback. . &lt;/span&gt; .",
          "url": "https://waleopakunle.com/contact",
          "relUrl": "/contact",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "This site documents a fraction of my learning process as I strive to become a world class deep learning practitioner. . . There are no Ads whatsoever on Waleopakunle.com at this time. Posts .",
          "url": "https://waleopakunle.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Papers",
          "content": "This page contains a selection of research papers, pooled from my weekly digest reading list, along with succulent code of course. . Discussed Papers . Progressive GANs . Background Matting - Making the world your green screen . GANSpace - Discovering Interpretable GAN Controls . . Weekly reading list of the latest in AI research . AI Research Reading List - Week 15 . AI Research Reading List - Week 16 . AI Research Reading List - Week 17 . AI Research Reading List - Week 18 . AI Research Reading List - Week 19 .",
          "url": "https://waleopakunle.com/papers",
          "relUrl": "/papers",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Projects",
          "content": "This page contains details of deep learning projects I am working on. . Posts .",
          "url": "https://waleopakunle.com/projects",
          "relUrl": "/projects",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "This site documents a fraction of my learning process as I strive to become a world class deep learning practitioner. . {% include alert.html text=”There are no Ads whatsoever on Waleopakunle.com at this time.” %} . Posts .",
          "url": "https://waleopakunle.com/page2/",
          "relUrl": "/page2/",
          "date": ""
      }
      
  

}