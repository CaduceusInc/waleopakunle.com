{
  
    
        "post0": {
            "title": "Background Matting Making The World Your Green Screen",
            "content": "Background Matting - Making the world your green screen . Introduction . . ​ Source: giphy.com . Have you ever wished to replace the background of a picture you took and just did not want to go through the hassle of using a professional studio? Are you a graphics designer or movie director and are just sick of having to use green screens for your video recordings? Well, if that happens to be you for some reason, I have great news for you! You can now easily replace the background of your images and videos with ease, depending on you or your team’s level of technical experience. You can now do this from the comfort of your home using a fixed or handheld camera, thanks to the work of a computer vision research team at the University of Washington. In their paper released on the 1st of April, they describe a novel architecture they came up with which lets you do just that! Now, all you need to make this a reality is a couple of GPUs, a picture(or video) of the background you want (without you in it) as well as a picture(or video) of you, all taken from your handheld mobile device. Their method is state of the art, and in my opinion, provides images that are comparable to professional images. . In this post, I am going to be giving you the key take-aways from their paper without getting too technical. I will walk you through the core ideas of their paper, possible lucrative applications of the technology, as well as other resources to help you dive further in. I will also be providing a notebook set up for you to try your hands on this novel architecture, and make new mattes of yourself and your friends. . Paper Summary . Which issue/problem are they improving/solving? . The problem they attempt to solve is the problem of matting a foreground to a different background, while maintaining the integrity and quality of the image. In order to achieve this, they modeled the problem using this equation: . C = F * α + B *(1-α) i. . This is called the matting equation. . Before I proceed, let me break down the equation so you fully understand what is going on. . First of all, what is matting? . Matting is the process of separating an image into foreground (F) and background (B) so you can composite the foreground onto a new background. This is the key technique behind the green screen effect that is widely used in video production, graphics, and consumer apps. . Now, what are foreground, alpha and background in the matting equation? . “Foreground” refers to the content of the image, that is, the human in the image, who is to be transferred to a different background for example. . “Background” here refers to the part of a picture, scene, or design that forms a setting for the main figures or objects, or appears furthest from the viewer. . “Alpha” refers to the mixing coefficient. It can also be thought of as the transparency of the subject being captured in the image or scene. . Now let’s look at that matting equation again: . C = F * α + B *(1-α) i. . Precisely, they attempt to solve for the Foreground (F), Background (B) and transparency (α) for each pixel in a given image. . If you are familiar with computer vision, you may know that an image consists of three channels (R, G and B). The presence of these channels imply that the number of unknowns increases significantly. In this case, there are now 7 unknowns, from only 3 observations per pixel. My understanding of this is the following: . Colored Foreground (with the channels): 3 unknowns . Colored Background (has 3 channels): 3 unknowns . Mixture factor/ transparency (α): 1 unknown. . What do they improve? . Most existing matting methods of creating mattes require a green screen background or a manually created trimap to produce a good matte. Other methods rely on segmentation. Automatic, trimap-free methods are beginning to appear, but are not of comparable quality. Instead, the group from the University of Washington propose taking an additional photo of the (static) background just before or after the subject is in frame, and using this photo to perform background matting. Essentially, they reduce the . What older methods are they improving on? . Context Aware Matting (CAM) which simultaneously predicts the alpha and the foreground, thus solving the complete matting problem, but is not robust to faulty trimaps. | Sampling based methods which use sampling to build the color statistics of the known foreground and background, and then solve for the matte in the ‘unknown’ region. | Propagation-based approaches aim to propagate the alpha matte from the foreground and the background region into the ‘unknown’ region to solve the matting equation. Both sampling and propagation approaches are traditional approaches. | Matting with known natural background: Some older methods solved matting with a natural background by simple background subtraction and thresholding but the results were very sensitive to the threshold and produced binary mattes. | . . Difference between segmentation and the background matting procedure. . Source: Vivek Jarayam’s blog post on Towards Data Science . Issues with existing methods that necessitate this improvement . Previous methods took too long | Were not robust enough (e.g, to faulty trimaps) | Binary mattes were produced in some cases. | Background subtraction methods consider shadows and so do not produce alpha mattes. The problem with this is that a background image with and without a shadow are inherently different and could produce undesirable results. | . What do they propose to do differently? . Use casually taken images of the background with and without the subject. | Use a context switching block to switch between scenarios. | Use a GAN architecture to refine the matting output. | . Limitations of their method? . Well, first of all, the procedure requires two images. | Second, they require a static background and minute camera motion; their method would not perform well on backgrounds with people walking through (photo-bombers) or with a camera that moves far from the background capture position. | Their approach specializes exclusively on human subjects. | This approach will fail for transparent objects, waterfalls, or backgrounds containing moving humans (in the case of image inputs). | . Core ideas of the paper . Contributions introduced by the paper . The first trimap-free automatic matting algorithm that utilizes a casually captured background. | A novel matting architecture (Context Switching Block) to select among input cues. | A self-supervised adversarial training to improve mattes on real images. | Experimental comparisons to a variety of competing methods on a wide range of inputs (handheld, fixed camera, indoor, outdoor), demonstrating the relative success of our approach. | . . An overview of the background matting architecture. . Source: Soumyadip Sengupta et al. . The network architecture . Inputs and outputs | . There are actually 4 input images to the network, two of which are supplied by the user and the other two are generated off of the user supplied images. The user’s input to the system is expected to be: . A 512×512 image or video “I” of a person in front of a static, natural background | An image of just the background “B”. | Basically, all the user has to do is to take a picture using a smartphone camera and then step away from the frame to capture the background, like in the images below. . . Source: Vivek Jarayam’s blog post on Towards Data Science . The augmentation script then generates a soft segmentation (S) of the provided image, I, that contains the subject. “S” is derived by using Person Segmentation. . In addition, in the event that an image is passed rather than a video, the 4th input to be generated by the script is 4 concatenations “M” of the single frame I , converted to grayscale. However, if a video is passed instead of an image, the 4th input generated will consist of 4 concatenations of two frames before and two frames after the input video I. This is done to account for the motion cues. . This concatenated object then have their features selectively combined and passed on to Residual Blocks which then feed into the decoders that decode the Foreground and α-matte. . Training procedure | . The training of the network consists of two parts: . Supervised training . | This is done using 4 encoders managed by a Context Switching block of selectors and 3 Residual Blocks and Decoders, trained on the Adobe dataset. . . Supervised portion of the architecture. Source: Vivek Jarayam’s blog post on Towards Data Science . The purpose of the Context Switching block in the supervised training is to manage the interaction between the 4 groups of input cues [Image, Background, Segmentation and Motion] from the image and prior encoders and combine the features efficiently. They observed that the Context Switching Block architecture helped to generalize to real data. . The output of the supervised training are the foreground image “F” as well as alpha matte α. . Remember the matting equation we talked about? These are the unknowns they are trying to decipher. F and α are then laid over various backgrounds from the Adobe dataset. However the outputs at this stage were discovered to not be refined enough. That’s where the other part of the system comes in. . An unsupervised training using a Generative Adversarial Network. . | . Self-Supervised portion of the architecture. Source: Vivek Jarayam’s blog post on Towards Data Science . In order to produce refined images, the output of the supervised training is fed into the unsupervised architecture, consisting of a discriminator which has been trained to identify what real images look like, and a generator which tries to come up with new images using the outputs of the supervised network. The discriminator, which knows what real images look like, then scores the images produced by the generator in a teacher-student kind of scenario; a min-max game. The least square error approach used in LS-GAN framework is adopted and the generator tries to minimize the error, to fool the generator into assigning an image a “Real” value, while the discriminator, which is essentially a classifier, tries to spot the errors in the image produced and assign it a “Real” or “Fake” label. . The result of this is that the generator learns to produce increasingly better images. The errors of copied background, and lack of quality of the images produced in the supervised arm of the architecture are gradually overcome and the matte which now has a new background cannot be categorized as a fake image. . To get a feel of the adversarial training losses and more details about the architecture used, please consult the paper, or better still the source code showing their implementations. . Advantages of the new method introduced . Speed of producing new matting with varying backgrounds. | Gets rid of the need to use a trimap altogether. A casual image in a natural background is sufficient. | . KEY ACHIEVEMENTS OF THE PAPER . It is my understanding that because this is a novel architecture and approach, there was no numerical benchmark available for this research. Hence, they conducted user studies for their real data testing. What they did was use previously available methods that make use of trimaps to generate mattes, and then take a survey of users to find out which they thought was best. . . Summary of survey results. Source: Soumyadip Sengupta et al. . Future research areas . Making the method work for subjects other than human. | Addressing the problem that subject has to remain static. | Working on transparent subject, such as glass and waterfalls. | Eradicate the need for taking more than one photograph. | Possible business applications . The movie industry for scene generation. They may no longer have to use green screens for recording videos. Now, they could use any background, and simply matte the characters into a background of their choice. | Professional photography, to generate images of people in diverse locations/backgrounds. | . Conclusion . I have gone over a proposed background matting technique that enables casual capture of high quality foreground+alpha mattes in natural settings, presented by researchers from Washington University. Their method requires the photographer to take a shot with a (human) subject and without, not moving much between shots. This approach avoids using a green screen or painstakingly constructing a detailed trimap as typically needed for high matting quality. They have developed a deep learning framework trained on synthetic-composite data and then adapted to real data using an adversarial network. . For even more details such as the nitty gritty of the network architecture, training dataset, hyperparameters and such, I suggest you read their official paper. . Resources . Link to their paper: https://arxiv.org/pdf/2004.00626.pdf | Link to their inference code base: https://github.com/senguptaumd/Background-Matting | . Link to their blog post and project page: | https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635 | https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635 | . Link to my colab notebook: Coming soon | New Concepts . Trimap | . As the name suggests, a trimap of an image I is a partition containing three sets of regions - F, B and M. The set F contains foreground pixel information, set B contains background pixel information and M is an intermediate region separating F from B. You could think of trimap as a mask that contains a white region that defines F, a black region that defines B and a gray region that defines M, which is often a region of uncertainty (some parameters are undefined too, depending on the application) . Random affine transformations | . Affine transformation is a linear mapping method that preserves points, straight lines, and planes. Sets of parallel lines remain parallel after an affine transformation. . The affine transformation technique is typically used to correct for geometric distortions or deformations that occur with non-ideal camera angles. For example, satellite imagery uses affine transformations to correct for wide angle lens distortion, panorama stitching, and image registration. Transforming and fusing the images to a large, flat coordinate system is desirable to eliminate distortion. This enables easier interactions and calculations that don’t require accounting for image distortion. - Mathworks.com .",
            "url": "https://waleopakunle.com/matting/gans/2020/04/12/background-matting-making-the-world-your-green-screen.html",
            "relUrl": "/matting/gans/2020/04/12/background-matting-making-the-world-your-green-screen.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "from pathlib import Path loadpy = Path(&#39;load_covid_data.py&#39;) if not loadpy.exists(): ! wget https://github.com/machine-learning-apps/covid19-dashboard/blob/master/_notebooks/load_covid_data.py . C: Anaconda3 lib site-packages ipykernel_launcher.py:14: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike . df_50.columns . Index([&#39;country&#39;, &#39;state&#39;, &#39;confirmed&#39;, &#39;type&#39;, &#39;critical_estimate&#39;, &#39;days_since_10&#39;, &#39;recovered&#39;, &#39;deaths&#39;], dtype=&#39;object&#39;) . Analysis for Africa . C: Anaconda3 lib site-packages ipykernel_launcher.py:14: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike .",
            "url": "https://waleopakunle.com/covid-eda-project/",
            "relUrl": "/covid-eda-project/",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Progressively Grown Gans",
            "content": "A Summary of Progressive Growing of GANs for improved quality, stability and variation (Pro-GANs) . Before we begin, I thought it would be more insightful (and interesting) if you could first see for yourself what the algorithm produced in this paper is capable of. If you’re interested, please check this youtube video out. . Original Abstract . We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024x1024. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8:80 in unsupervised CIFAR10. . Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset. . My Summary . What did they contribute? . The NVIDIA team improved the stability, resolution (image quality) and variation of generated images. They also propose a new metric for evaluating generated images. . What older methods were they improving on? . Resolution (Image quality) | . Durugkar et al. (2016) who use one generator and multiple discriminators concurrently. . Ghosh et al. (2017) who use multiple generators and one discriminator. . Wang et al. (2017), who use multiple discriminators that operate on different spatial resolutions. . Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017 define a generator and discriminator for each level of an image pyramid (Hierarchical GANs). . Variation | . Metz et al., 2016 who unroll the discriminator to regularize its updates. . Zhao et al., 2017 who use a “repelling regularizer” that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. . Ghosh et al. (2017) who use multiple generators. . Stability | . Ioffe &amp; Szegedy, 2015; Salimans &amp; Kingma, 2016; Ba et al., 2016 and many others tend to use a variant of batch normalization in the generator and discriminator to discourage the escalation of unhealthy competition between the networks. . Issues with existing methods that necessitate this improvement . Autoregressive models have limited applicability because they are slow to evaluate and do not have latent representation although they produce sharp images. Variational autoencoders (VAEs) are easy to train but tend to produce blurry results despite recent advances and this is due to restrictions in the model architecture. GANs produce sharp images, but they do so in fairly small resolutions and somewhat limited variation. Also, training GANs remains notoriously unstable despite recent advances. . Which of the issues were they improving/solving? . Their primary contribution was to develop a training methodology for the training of GANs in which the resolution of the resulting output images could be progressively increased from an initial low-resolution images. They achieved this by progressively adding layers to the networks. . What did they propose to do differently and why? . Change: Use a single generator and discriminator network that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. . Reason: In previous approaches to training, the networks had to learn both the large-scale structure and fine scale detail of image distributions and this understandably took a long period of time. . Result: . | . Stabilized training sufficiently to enable the reliable synthesis of high resolution images. . | Reduced training time (2 - 6 times faster depending on the output resolution). . | Change: Dynamically setting the weights and scale it at runtime rather than careful initialization . Reason: Previous methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. . Result: Dynamically setting the weights ensures that learning speed is the equal for all weights. . | Change: Normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. . Reason: To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition. . Result: prevents the escalation of signal magnitudes very effectively when needed. . | Change: Considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid representations of generated and target images, starting at a low-pass resolution of 16 x 16 pixels. . Reason: Previous methods of evaluating generative performance work reliably in finding large scale mode collapse, but fail to react well to smaller effects such as loss of variation in colors and textures. They also do not directly assess image quality in terms of similarity to the training set. . Result: A new approach for evaluating generated images based on a combination of older methods, including sampling from a Laplacian pyramid and sliced Wasserstein distance (SWD) for estimating statistical similarities. A smaller SWD at a given Laplacian layer indicates that training and generated image samples appear similar in both appearance and variation at the pyramids spatial resolution. . | Change: Use minibatch standard variation to increase variation. . Reason: GANs tend to capture only a subset of variations present in training data and other approaches to tackle this problem are rather cumbersome. . Result: In their experiment, including mini batch standard variation improved the sliced Wasserstein loss. . | . Did their experiments show an improvement? . source: Prograssive growing of GANs for Improved quality, stability and variation. . As can be seen from their results that compare how different training configurations compare in terms of their SWD losses and structural similarities (MS-SIM) of generated images from different datasets (CELEBA and LSUN), progressive GANs have a significantly improved performance. Removing any of the teams new contributions makes the network perform in a handicapped manner as can be seen with and without minibatch standard deviation in (e) and (e*) respectively. . source: Prograssive growing of GANs for Improved quality, stability and variation. . Core Ideas . Older methods had notable weaknesses which were improved on in the paper, including training speed, stability in training and resolution of output images. . | This paper introduced a new methodology for generating high resolution images by progressively increasing the layers in the network. They also tackled mode collapse by using pixel-wise feature normalization and equalized learning rates for all weights. Finally, they increased variation by introducing minibatch standard deviation and proposed a new way for evaluating generated images. . | Advantages of the new method introduced: . | . Decreased training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2–6 times faster, depending on the final output resolution. . | More stable GAN training. . | Images are one step closer to photorealism. . | . What the community says about the paper? . #2 Best Image Generation model for CIFAR10 dataset | . Future research areas . Conditional Pro-GANs. . | Increasing resolution threshold. . | Increasing training stability (i.e decreasing the chances of mode collapse) Notably, some work seems to have been done on that here . | . Possible business and other applications . Applications of Pro-GAN to critical domains, such as this one. Where a Pro-GAN architecture was used to generate high resolution synthesized training data for computer-assisted diagnosis, or this one used to generate Gastritis data. . | Generating high resolution example datasets for computer vision tasks. . | Generate high resolution logo (or of anything, really) images for creatives. . | . P.S: If you think of any I may have omitted, please reach out to me. wink . Implementations . You can find the original implementation by Tero Karras here. Alternatively, you can follow this link for a quick start to the colab notebook I set up using his implementation and pretrained network, so you can run it interactively. Create a copy for yourself and hack away. . New Concepts / Terminology . The Inception Score, or IS for short, is an objective metric for evaluating the quality of generated images, specifically synthetic images output by generative adversarial network models. You can read more about how it is calculated and its application here. . Average pooling: Pooling refers to techniques used to downscale / reduce the size of an image gotten from a preceding convolutional layer, in such a way that the information in the image is retained without distortion. Hence, average pooling is just one of the versions of pooling available. You can read all about pooling in this post here. . He’s initializer: A weight initialization method that takes into account the size of the previous layer. The weights are still random, but differ in range depending on the size of the previous layer. . Laplacian pyramid: Used to reconstruct an upsampled image from an image lower in the pyramid (with less resolution). Please checkout this post by Kang and Atul for a very nice explanation and description of the implementation in OpenCV. . Sliced Wasserstein distance (SWD): A modification of the Wasserstein loss function used for increasing the stability of training GANs. You can read the rather involved paper that introduced it here. It leverages on the rather attractive feature of Wasserstein distance while making interesting modifications. . Least Squares Generative Adversarial Network (LSGAN): it’s an extension to the GAN architecture that addresses the problem of vanishing gradients and loss saturation. If you would like to read more aboout it, including how to go about implementing it, check out this blog post. . WGAN-GP or Wasserstein GAN Gradient penalty: It is a modification of the cost function used in WGANs. You can read this article to learn about the limitations of weight clipping in WGANs and the slight modification that makes WGAN-GP approach more desirable. . Training resources used . Network Architecture source: Prograssive growing of GANs for Improved quality, stability and variation. . | Hardware / Compute 8 Tesla V100 GPUs, trained for 4 days to achieve convergence. | | Datasets CELEBA-HQ - A dataset containing HD images of celebrities. 30,000 images were used in total. | LSUN - It contains around one million labeled images for each of 10 scene categories and 20 object categories. | CIFAR10 - It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. | MNIST - A database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. | | Hyperparameters Optimizer: Adam | | . Activation function: leaky ReLU, leakiness of 0.2 . | No learning rate decay or rampdown. . | ncritic = 1 . | Small weight added to discriminator output. . | Adaptive minibatch size depending on the current output resolution. . | Authors of the original paper: Tero Karras (NVIDIA), Timo Aila (NVIDIA), Samuli Laine (NVIDIA), Jaakko Lehtinen (NVIDIA and Aalto University) . That’s it for this post. Thank you for your time! . Please let me know in the comments: . Any new AI research papers you would like me to review. . | Suggestions regarding the content that might need more explanation. . | Anything you want really. . |",
            "url": "https://waleopakunle.com/gans/research%20paper/pro-gan/2020/03/09/progressively-grown-gans.html",
            "relUrl": "/gans/research%20paper/pro-gan/2020/03/09/progressively-grown-gans.html",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://waleopakunle.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Introduction . Hi there! In case it was not obvious enough from all the name tags, my name is Wale Opakunle. I am a deep learning practitioner, machine learning engineer and data journalist, well, most of the time. Other times, I am just a learning machine confounded by the velocity at which the tech space is accelerating. . I reside in Lagos, Nigeria and I have a first degree in Electrical and Electronics Engineering from the University of Lagos, right down by the Lagos lagoon. . It was there, in my final year in 2018 that I awoke my passion for artificial intelligence. I saw, and still see, numerous untapped paths in which the technology could be harnessed to change my country. In light of this, I began to voraciously read, watch and listen to all materials that so much as mentioned how to architect, produce and distribute products embedded with artificial intelligence. . I dare say that I am still a long way of from understanding all the intricacies - if that is even a thing. However, I have come to learn a whole lot by putting in the effort and I am determined to give back to the community that has helped me grow so much. . It is my deepest desire that I create an awareness of the possibilities that developments in artificial intelligence affords us and by so doing, infect as many as possible within my community with this passion and knowledge, that we may bring about the change we so much desire to see in my country, for ourselves and for posterity. . . Vision for my blog . In light of the above, my blog is dedicated to do the following: . Discuss State of The Art (SOTA) research papers within my field of interest, their implementations as well as possible impacts they could have if harnessed in Nigeria. . | Discuss projects I work on that could be helpful to the community. . | Discuss the various advances in Artificial Intelligence. . | Spurn the audience to contribute to building AI tools (based on all discussions above) for the improvement of Nigeria. . | Contribute to the decentralization and dissemination of AI knowledge in Nigeria. . | Should our interests and passions align, and you wish to collaborate, please do not hesitate to reach out to me. .",
          "url": "https://waleopakunle.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Analytics",
          "content": "This page contains a selection of data science projects I embark on. .",
          "url": "https://waleopakunle.com/analytics",
          "relUrl": "/analytics",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Contact",
          "content": "Quick small favor: . Have you spotted an error I made in my post bants? Do you have suggestions on which papers you would like reviewed next? Would you like to collaborate on a deep learning project? Please send your message to and I will reply as soon as possible! . . Thank you! I really value your feedback. . &lt;/form&gt; .",
          "url": "https://waleopakunle.com/contact",
          "relUrl": "/contact",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "This site documents a fraction of my learning process as I strive to become a world class deep learning practitioner. . Posts .",
          "url": "https://waleopakunle.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Papers",
          "content": "This page contains a selection of research papers, pooled from my weekly digest reading list, along with succulent code of course. .",
          "url": "https://waleopakunle.com/papers",
          "relUrl": "/papers",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Projects",
          "content": "This page contains details of deep learning projects I am working on. . Posts .",
          "url": "https://waleopakunle.com/projects",
          "relUrl": "/projects",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

}