<h1 id="latest-in-ai-research---week-17">Latest in AI Research - Week 17</h1>

<p><img src="https://media.giphy.com/media/26FPC5oAdfeFPkQQE/giphy.gif" alt="R for Research" /></p>

<p>Hey there! It’s another Tuesday, and here is a collation of what’s new in AI research.</p>

<ul>
  <li>
    <blockquote>
      <p>Title:  Learning to see in the dark</p>
    </blockquote>
  </li>
</ul>

<p><img src="/images/2020-04-21-latest-in-ai/learning-t-see-in-the-dark.png" alt="learning_to_see_in_the_dark" /></p>

<blockquote>
  <p>Abstract:Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Deblurring</td>
        <td>Denoising</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/cchen156/Learning-to-See-in-the-Dark">code1</a> - Python 2.7, Tensorflow</td>
        <td><a href="https://github.com/cydonia999/Learning_to_See_in_the_Dark_PyTorch">code2</a> - Pytorch</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/1805.01934v1.pdf">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  DeepFactors: Real-Time Probabilistic Dense Monocular SLAM</p>
    </blockquote>
  </li>
</ul>

<p><img src="/images/2020-04-21-latest-in-ai/df_teaser2.gif" alt="df_teaser" /></p>

<blockquote>
  <p>Abstract: The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/jczarnowski/DeepFactors">code</a> - C++</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://github.com/jczarnowski/DeepFactors">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  Weight Poisoning Attacks on Pre-trained Models</p>
    </blockquote>
  </li>
</ul>

<p><img src="/images/2020-04-21-latest-in-ai/evil_bert_weight_poisoning.jpg" alt="evil_bert" /></p>

<blockquote>
  <p>Abstract:Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct weight poisoning’’ attacks where pre-trained weights are injected with vulnerabilities that expose backdoors’’ after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Sentiment Analysis</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/neulab/RIPPLe">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/2004.06660v1.pdf">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents</p>
    </blockquote>
  </li>
</ul>

<p><img src="https://i.imgur.com/Ntgcecq.png" alt="Architechture of different off-policy agents" /></p>

<blockquote>
  <p>Abstract: The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Atari Games</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/google-research/batch_rl">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/1709.06009v2.pdf">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  Footprints and Free Space from a Single Color Image</p>
    </blockquote>
  </li>
</ul>

<p><img src="/images/2020-04-21-latest-in-ai/footprints_and_free_space.gif" alt="footprints_and_free_space" /></p>

<blockquote>
  <p>Abstract: Understanding the shape of a scene from a single color image is a formidable computer vision task. However, most methods aim to predict the geometry of surfaces that are visible to the camera, which is of limited use when planning paths for robots or augmented reality agents. Such agents can only move when grounded on a traversable surface, which we define as the set of classes which humans can also walk over, such as grass, footpaths and pavement. Models which predict beyond the line of sight often parameterize the scene with voxels or meshes, which can be expensive to use in machine learning frameworks. We introduce a model to predict the geometry of both visible and occluded traversable surfaces, given a single RGB image as input. We learn from stereo video sequences, using camera poses, per-frame depth and semantic segmentation to form training data, which is used to supervise an image-to-image network. We train models from the KITTI driving dataset, the indoor Matterport dataset, and from our own casually captured stereo footage. We find that a surprisingly low bar for spatial coverage of training scenes is required. We validate our algorithm against a range of strong baselines, and include an assessment of our predictions for a path-planning task.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Semantic Segmentation</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/nianticlabs/footprints">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  An Optimistic Perspective on Offline Reinforcement Learning</p>
    </blockquote>
  </li>
</ul>

<p><img src="https://i.imgur.com/Ntgcecq.png" alt="Architechture of different off-policy agents" /></p>

<blockquote>
  <p>Abstract: Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this replay dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. The results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Atari games</td>
        <td>Q-Learning</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/google-research/batch_rl">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/1907.04543v3.pdf">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  AI Feynman: a Physics-Inspired Method for Symbolic Regression</p>
    </blockquote>
  </li>
</ul>

<p><img src="" alt="" /></p>

<blockquote>
  <p>Abstract: A core challenge for both physics and artificial intellicence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult test set, we improve the state of the art success rate from 15% to 90%.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/SJ001/AI-Feynman">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/1905.11481v2.pdf">here</a></p>
</blockquote>

<hr />

<ul>
  <li>
    <blockquote>
      <p>Title:  Few-Shot NLG with Pre-Trained Language Model</p>
    </blockquote>
  </li>
</ul>

<blockquote>
  <p>Abstract: Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of \textit{few-shot natural language generation}. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement.</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Task:</td>
        <td>Few Shot learning</td>
        <td>Language Modelling</td>
        <td>Text Generation</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Implementation:</td>
        <td><a href="https://github.com/czyssrs/Few-Shot-NLG">code</a></td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Read the paper <a href="https://arxiv.org/pdf/1904.09521v3.pdf">here</a></p>
</blockquote>

<hr />

<p><strong>Disclaimer</strong>: All papers and their related information are scraped from <a href="https://paperswithcode.com/">Paperswithcode</a>, a super awesome site!</p>

<hr />

<p>Which paper interests you? Which would you like to see imlemented in an easily accessible colab notebook? Let me know in the comments below!</p>

<p>Until next time, stay safe!</p>
