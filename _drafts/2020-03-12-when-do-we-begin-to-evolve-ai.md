---
toc: true
author: Wale
categories: [AGI, AI, research, personal opinion]
comments: true

---

# <u>When do we begin to evolve AI?</u> 

> Full disclosure: I am not an AI researcher... yet. All views and questions presented below are a condensation of my thought process having read a wide variety of research content from very talented individuals that I look up to.



## INTRODUCTION

One of the key reasons I began my sojourn into machine learning a while back was because I was intrigued with the possibility of humans developing artificial general intelligent systems in my lifetime, no matter how slim the chances of this potential was purported to be at the time. 

Knowledge of the various techniques and algorithms required to achieve state of the art results in tasks previously exclusive to humans were (and still are) available with the click of a finite amount of computer keys. At the time, in 2017, the AI hype was at an all time high, and with good reason. Computers were beginning to encroach on human level scores in certain tasks, and totally smashing others.

Those times were really remarkable learning periods for me. The excitement in the community was palpable and the excitement at new novel neural net architecture that could solve highly specialized tasks was infectious.

However, over the course of my personal growth, I have come to form the opinion that we seem to be focusing a whole lot of all our resources on solving highly specialized tasks, seeking to raise benchmarks even higher by nibbling away at the cost functions or inception scores. I am aware that this opinion is biased as it is formed based on the genre of research papers I have absorbed. However, having sampled from over 50 research papers, I want to believe that there is a pattern that can be extrapolated from this.

Please don't get me wrong. I do not believe that it is a waste of time to pursue this endeavor of increasing benchmark scores. As a matter of fact, I understand that it is crucial and I admire all the effort the community puts into developing these optimized algorithms. Heck, we don't want to develop a trading algorithm that doesn't adjust to falling oil prices and lose all our money because of some sub--optimized loss function, for example. I get it.

My point is that we explore other alternatives to achieving the big picture, the reason we began this in the first place - to build agents capable of generalization. 



## The Problem?





## A Proposal





## Conclusion

