---
toc: true
comments: true
author: Wale
image: /images/2020-04-27-ganspace/ganspace1.png
categories: [Research paper, Image generation]
permalink: /papers/:year/:month/:day/:title
---

# **GANSpace - Discovering Interpretable GAN Controls**

## Quick Summary

## What do they improve?

Efficiency and ease of controlling the output of two very popular GAN architectures; the StyleGAN and BigGAN.

## What older methods are they improving on?

Continuous control of GANs using some form of supervision, e.g., augmenting GANs with with image labels at training time.

## Issues with existing methods that necessitate this improvement

- Training in older methods is very expensive.
- Limitation in style and variational control in most existing GAN architectures.
- Users usually have little to no control over the direction of the generated output.

## Which of the issues are they improving/solving?

- Training expense:
The components found by the PCA make it possible to control layerwise styling without the need for retraining. The paper demonstrates a way to control BigGANs output style mixing without retraining.

- Limitations in styling:
The components found by PCA reveal that a lot of features of the inputs could be manipulated by manipulating the decomposed components responsible for those features. For example, different components were found to be responsible for large scale geometric configuration and viewpoint such as zoom level, angle and so on, while successive components control the finer details and overall appearance.

- Control of styling:
They identify the most important principal axes directions of the creative process. As it turns out, these axes at the initial layers control very specific features of the output such as gender, age, background, rotation, etc. It also turns out that these axes can be targeted efficiently by the user.

## What do they propose to do differently?

Rather than using a supervised approach of augmenting the GAN with labeled images at training time to control the direction of generation, they propose that the controls should be determined after the GAN has been trained. The directions for the control should be determined by deducing the principal axis direction post-training.

## Do their experiments show an improvement?

Yes. They provide a user interface in which users can explore the principal directions interactively and visualize the impact that the modification of the components have on the output. They were able to achieve principal decomposition of the latent space and use the components to visualize and interactively edit the output.

## Key achievements of the paper

Made BigGAN behave like StyleGAN with an inexpensive modification that enabled layerwise edits. They achieved this by allowing the Skip-z connections to vary individually between layers in BigGANs. as is the case in StyleGANs.

## Future research areas

- Effect of the application of other unsupervised techniques other than PCA, to activations.
- Modifying activation data arrangements before PCA application
- Improving other existing GAN architectures with PCA
- Discovery of more editing controls in GANs
- Applying PCA technique to GANs with audio input.

## Possible business applications

- Social applications for generative art, with more user controls
- Photo editing applications
- Research tool for segmentation tasks

## More on the GANSpace

If you know anything about GANs, you know that first of all, they are notoriously difficult to train due to their instability (e.g mode collapse, etc.) , and you might also know that they could take an awful long time to train as well! And many times, you may not have the resource to get to the end of your training, let alone control the outputs at test time.
Anyway, these guys from Alto University, Adobe Research and Nvidia,  did an awesome job. They found a way to efficiently visualize and control the kinds of styles that two flavors of GANs produce. Their experiments cover StyleGAN and BigGAN, but I can imagine that the concepts they lay out in this paper can be extended to other GAN flavors as well, albeit with a little modification and creativity.

In order to explain why it is interesting (and important) to be able to control the direction that the generator takes in generating these output images/videos/audios, imagine that you had an image of yourself, which you took yesterday when you had your hair cut at home. You see, due to the lockdown, you had your haircut done by your sibling, because you know, what could go wrong?

Kikiki.

Imagine that you really, really, really miss your long hair and forgot to take a picture of it just before you cut it and your looks went horribly wrong. Imagine also, that you somehow don’t have an image of your past self for some absurd reason… Now, imagine that you download my beautiful app, that my future intelligent self will build that incorporates a novel GAN that I have trained and deployed for your convenience. Suppose that this “great app” lets you make alternate images of yourself. For example, you could generate images of yourself wearing glasses, 10 years from now, smiling with your favorite vacation spot in the background, and with your long hair that you miss so much… just imagine. Now imagine that the selection of any of these variations was entirely random. Meaning you have no direct control over the type, length and color of hair that you want and could end up with an end image looking bald and with extraordinarily large earrings.

Oh dear.

On a scale of 1 to 10 seconds, at what speed would you uninstall the application??

That’s what I thought.

Don’t worry, the app my future self builds won’t be so horrendously silly.

Visualizing and understanding what goes on in GANs is indeed still an open, tough problem. Maybe if someone at OpenAI reads this, they’ll consider investing in a BigGAN “adversarial organism” that's compatible with Microscope, which they just made [open source recently]({% post_url 2020-04-14-openai-microscope %}). Maybe.

Apparently, lots of people have tried their hands at visualizing GANs. Most existing methods prior to this paper used some form of supervision (supplying labels) to guide the direction of generation of the output. This may sound easy, but I assure you they are computationally expensive. 

So, what exactly do these researchers do differently? 

So you know how GANs are (usually) fed random latent vectors z (noise) during training and then they somehow generate an image very similar to one in our training data? Well, how does the GAN, specifically the generator,  know which direction to pursue in its “creative” process? This is the first problem they addressed. The approach they took should be known to most of you, especially if you are just starting out in the field of data science. I certainly had to brush up on it to understand how they went about it in this paper.  It's called Principal Component Analysis (PCA). In case you don't know about it, it is a method commonly used to reduce the dimensionality of large vectors that have some form of correlation, while maintaining the variation in the data as much as possible. You can read more about PCA [here](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) and [here](https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/).
In essence, they used PCA to find the most important spatial directions to explore during the “creative” process of the GAN (StyleGAN and BigGAN in this case).

The way I understand the goal of applying the PCA is that they attempted to find the principal axes of synthesis in each layer of perceptrons that the latent vector (z) activates in the training process.

Now, because the way the StyleGAN is trained is slightly different from the way the BigGAN is trained, the method for obtaining the Principal Activation direction (the most important axes to be explored while starting from the random latent vector input) is different for each of these GAN architectures.

To the initiated, you will recall that the input as well as the intermediate layers in the BigGAN take in latent vectors z and Skip-z as inputs respectively, while the class label remains constant. In contrast, in the StyleGAN, the input is kept constant while the output is controlled by a non-linear vector z as input to its intermediate layers.

To the uninitiated, a basic GAN consists of a probability distribution p(z) from which a latent vector z is sampled, a neural network G(z) that generates novel  images, and a discriminator network D that tries to make G slip up (or improve, depending on how you look at it). The end result is to generate an image (or video, or audio) that looks (or sounds) very convincing. In fact, it should look so convincing that the unsuspecting observer would assume the end result was sampled from the initial training distribution, *p(z)*. In a nutshell, that is the overview of the structure and purpose of a GAN.
Due to the latent vector distribution not being learnt in BigGAN, and there not being a way to parameterize the input image like in the StyleGAN, a roundabout method of obtaining the Priority Activation directions:

- Take N samples of the latent vectors
- Allow complete forward propagation through the model, so as to produce N activation vectors. 
- Compute PCA from the N activation vectors
- Compute PCA coordinates of each activation by computing the dot product of the transpose of the low-rank basis matrix *V* and the difference between the previous layers input and the data mean.
- Transfer the basis to latent space by linear regression.

Meanwhile, the way to achieve this in StyleGAN is fairly straightforward. Principal Activation Direction discovery in  the StyleGAN has the following steps:

- Random sampling of latent vectors
- Compute the styling weights for each layer
- Compute PCA on the weight values

By using PCA on these activations, they discovered that:
The principal components discovered in earlier layers are responsible for large-scale variations like gender expression and head rotation, while components further in the layers generally control details and overall appearance.
BigGAN components appear to be class invariant, at least for earlier layers. That is, the PCA component for zooming in a class such as “German Shepherd” is the same for a class “Poodle”, for example. The interpretation of the components deeper in the network may have varying interpretations across classes.
Quality and generality depends on the dataset to a large degree.
By modifying each of these components across the layers, various changes can be made to the final output image, without the need for a laborious retraining procedure. In BigGAN, this is achieved by varying the Skip-z inputs separately from the latent vector z inputs between layers. This is the key change in making the BigGAN behave in a way similar to the StyleGAN.
I think this paper is very interesting because it profers an efficient way to control GANs, and the applications I can think about that involve this are especially interesting. I look forward to more advances in this thinking direction.

---

Authors of the awesome paper: Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris

---

Let me know if you liked this summary, and if you have a paper you would like me to append to my “read'n summarize” list. 
Until next time, keep home and stay safe!
